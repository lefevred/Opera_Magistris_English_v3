	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Corpuscular Quantum Physics}

	\lettrine[lines=4]{\color{BrickRed}N}ow it is time to plunge into the dark and impenetrable waters of atomic physics. It goes without saying that we will cover the theories of atomic physics only in the outline. In facts, we will limit ourselves only to the theoretical developments made between the years 1910 and about 1935 (beyond the complexity of theories requires too many pages for a general book like this one). We will also pass on many mathematical details that have already been proven and checked in other sections of this book.

	Atomic physics as you probably already know is the world of the infinitely small (points of zero dimension). This is a world, you'll see, almost special where fairly classical laws, those that govern our everyday macroscopic, does not apply in an intuitive way.

	Thus, in the early 20th century we knew only that atoms were formed by a simple core and electrons in orbit.

	The electron, the first subatomic particle (smaller than the atom) to be revealed, was showed to us by experiments on electrical currents in solids, liquids and gases. In the 19th century, physicists had no idea what was the charge, if it was continuous or particulate. Today we know that the charge seems to be a property of matter and that the total load in a system seems to be a multiple of an elementary charge corresponding to the charge of an electron (or proton).

	Michael Faraday suggested by electrolysis experiments that electricity consisted of particles of elementary charge $e$ and that a mole of such charges (see Chemistry section for the definition of the mole) was equivalent to a load of 1 Faraday that is to say $96,485\;[\text{C}]$. As Avogadro's number was not known at the time, it was not possible to determine e. However, one mole of a monovalent substance that can carry$ 1\;[\text{F}]$ charge, it had to follow only half a mole of the same substance would carry $1/2\;[\text{F}]$ and so on until the most smallest unit of charge $e$, which was to be carried by the smallest unit of mass $m$, corresponding to the mass of a single atom of the substance. In 1881, Helmholtz stated that if one accepts the hypothesis that the elementary substances were composed of atoms, we must logically conclude that electricity, both positive and negative, should be divided into finite portions that should behave like electricity atoms. George Stoney named this fundamental unit charge "\NewTerm{electron}\index{electron}". The elementary charge value is named prosaically today "\NewTerm{quantum charge}\index{quantum charge}".

	All subatomic charge known today whether positive or negative, carry a net charge that is an integer multiple of $e$. Quarks have a fractional charge but they do not appear as isolated entities. There are also fractional charges in the quantum Hall effect, but this is another story...

	Even today, some of the best physicists say that they don't really know what are an electron or even an atom. In fact, we still do not know what really what matter is (matter is energy but what is energy??)... The only thing that interest the scientist anyways is to have a mathematical tool that explains thinks quite well its field of interest and the interpretation in comparison with the sensible reality is not the main priority...

	Scientists have tried the development of several models to explain the experimental  observations of the microscopic world. Thus, there has been in the order for the must well know: the models of Dalton, Thomson, Rutherford, Bohr, Schrödinger, Sommerfeld (the latter including the major contributions of Heisenberg, de Broglie, Pauli, Dirac and Einstein for the most famous) and the standard model.

	We can situate the birth of the corpuscular quantum physics or simply "\NewTerm{quantum physics}\index{quantum physics}" ("quantum" meaning "fixed amount") in 1900, when Max Planck presented his famous paper on black body radiation (\SeeChapter{see section Thermodynamics}) at a meeting of the German physical Society and the inability of classical physics (mechanics, thermodynamics, electromagnetism) to explain certain behavior of matter at the microscopic level, that is to say at the level of of phenomena where particles of small masses located in very small regions of space are the main object for study.

	To arrive to a coherent interpretation of these experiments, it was necessary to introduce concepts radically different from those of classical physics. For example, we had to abandon the notion of classical trajectory, adopt the  quantification of energy (Planck's law) and consider that microscopic particles sometimes have a behavior similar to a wave. All these new concepts gave birth to a new physics, "quantum physics", which has developed rapidly since  in 1927, already, the foundations of the theory were completed. By abandonment of the key concepts of classical mechanics, we can say that quantum physics is a revolution (named also the "second revolution," the first being the theory of relativity) in the way of interpreting experimental measurements. With relativity introduced by Einstein, quantum physics is one of the pillars of the theoretical edifice of modern physics in the 21st century.

	Just as relativity contains classical mechanics as a limiting case (relativistic laws approaching classical laws when the velocity of a particle is sufficiently low compared to that of light), the new quantum physics contains also in the limit case the classical laws of statistical mechanics or even electromagnetism.

	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We will see that the fundamental constant characterizing quantum physics (like the speed of light characterizes relativity) is the Planck constant and deeper the fine structure constant.
	\end{tcolorbox}	

	\subsection{Dalton's model}

In 1803, John Dalton made the assumption that matter is composed of atoms of different masses and combine respecting simple mass proportions (though the idea of atom was really not new, it dates from thousands years before!). It is this theory that Dalton proposed that is the cornerstone of modern physical science. In 1808, Dalton's work entitled \textit{A new system of chemical philosophy} was published. In this book he listed the atomic weights of a number of known elements relative to the mass of hydrogen. His masses "AMU" (\SeeChapter{see chapter of Nuclear Physics}) were not entirely correct, but they are the basis for the modern periodic table of elements. Dalton arrived at his atomic theory through a study of the physical properties of atmospheric air and other gases.

Dalton assumed that the atom was a sphere:

\begin{figure}[H]
\centering
\includegraphics{img/atomistic/dalton_model.eps}
\caption{The ideal approach of Dalton}
\end{figure}

Thus, he could make a first estimate of the size of atoms:

Indeed, either $\rho$ the typical density, $m_A$ the atomic mass and $R$ the radius (unknown value) of an element which we seek to determine the size of the atom. We then have very simply:
	
Dalton knowing $\rho$ and  $m_A$ thanks to experimentation, he get:
	

\subsection{Thomson's model}

Thomson is at the origin of the discovery of the electron by his experiments on the flow of particles (electrons) created by cathode rays. Theoretician and experimenter, Thomson advanced in 1898 the "raisin bread theory" of atomic structure, in which the electrons are considered negative grapes pressed into a loaf of positive material. His model of the atom is represented by the figure below:

\begin{figure}[H]
\centering
\includegraphics{img/atomistic/thomsom_model.eps}
\caption{Thomson's Gourmet approach...}
\end{figure}

But, we know (the physicists of the 19th century also knew it) that no arrangement of static charges is stable if these charges are under the influence of the Coulomb force:
	
that we had studied in detail in the section of Electrostatic. Thus that this requires that the particles that make up the atom are in motion which leads us to develop another model: the following "Rutherford model" following.

\subsection{Rhuterfords's model}

Thus Rutherford assimilated intuitively by this theoretical fact, a few years after the discovery of Thomson, the atom to a planetary system whose center was occupied by a nucleus of positive charge, which contained almost the entire mass of the atom. The core was a hundred thousand times smaller than the atom and thus occupied a very small volume. The assumptions was more adapted to experimental results:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.55]{img/atomistic/rutherford_thomson.jpg}	
		\caption{Rutherford experiment (source: OpenStax)}
	\end{figure}
Therefore here is a pictorial representation of how should looks like an atom in this model (the distances are not at scale for obvious reasons...):

\begin{figure}[H]
\centering
\includegraphics{img/atomistic/rutherford_model.eps}
\caption{The Rutherford's planetary approach...}
\end{figure}

Rutherford applied the results we get in astronomy (\SeeChapter{see section Astronomy}) during our study of Keplerian orbits in the atom and therefore obtained conical trajectories for the rotation of the electron around the nucleus such as:
	
where $e$ is the eccentricity ($e=c/a<1$) and $p$ the focal parameter ($p=b^2/a$) of an ellipse (\SeeChapter{see section Analytic Geometry}), and where:
	

	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
\textbf{R1.} You will have to remember that when we will discuss later on Bohr's model that in the Rutherford model, $r$ can take theoretically any value!\\\\
\textbf{R2.} We will see in our study of the Rutherford scattering (\SeeChapter{see section of Nuclear Physics}) that Rutherford determined that the size of the gold atom as being worth $R\cong 3\cdot 10^{-14} [m]$. So we have a model with a factor 10,000 in comparison of the Dalton's model (that is to say...).
	\end{tcolorbox}	

But we have proven that during our study of electromagnetism that Maxwell displacement equations were given by (\SeeChapter{see section of Electrodynamics}):
	
and:
	
describe that an electron in motion (accelerated) emits energy in the form of electromagnetic radiation that we call in physics "Bremsstrahlung" explained by the Liénard-Wiechert's potential (\SeeChapter{see section Electrodynamics}).

Rutherford and Thomson were therefore faced with the following dilemma:

If the electron emits energy in the form of electromagnetic radiation, it loses kinetic energy (speed) and thus necessarily sooner or later (except external intervention) will fall on the core of the atom (illustration of the phenomenon in the figure below). But we that material surrounding us is in fact stable.
\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/atomistic/bremsstrahlung.eps}
\caption{Simplistic illustration of Bremsstrahlung}
\end{figure}
So they rejected their model and Bohr intervened then with a revolutionary hypothesis but that had to explain facts and not necessarily true reality... (Bohr was a genius experimentalist physicist  and probably the best debate partner of Einstein).

\subsection{Bohr's Model}

In 1913, Niels Bohr, who participated to the work of Rutherford on the diffusion $\alpha$ of particles (nuclei of two proton and two neutrons free of electrons), takes the model of Rutherford but includes in it three fundamental assumptions:

\subsubsection{Bohr's Postulates}

	\begin{enumerate}
		\item[P1.] The electron does not emit radiation when on certain orbits named "\NewTerm{stationary orbits}\index{stationary orbits}". This assertion is contrary to the theories of classical electrodynamics. So this implies that all the orbits are not allowed and this is a revolution in the approach of theoretical physics (the prohibited orbits are named "\NewTerm{non-stationary orbits}\index{non-stationary orbits}").
		
		\item[P2.] On any stable orbit the linear momentum  $p$ integrated on the path $r$ of the orbit is an integer multiple of the Planck's constant $h$ (assumption arising from the first one) according to the quantification of energy exchanges established by Planck's relation (\SeeChapter{see section Thermodynamics}). This assumption is sometimes named "\NewTerm{Planck's quantum hypothesis}\index{Planck's quantum hypothesis}".
		
		\item[P3.] The Planck's-Einstein relation (Planck's relation):
			
applies to the emission or absorption of radiation during the transition of an electron from one energy state $E_1$ to a state $E_2$ (which solidifies the first postulate).		
	\end{enumerate}
In fact, we find here a revolutionary and unprovable concept (today and to our knowledge) consisting in the quantification of certain physical properties.

Let us continue our analysis:

\subsubsection{Quantification}

Let $M$ be the mass of the core of an atom with an electric charge $+e$ and $m$ the mass of the electron in "orbit" around the core. We assume that $M \gg m$ and that the central mass is motionless (which is obviously false in reality).

We assimilate the circular motion of the electron around the nucleus to that of a harmonic oscillator (mass connected to a spring exerting a force opposed to a proportional constant $k_r$ to retain the  electron linked).

If the oscillations occurs in the plane, its differential equation is (\SeeChapter{see section Classical Mechanics}):
	
A (special) simple solution of this equation is (\SeeChapter{see section Differential and Integral Calculus}):
	
The kinetic energy of the system is therefore given  by (\SeeChapter{see section Wave Mechanics}):
	
and the potential energy of the system (\SeeChapter{see chapter Wave Mechanics}):
	
If we note $\nu$ the frequency of oscillation of the oscillatory motion, we then have of course (\SeeChapter{see chapter Wave Mechanics}):
	
The total energy of the system is finally written after summation and simplification (elementary trigonometry):
	
We now assume that the bound edelectron can only occupy certain energy levels (first hypothsesis) according to Planck-Einstein relation:
	
This gives us when we include the Planck-Einstein relation in the penultimate equation:
	
We note here that since the energy of the electron is quantified the amplitude of its movement is also quantified.

Consider now the following integral path also named "\NewTerm{action integral}\index{action integral}" (this is in fact angular momentum):
	
and considering the expression  for velocity previously obtained:
	
Over a period of revolution, we have:
	
Since (\SeeChapter{see section Trigonometry}):
	
The integration becomes:
	
Because $\omega=\dfrac{2\pi}{T}$ (\SeeChapter{see chapter Wave Mechanics}) we have:
	
So finally we get:
	
Given that $A^2=n\dfrac{h}{2\pi^2m\nu}$ and $\omega_0=2\pi\nu$ and also $\nu=\dfrac{1}{T}$ we get:
	
Finally:
	
This condition imposed by Bohr (second postulate) results from the quantization of energy exchanges (Planck-Einstein relation). This has the effect of imposing stationary energy levels that the electron around the nucleus can occupy.

For a circular orbit (remember though for now that we consider a circular orbit!) or radius $r$ the angular momentum (yes in fact the integral action is just the angular momentum) along the length of the orbital is:
	
or by using one of the traditional notation of angular momentum in quantum physics:
	
The angular momentum is quantized and non-zero in the Bohr's model (since $n$ is not zero). We will see that this is no longer the case in the wave model where the angular momentum can be zero.

	\subsubsection{Hydrogen Type Atoms Model without dragging}
	
	We define the study of "\NewTerm{hydrogen type atom without dragging}\index{hydrogen type atom without dragging}" as the fact of considering the atoms with a single electron of mass $m$ rotating around a central core with a charge of $Z \cdot e$ and mass $M$ such that $M \gg m$ (so the kernel is supposed fixed).
	
Let us now calculate the stationary orbits radius!

On its stationary orbit, the electron is in equilibrium because there is a true antagonism between the Coulomb force and centrifugal force. This should result in the following equality of strengths:
	
We will write starting from now (to reduce the notations):
	
This allows us to write the relation:
	
By using the quantization condition of Bohr and by squaring:
	
By dividing the last two relations to one another by:
	
we get:
		
given the expression of $k$.

The radius of the electron orbits allowed is then:
	
with $n \in \mathbb{N}^{*}$. This relation is commonly named the "\NewTerm{Bohr radius}\index{Bohr radius}" for $n=Z=1$.

The orbits of an atom in this model therefore looks like:

\begin{figure}[H]
\centering
\fbox{\includegraphics[scale=0.8]{img/atomistic/bohr_plane_planetary_model.eps}}
\caption{Bohr planar planetary model}
\end{figure}
The energy of the hydrogenoide-like atom without dragging is given by classical mechanics (case of a central force), the sum of kinetic and potential electrostatic energy:
	
with:
	
we get:
	
By introducing the expression of quantified radius obtained previously:
	
	
We therefore find that the total energy of the atom is quantified and considered as negative (corresponding to stable states because it takes a supply of energy to undo the link) such that:
	
Between two levels, the transition of an electron of the level $n_2$ to the level $n_1$ (we will specify how during the study of the photoelectric effect later) results in the emission of a frequency line given by the expression of Planck's quantization hypothesis:
	
Thus, quantum physics explain the photon energy emission at a transition between two levels!

In fact, if we assume with Bohr that the energies of an electron on its orbit are given by the inverse of the square of the integer, the energy difference between two orbits characterized by high values of these integers approaches zero when the integers tend to infinity. We find then a semblance of continuous change for the energies exchanged by an atom with the electromagnetic field and the concept of trajectory of an electron takes then again sense!

By using the full expression of the total energy, we then find the appropriate frequency to the line emitted:
	
The wavelength emitted is thus easily deduced:
	
The constant $R_H$ (denoted as $R_y$ depending on the context) is named the "\NewTerm{Rydberg constant}\index{Rydberg constant}".

An electron occupying an orbit $n$ is in a "\NewTerm{steady state}\index{steady state}" if its energy does not change.

However, a direct transition $n_2 \rightarrow n_1 (n_1<n_2)$ is accompanied by the emission of a photon whose energy is given by the frequency as we proved it before.

The "\NewTerm{ionization energy}\index{ionization energy}" is the energy it takes to give away an electron to infinity of its orbit. Thus for the ground state of hydrogen, it would be necessary to write $n_1=1$ and $n_2=+\infty$.

The result obtained by Bohr for expressing the frequency depending on the electron energy levels is a great result (which surprised Bohr himself) because by involving only major fundamental constants he theoretically founded the spectrum law that hydrogen lines followed. Law that Johann Jakob Balmer had discovered experimentally in 1885 (28 years before).

Balmer had noticed that the spectral lines were extremely thin. This implied that the energy is not emitted by the atoms continuously but only at certain specific frequencies. Moreover, this fine stripes explains the precision with which he could determine the Rydberg constant.

Chemists had also seen that each element had its own atomic spectrum. It was therefore clear that any atomic theory should reflect these two characteristics and this is what made brilliantly the Bohr's model using the assumptions of energy levels.

We define, moreover, the following spectrum series of the hydrogen atom:

	\begin{itemize}
		\item For the series starting from $n_1=1$ and going to $n_2=2,3,4,...$  we get the result of measurements (spectrum) by Lyman in 1906 in the UV.
		
		\item For the series starting from $n_1=2$ and going to $n_2=3,4,5,6,\ldots$  we get the result of measurements (spectrum) by Balmer in 1885 in the visible spectrum (respectively red, bluegreen, violet, violet).
		
		\item For the series starting from $n_1=3$ and going to $n_2=4,5,6...$  we get the result of measurements (spectrum) by Paschen in 1908 in the infra-red.
		
		\item For the series starting from $n_1=4$ and going to $n_2=5,6,7...$  we get the result of measurements (spectrum) by Brackett in 1928 in the infra-red.
		
		\item For the series starting from $n_1=5$ and going to $n_2=6,7,8...$  we get the result of measurements (spectrum) by Pfund in 1924 in the infra-red.
	\end{itemize}
	Here is a picture representation resuming this:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/atomistic/hydrogen_spectrum_series.eps}
		\caption{Some spectral series of the hydrogen atom}
	\end{figure}
	or another representation for the three most known series:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/atomistic/hydrogen_spectrum_series_simplified.eps}
		\caption{Three main series of the hydrogen atom (source: Wikipedia)}
	\end{figure}
	It follows of what we just see something very important in chemistry (to recognize easily with what element we are dealing with) and astrophysics (to analyze the elements in a cloud of gas surrounding a Star or another planet turning around a Star): the fact that materials (elements) all have a specific Line Spectra as show in the figure below:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/atomistic/line_spectra.jpg}
		\caption{Line spectra of various elements (source: OpenStax)}
	\end{figure}

	\subsubsection{Hydrogen Type Atoms Model with dragging}
	
	The atomic nucleus has a mass $M$ that we have assumed stationary for simplification. In reality, the assembly core ($M$) and electron ($m$) is rotating about a common center of mass (obviously!).
	
	Hypothesis (assumptions):
	\begin{enumerate}
		\item[H1.] The hydrogen-like atom is considered an isolated system.
	
		\item[H2.] The nucleus and the electron orbit each in a circular orbit around a common center: the "center of mass CM" (\SeeChapter{see section Classical Mechanics}).

		\item[H3.] They have the same angular velocity.
		\end{enumerate}
	The hydrogenoid atom being an isolated system, the movement of the center of mass is either in uniform rectilinear motion or at rest. It is therefore acceptable to place there an inertial reference system.
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/hydrogenoid_center_of_mass.jpg}
		\caption{Defining hydrogenoid atom configuration}
	\end{figure}
	The definition of the center of mass in a laboratory system is given by the theorem of the center of mass (\SeeChapter{see section of Classical Mechanics}):
	
The present study will be made relatively to the center of mass, the above relation becomes (\SeeChapter{see section of Classical Mechanics}):
	
From the above relation, taking the norm and absolute value, it follows that:
	
The distance between the nucleus and the electron being constant (hypothesis) such that $r=r_M+r_m$ we write:
	
We conclude that trivially:
	
	Applying the law of dynamics, we write that the sum of forces (electrostatic and centrifugal) on the electron (only!) is balanced such that:
	
that we can rewrite by isolating $\omega^2$:
	
	We find again the well-knows expression of the reduced mass for a two-body system:
	
	The both relations:
	
	will also be useful to us in the sections of Quantum Chemistry and Molecular Chemistry!
	
	Let us now determine the total energy of the atom!

	The kinetic energy of the atom is the sum of the kinetic energies of the core ($N$) and the electron ($e$) such that:
	
As equation with the assumption that the angular velocity is identical for the nucleus and the electron:
	
Using the different radii determined previously:
	
The potential energy of the electron relatively to the center of mass is given by (\SeeChapter{see section Electrostatics}):
	
The total energy of the hydrogen-like atom is then:
	

Relatively to the center of mass, the total angular momentum is the sum of the angular momentum of the electron $b_m$ and the kernel $b_M$ (remember that the angular momentum is also often denoted by the letter $L$):
	
The parenthesis of the last equality has already been the subject of a calculation previously so we have:
	
It is here that Bohr introduced its quantization condition:
	
	But we know the detailed expression of the square of the pulsation:
	
	The  quantified radius has therefore for expression:
		
	The total energy of the atom finally becomes:
		
		Either with a condensed notation:
		
From this last relation, we can easily determine the expression (as we have already done) for wavelengths emitted by a de-excitation of the electron from one orbit $n_2$ to $n_1$. Let us calculate with the same way as we did for the free training model, the expression of the wavelength emitted during the transition from one level to another. We then identical developments:
		
It comes then:
		
The wavelength emitted is then easily deduced:
		

	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
It obviously to realize that this model is more accurate than the previous one.
	\end{tcolorbox}	
	
	\subsubsection{Neutron Assumption}

Spectroscopy results are known with high precision, therefore the Rydberg's constants also (because dependent on the mass of the atomic element studied).

The two blue stripes of the  Balmer series for the hydrogen denoted  by H ($\lambda_H\cong 4861.3 \left[ \mathring{A} \right]$ composed of one proton and one electron) and of deuterium D (isotope of hydrogen consisting in one neutron more) have a difference of wavelength of $\Delta \lambda \cong 1.32 \mathring{A}$.

The wavelength belonging to the Balmer series ($n_1=2,n_2=3,4,5,...$ is therefore expressed by (with the correction of the mass center as seen above):
	
	This last expression written successively for hydrogen and deuterium leads to:
	
	where we recall that the mass of the electron $m$ is known to us! 
	
	What is interesting here is that these two elements have identical chemical properties (hydrogen and deuterium) but different spectrum lines. Scientists of the this time wondered why and after that the Bohr hydrogen atom with dragging get available to them they were able to conclude that this difference came from the difference in the mass of the atomic nucleus.
	
	It was still necessary to determine the difference in mass and explain its origin!
	
	We therefore have:
	
	This simplifies to:
	
	Therefore:
	
	Which showed the scientists of that time that the deuterium nucleus consists of two particle mass equivalent to that of the proton. Thus by logical deduction, this core must be made of a proton (which is obviously knows!) and a neutral particle!!!
	
	This hypothesis is this of the "neutron", which was later discovered experimentally in 1932 by Chadwick.
	
	Today we know that:
	
	
	\pagebreak	
	\subsection{Wilson and Sommerfeld's Model}
	
	In developing their model, Sommerfeld and Wilson used classical dynamics to generalize Bohr's model to Keplerian orbits type (thus not only circular but elliptical in the general case!) because the Bohr's model did not explain the splitting of certain spectrum lines (without presence of any electric or magnetic field) that at the time was named "\NewTerm{fine structure}\index{fine structure}".
	
	As we have seen above, in the case of a system with two bodies solicited by a central force, the total energy of the system (we neglect gravitational potential energy...):
	
	To find the expression of the trajectory of the mass m, we will proceed in exactly the same manner used in astronomy (see section Astronomy) to determine the Keplerian orbits.
	
	Thus, we have proved in the section Astronomy that:
	
	With:
	
	We don't need to insist that in our case, it's not anymore a gravitational potential but an electric potential. Which brings us to write to our problem as following:
	
	We still yet have to find the expression of $K$ in a quantified form (according to the Bohr's postulate).
	
	We fill focus first to determine the expression of the focal parameter $p$ of the trajectory:
	
	In our current problem, kinetic and potential energy expressed in polar coordinates give (\SeeChapter{see section Vector Calculus}):
	
	The total energy of the atom is therefore given by:
	
	In an identical way of that of Bohr, Sommerfeld and Wilson applied the same form of quantification for the vector-radius and extended it to the quantification for the azimuthal angle.
	
	Given the angular momentum:
	
	Momentums are obtained by derivation of the Lagrangian relatively to the generalized coordinates (\SeeChapter{see section of Analytical Mechanics)}:
	
	The quantification on the angle is immediate, since $p_\theta$ is a constant of the motion. Indeed, the Lagrangian $L$ being independent of $\theta$ (but no of $\dot{\theta}$), the invariance of the angular momentum results in the following Lagrange equation:
	
	This gives us:
	
	with $n_\theta \in \mathbb{N}^*$ being the "\NewTerm{azimuthal quantum number}\index{azimuthal quantum number}" (i.e. the second quantum number), to remember that it is related to the quantization of the polar angle.
	
	From this last relation we also get:
	
	Let us come back to:
	
	That give us:
	
	Let us focus now to determine the eccentricity $e$ of the path (notation not to be confused with the electrical charge if possible!).
	
	This give us:
	
	To determine the quantization of the angular momentum with respect to the radial variable, we will use a substitution:
	
	By simply noting $r'$ the derivative $\mathrm{d}r/\mathrm{d}\theta$, the integral will be written:
	
	where we used $\dot{\theta}=p_\theta/(mr^2)$ as we have already prove it.
	
	Reporting it:
	
	in the integral of the radial momentum, we get (easy to obtain but you can ask for the details if needed as always):
	
	from which we deduce given $p_\theta=n_\theta\hbar$ that:
	
	which brings us to:
	
	and therefore:
	
	After some elementary algebraic simplifications we finally get:
	
	where $r$, also named "\NewTerm{radial quantum number}\index{radial quantum number}" can be null! Because this is the case when $\dot{r}=0$, that is to say, if the trajectory is a circle (special case of Bohr model!!!).
	
	We introduce then an integer $n$ so named "\NewTerm{principal quantum number}\index{principal quantum number}" as:
	
	with $n\in \mathbb{N}^*$.
	
	Sommerfeld and Wilson the proved that the orbital Bohr model must be determined by these two new quantum numbers:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	For $n=2$ we have two possible sub-orbital:
	
	\end{tcolorbox}
	The value $n_\theta=0$ is impossible by definition because it would mean that the minor axis is zero (ellipse degenerated into a line) and the electron can not pass through the nucleus (in the classical model at least...). So the smallest possible integer value for $n_\theta$ is $1$.
	
	There is therefore $n$ orbits giving the same spectral term. In other words, there are $n$ times the same quantization energy. We also say that the energy level (total) $E_n$ is "\NewTerm{$n$ times degenerated}".
	
	The idea of Sommerfeld was to account the richness of the observed spectra. From this point of view, the results are disappointing: the quantification of all degrees of freedom clearly shows more states (now we need two quantum numbers to completely specify the state, while the Bohr model only considers one) but the additional degree only introduces an energy degeneration (sad!).
	
	To resume this model, so there is exactly the same number of energy levels and therefore the same number of transitions of possible energy states than the Bohr model. At the spectral point of view, the Sommerfeld-Wilson theory does nothing more than that of Bohr except that the orbits are elliptical and therefore does not explain the extent of the observed spectra.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/atomistic/sommerfeld_model.jpg}
		\caption{The eccentric... Sommerfeld approach...}
	\end{figure}
	In fact, the idea is going to be from now to take again the same model but by adding relativistic corrections. The work will necessarily be longer but oh so successful!
	
	\subsection{Relativistic Sommerfeld Model}
	However, the Sommerfeld-Wilson's model can be considered incomplete if we do not take into account changes in parameters engendered by the results of the theory of Special Relativity (\SeeChapter{see section Special Relativity}).
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The section of Quantium Relativistic Quantum Physics is reserved solely to the study of probabilistic quantum physics based on the relativistic version of the Schrödinger equation (so strictly speaking the Relativistic Quantum Physics section should be named "Relativistic Wave Quantum Physics"). This is why it seemed to us more appropriate to put a corpuscular model, deterministic and relativistic (opposite of a wave model, probabilistic and relativistic) such as we will now study in the section of Corpuscual Quantum Physics.
	\end{tcolorbox}
	Indeed, as we have proved in the developement of the Bohr model, the kinetic energy of the electron is given by:
	
	That give us:
	
	For hydrogen and the level $n=1$, we find $v=2.19\cdot 10^{19}\; [\text{m}\text{s}^{-1}]$ and as Michelson-Morley factor (\SeeChapter{see section Special Relativity}):
	
	What the reader can quickly check with the then English version of Microsoft Excel:
	
	\texttt{=1/ROOT(1-(1*(1.60217656E-19)\string^2/(2*8.854187E-12*1*6.62068E-34))\string^2/(299792458)\string^2)}
	
	For sure the change is small but spectrometry values were so accurate that it was necessary to introduce Special Relativity to accommodate these tiny variations and thus validate the theory by experiment.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	As we can easily see it, the relation shows that the particle is more away from the nucleus ($n$ large) more its speed is low. This result has been confirmed experimentally by replacing the electron artificially by a muon and scientists have found that the lifetime of the latter slightly increased with the value of $n$.
	\end{tcolorbox}
	Let us determine in the order of things, the expression of quantification conditions with the relativistic factors. Before we begin, it is important to understand that we consider the nucleus as fixed and as a repository of our system. Thus, compared to the referential, the mass of the electron undergoes a relativistic variation but not the electric potential (we should take into account the variation of the latter - as it is energy - if and only if the repository was the electron itself).
	
	In relativistic dynamics (\SeeChapter{see section Special Relativity}), we proved that the kinetic energy (in the form of Lagrangian notation Lagrangian with "$T$" instead of $E_{\text{cin}}$) is expressed in the form:
	
	The potential energy (in Lagrangian notation with "$V$" instead of $E_{\text{pot}}$) not undergoing relativistic variation, we always have:
	
	The Lagrangien is therefore:
	
	By working in polar coordinates, where the velocity has for expression (\SeeChapter{see section Vectro Calculus}):
	
	Therefore:
	
	The Sommerfeld quantization conditions being:
	
	Now we must seek relativistic expressions for $b_r$ and $b_\theta$.
	
	Let us begin $p_r$:
	
	with:
	
	Therefore:
	
	This gives:
	
	As:
	
	We finally have:
	
	The first quantification condition gives therefore:
	
	For $p_\theta$:
	
	always with:
	
	Therefore:
	
	Which gives:
	
	As:
	
	we finally have:
	
	The second quantification condition is therefore written:
	
	In summary, the conditions ofquantification for the relativistic atom of Sommerfeld are:
	
	We could, seeing the bot above results, conclude a little bit too quickly by thinking it would suffice to multiply the two conditions of quantification by the Michelson-Morley factor relative to the relativistic mass transformation. But, such a shortcut is completely wrong and all but rigorous! Indeed, if you apply this reasoning, it would be sufficient to take the expression of the total energy of the non-relativistic Sommerfeld-Wilson model and introduce everywhere where the mass appearss the Michelson-Morley factor. However the result we get following this way has absolutely nothing in common with the result that we will get further below. So we must always be careful and work as a mathematician: without leapfrogging!
	
	The total relativistic energy of the atom (sum of kinetic energy, mass energy  and the potential energy of the electric field for the whole atom) is given by: 
	
	Indeed, using the notation of the section of Special Relativity:
	
	Now, as part of the study of particle quantum physics, it is (unfortunately) usage to write the rest mass with the symbol of the relativistic mass as:
	
	Therefore:
	
	We must express this total energy in function of the quantization conditions. There's a long mathematical work to do but necessary to achieve the result of our study.

	Given the calculation of the expression:
	
	with:
	
	Squaring:
	
	Therefore:
	
	We add of both sides of the equality $m^2c^2$ (in the idea to include the mass energy as you'll see some lines below), which gives:
	
	By multiplying both sides by $c^2$ we get:
	
	Extracting the square root:
	
	If we introduce this last relation in the expression of the total energy we get:
	
	Now it remains to us to determine the expressions of $N-r$ and $n_\theta$ according to $p_r$ and $p_\theta$.

	The integrale of quantification of the azimuthal angle is immediate:
	
	Therefore:
	
	The integral of quantification of the radius-vector requires a more significant development:
	
	Then come long and joyful mathematical developments...

	Reusing the expression of the total energy:
	
	We get:
	
	By squaring and by making a few changes:
	
	Therefore:
	
	Working on the bracketed term, we will put it as being equal to $A$ as:
	
	By adding and subtracting $2E_\text{tot}m$ and decomposing the term $-m^2c^2$ in $m^2c^2-2m^2c^2$ and then by regrouping:
	
	We will put for the simplification of the calculations (to reduce the number of terms to handle):
	
	Therefore we get:
	
	By taking $m^2c^2$ in evidence, we have:
	
	By adding and subtracting $1$ in the parenthesis:
	
	
	As $E_\text{tot}=E'+mc^2$ we have:
	
	By putting
	
	By putting also:
	
	as $p_\theta=n_\theta\hbar$

	Sommerfeld then introduced what he name the "\NewTerm{finestructure constant $\alpha$}\index{finestructure constant}" defined by the relation:
	
	We will meet this constant again in the section of Particle Physics.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The fine-structure constant is one of the most important physical constants. First, because it is dimensionless, and secondly because it is so far the best known (in terms of accuracy) of all constants and thirdly, because it depends only on terms that seem to be fundamental constants of the atomic world. So physicists and astrophysicists are looking to see if the value of this constant varies over time, which would imply immediately that at least one of the implicit constant is not timeless.
	\end{tcolorbox}
	Given the fine structure constant, we write:
	
	To resume:
	
	With:
	
	So we arrive at the following integral:
	
	The residue theorem (\SeeChapter{see section Complex Analysis}) applied to the preceding integral gives for expression:
	
	We see that there is a trivially a pole at origin $r=0$.

	We will calculate the residue at this point going to the limit $r=0$. We put for this:
	
	Passing to the limit built on the basis of the residue theorem:
	
	The residue corresponding to the pole $r=0$ is then:
	
	We also see that there is a second residue at infinity $r=+\infty$ and to calculate it, we make again a change of variable. We put (according to the method that we saw in the section of Complex  Analysis):
	
	The integral is then written:
	
	To find the residue, we will make a Laurent series expansion of:
	
	around this pole of zero value. To do this, we put:
	
	We know the Taylor expansion (\SeeChapter{see section Sequences and and Series}) of the resulting expression of this variable change:
	
	Applied to the radical, we get:
	
	It comes then automatically the Laurent series (nice!):
	
	we where see quite quick that the pole is of order $2$.
	
	The second residue is the coefficient on $\dfrac{1}{z}$:
	
	Indeed, we have simply applied the relation proved in the section of Complex Analysis:
	
	
	With:
	
	For the calculation of $\dfrac{B}{\sqrt{A}}$ we have:
	
		Therefore the curvilinear integral has for expression:
	
	After simplification:
	
	Finally:
	
	We raise squared and at each line below we simplify and rearrange:
	
	Therefore:
	
	hence:
	
	We put $n=n_r+n_\theta$. By working on the denominator:
	
	we can transform:
	
	By adding and subtracting $n_\theta$:
	
	Therefore:
	
	or also:
	
	or also:
	
	We consider in the term:
	
	the radical that can be written:
	
	Given the series development (\SeeChapter{see section Sequences and Series}) in analogy with the previous relation:
	
	then:
	
	Therefore:
	
	As $\dfrac{\alpha}{n_\theta} \ll 1$, we can neglect the terms above the order $2$ such that:
	
	Finally we can the write:
	
	Working on the term in the brackets we get taking into account our previous result:
	
	Given the following Taylor series development (\SeeChapter{see section Sequences And Series}):
	
	then:
	
	Neglecting terms over order $2$ we have finally:
	
	Therefore the term between braces can be written:
	
	We undertake the Taylor  series development of the right term between braces using again:
	
	Neglecting terms over order $2$ we have finally (an approximation of an approximation that is itself an approximation...):
	
	By developing the square of the third term, we get:
	
	
	The total energy of this atom the becomes:
	
	Finally we get of the expression of the energy:
	
	We can give another expression for the energy of the hydrogenoid atom as:
	
	The expression of the total energy of the hydrogen-like atom becomes then:
	
	Therefore:
	
	In the literature, we find other expressions for the total energy that are more interesting than the previous one (because more traditional). So, considering that $n=n_r+n_\theta$, we get:
	
	If we seek for an expression using to the Rydberg constant $R_H$ (see above) we get:
	
	So the most condensed expression of the total relativistic energy of the hydrogenoid  that we can find in the literature and we adopt in this books is:
	
	The above relation shows well the existence of a fine structure as the characteristics $n_r$ and $n_\theta$of the electron orbit appear separately in a ratio and not just as a sum as in the first model of Sommerfeld and Wilson.
	
	But strictly speaking, we should write because of the nucleus movement:
	
	or:
	
	In which the Rydberg constant ${R'}_H$ is defined as:
	
	However, as the mass of the core is $1,840$ times heavier than that of the electron, we can admit that as a first approximation in the case of the hydrogenous atom:
	
	
	\pagebreak
	\subsubsection{Magnetic dipole moment}
	At the time of the development of the Sommerfeld model, some physicists strive to study another property of the atom. They observed that under the application of the magnetic field, the spectroscopic lines are doubled. To explain this, they had the brilliant and very simple idea  to explain this phenomenon by the magnetic moment of the electron.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We will see during our study of Wave Quantum Physics, that even in the absence of a magnetic field a very thing measuring of the rays shows that they are all double and this because of the "spin-orbit coupling". Therefore, a correct interpretation is that the magnetic field doubles the duplication of rays.
	\end{tcolorbox}
	Thus, given the expression of the norm of the magnetic dipole moment (\SeeChapter{see section Magnetostatics}):
	
	The magnetic moment is equal, in the corpuscular point of view...., to the area enclosed by the orbit of the electron multiplied by the current of the electron (perpendicular to the unit vector of the surface) on its orbit path thus:
	
	where:
	
	is the period of the movement.
	
	We have seen that the total angular momentum being equal to:
     
      thus the ratio magnetic moment/angular momentum gives:
      
      The ration $e/2m$ equation is named the "\NewTerm{orbital gyromagnetic ratio (for a classical rotating body)}\index{orbital gyromagnetic ratio}" equal to:
      
     and the quantity:
     
     is the "\NewTerm{Bohr Magneton}\index{Bohr Magneton}".
    \begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	It is important to remember the few developments and definitions just be made above for when we will develop the Pauli equation in the section of Relativistic Quantum Physics.
	\end{tcolorbox}
	Frequently we write the above relation as well:
    
     where $m_l$ is named "\NewTerm{magnetic quantum number}\index{magnetic quantum number}". 
     
     Knowing that the principal quantum number $n$ is divided by the radial $n_r$ and azimuthal $n_\theta$ quantum numbers, there are so many magnetic moments as there are different orbital geometries for a given value of the principal quantum number. In fact, there are even the double if we consider that the electron can rotate in clockwise or counterclockwise (the magnetic moment is a vector quantity!).
     
     Now let us take two examples using the previous proven rules:
    
     and:
     
    for which we put now:
    
     that is the number that we name "\NewTerm{quantum number of orbital angular momentum}\index{quantum number of orbital angular momentum}" and having values between (as shown in the both previous examples):
     
     What have we finally so far?
     \begin{enumerate}
         \item When $n=1$, we can only have $l=0$ and as $n=1$, there is only one sub-layer, so when applying a magnetic field we still always have one and only one visible line:
         \begin{figure}[H]
			\centering
			\includegraphics{img/atomistic/n_equal_0_orbital_decomposition.jpg}
			\caption{Decomposition of a very low orbital...}
		\end{figure}
			
		\item When $n=2$, we have $l=0$ and $l=1$ and as $n=2$ has two sub-layers. When no magnetic field is applied, the spectrum lines of the two sub-layers are superimposed so indistinguishable (we see only one). But when a magnetic field is applied to the two sub-layers are distinguished by their magnetic moment and therefore we-have two lines but in total there are 3 theoretically (without a field, and two with field):
		\begin{figure}[H]
			\centering
			\includegraphics{img/atomistic/n_equal_1_orbital_decomposition.jpg}
			\caption{Decomposition of an upper orbital...}
		\end{figure}
     \end{enumerate}
	Thus we have:
    
    where for recall:
     
    The potential energy of a magnetic moment $\mu_l$ placed in a magnetic field $B$ is (\SeeChapter{see section Magnetostatics}):
       
    So finally for each orbital electron subjected to a magnetic field, we have:
    
     always with :
     
     The observation of the spectrum of an atom in a magnetic field has the effect of adding spectrum lines by the potential energy of the magnetic moment. This is what we name the "\NewTerm{Zeeman effect}\index{Zeeman effect}" because it is the latter who measured these spectrum lines for the first time (before the theory that we will see later).
     
     \subsubsection{Spin}
     Various experimental facts led to attribute to the electron an intrisic magnetic moment and especially the experiment of the duplication of spectrum lines named "\NewTerm{abnormal Zeeman effect}\index{abnormal Zeeman effect}".

     It has indeed been experimentally measured that the magnetic moment was just equal to the value of the Bohr magneton. It is then tempting to attribute this to the electron magnetic moment and speculate that it might come from the fact that it turns on itself (intrinsic angular momentum), ie it would have a "spin" equal to the Bohr magneton and this latter cold take negative or positive values. We speak then of "\NewTerm{spin quantum number}\index{spin quantum number}" or historically "\NewTerm{Pauli quantum number}\index{Pauli quantum number}" and it gives the number of different values that can take the spin. Therefore the spin is one of two types of angular momentum in quantum mechanics, the other being orbital angular momentum

   However, this classic vision of a intrinsic rotation (intrinsic angular momentum) of the particle is actually too naive and at therefore erroneous.

    Indeed, at first, if the particle is punctual, the notion of intrinsic rotation around its axis is simply devoid of physical sense. Recall that since by definition, the axis of rotation of an object is the locus of points of the object which remain stationary, so if the particle is punctual, its own axis is on the particle, thus the latter is stationary...

   Secondly, if the particle is not punctual, then the rotation has a meaning, but it bring in this case to another difficulty. For example, suppose that the particle is an electron, modeled as a radius of a spherical body. We obtain an estimate of the radius by writing that the mass energy of the electron is of the order of magnitude of its electrostatic potential energy (\SeeChapter{see section Electrostatic}), that is:
   
   The numerical value of this "\NewTerm{classical electron radius}\index{classical electron radius}" is $r_c=10^{-15}$ [m] taking its mass at rest.

    If we attribute then this electron an angular momentum equal to $\hbar/2$ (which has units of angular momentum), we get for a point on the equator of the electron a speed $v$ satisfying:
    
    The numerical value of the speed is so then roughly $v\cong 6\cdot 10^{10}\;[\text{m}\cdot \text{s}^{-1}]$... so the speed is higher than the speed of light in vacuum..., which obviously causes problems with the theory of relativity (\SeeChapter{see section Special Relativity}).

   We can not with the mathematical tools of particle quantum physics rigorously formalize the notion of spin, but we will gome back on this subject in the section of  Relativistic Quantum Physics (especially during our study of the Pauli equation) and show that the spin is in fact (sadly) something much more subtle than a simple rotation...

    But let us come back to our classical point of view. So when we see a duplication of spectrum lines (abnormal Zeeman effect), we assume that this is due to the electron spin $s$ which can take two different directions (vector direction!).

    It has been measured that the intrinsic magnetic moment of the electron is equal to the value of the Bohr magneton, that is:
    
    If we put $e=1,m=1$ (what physicists like to do...) then we have:
         
    (this just to get a similarity with $b=n\hbar$...)

     This value is constant but may be negative or positive depending on the intrinsic rotation direction of the electron relatively to the observer (the angular momentum having a vector direction). So:
    
    and therefore in practice, spin is given as a dimensionless spin quantum number 
     
    This result, of the utmost importance, also leads us to the conclusion that each magnetic quantum number is degenerate twice by the spin quantum number! Thus, as we shall see a little further into concrete examples (with supporting diagrams), each principal quantum number $n$ is say to be "\NewTerm{degenerated}" a number $2n^2$ of times.
    
    The skeptical reader should know that Spin has important theoretical implications and practical applications. Well-established direct applications of spin include (we will come back on some elements of this list in further section but in a very technical point of view):
   \begin{itemize}
      \item Nuclear magnetic resonance (NMR) spectroscopy in chemistry
       \item Electron spin resonance spectroscopy in chemistry and physics
       \item Magnetic resonance imaging (MRI) in medicine, a type of applied NMR, which relies on proton spin density;
       \item Giant magnetoresistive (GMR) drive head technology in modern hard disks.
   \end{itemize}
    Electron spin plays an important role in magnetism, with applications for instance in computer memories. The manipulation of nuclear spin by radiofrequency waves (nuclear magnetic resonance) is important in chemical spectroscopy and medical imaging.
     
    \pagebreak
    \subsubsection{Pauli exclusion principle}
    The Pauli exclusion principle is the quantum mechanical principle that states that two identical fermions (particles with half-integer spin) cannot occupy the same quantum state simultaneously. In the case of electrons, it can be stated as follows: it is impossible for two electrons of a poly-electron atom to have the same values of the four quantum numbers: $n$, the principal quantum number, $l$ the angular momentum quantum number, $m_l$ the magnetic quantum number, and $m_s$ the spin quantum number. For example, if two electrons reside in the same orbital, and if their $n$, $l$, and $m_l$ values are the same, then their $m_s$ must be different, and thus the electrons must have opposite half-integer spins of $1/2$ and $-1/2$. This principle was formulated by Austrian physicist Wolfgang Pauli in 1925 for electrons, and later extended to all fermions with his spin-statistics theorem of 1940.
    
    Particles with an integer spin, or bosons, are not subject to the Pauli exclusion principle: any number of identical bosons can occupy the same quantum state, as with, for instance, photons produced by a laser and Bose–Einstein condensate (\SeeChapter{see section Statistical Mechanics}).
    
    Therefore to resume, due to the fact that the state of an atomic electron can be characterized with at least the following $4$ quantum numbers (the first quantum number coming from Bohr, the two others from Sommerfeld and the last one by Pauli) the we have just proved the origin:
	
	or under the following extended form:
	
	Wolfgang Pauli has then postulate to explain some regularities in the atomic properties (specifically: chemistry) a principle of exclusion today named "\NewTerm{Pauli exclusion principle}\index{Pauli exclusion principle}" and stated as follows:

	In an atom, two electrons can't have the same ordered quadruplet $n,l,m_l,m_s$ of quantum numbers.
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/quantum_numbers_summary.jpg}
		\caption{Summary of quantum numbers}
	\end{figure}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} We sometimes depending on the situations write $s$ rather than $m_s$ (this is not very important...).\\
	
	\textbf{R2.} Once again know from the wave quantum physics that the exclusion principle applies to particles that are "fermions". These are the particles (elementary or composite) which have half-integer spin, such as the proton, neutron and neutrino. This principle does not apply to the group of particles naed "bosons" which have zero or integer spin.
	\end{tcolorbox}
	It is possible from this principle, to establish a kind of catalog of atomic elements from the orbital filling possibilities, assumed layered, improving Mendeleev classification.

	Students often see them for the first time in school when they study chemistry. They use it most of the time, without knowing what they really represent (and also sometimes the teachers...).
	
	\subsection{Electron configuration (atomic orbital)}
	In the years 1920, Niels Bohr, Edmund Clifton Stoner and others designed a model of the electronic structure of atoms that gives the possibility to understand quite well the periodic table of elements. The work of Henry Moseley was used to determine the number of protons in the nucleus and, as the atom is globally neutral, then also the number of orbital electrons. It is not simple to determine the atomic structure and in this analysis, physicists have been helped by the experiments conducted by chemists (as for highly complexed atomes, electrons interacts together!).

	Thus, according to chemists, electrons occupy layers and sub-layers around the core by (average) increasing energy order based on rules associated with their quantum numbers we have previously determined. Thus, the "\NewTerm{electronic configuration}\index{electronic configuration}" is the arrangement of electrons in an atom, molecule or another body. Specifically, it is the position of the electrons in an atomic orbital, molecular or other forms of electronic orbitals.
	
	Knowledge of the electron configuration of different atoms is useful in understanding the structure of the periodic table of elements. The concept is also useful for describing the chemical bonds that hold atoms together. In bulk materials, this same idea helps explain the peculiar properties of lasers and semiconductors.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		Rigorously the concept of "layer" as we can imagine it has absolutely no sense if we refer to the results of quantum mechanics (\SeeChapter{see section Wave Quantum Physics})!!! This is why the debate on how to fill the layers is sterile because it does not strictly exist without a rough approximation of the general rule
	\end{tcolorbox}
	Each layer corresponds to a specific value of the "\NewTerm{principal quantum number $n$}\index{principal quantum number}" and traditionally these layers are designated (this tradition should be abandoned ... but as all the traditions it has a long life...) by the capital letters:
	
	corresponding to the principal numbers $1$, $2$, $3$, $4$, $5$ ... that can take the principal quantum number $n$.
	The "\NewTerm{second/azimuthal quantum number}\index{second/azimuthal quantum number}" conventionally denoted by the letter $l$ corresponds to the degenerate states that can thake the layers (shells) for a given value of $n$ such that:

	For the layer $K$ ($n=1$) we have as we know a unique underlayer (sub-shell):
	
	and therefore for the layer $L$ ($n=2$) we have two sub-layers:
	
	and so for $M$ ($n=3$) we have three sub-layers:
	
	and so on...

	Chemists are accustomed to note the first sub-layers with the Latin letters:
	
	who are the alphabetical equivalent of the azimuthal quantum number $l$. To resume this in a table we have:
	
	\textbf{Definitions (\#\mydef):}
	 \begin{enumerate}
		\item[D1.] An "\NewTerm{electronic layer}\index{electronic layer}" is a group of states that have the same principal quantum number $n$.

		\item[D2.] A "\NewTerm{sub-layer}\index{sub-layer}" or "\NewTerm{sub-shell}\index{sub-shell}" is a smaller group of states which are characterized by the quantum numbers $n$ and $l$.

		\item[D3.] An "\NewTerm{orbital}\index{orbital (electronic)}" is specified by the three numbers quantum $n,l,m_l$ and it can contain two electrons one with spin up and the other with spin down.

		\item[D4.] A "\NewTerm{state}\index{state (electronic)}" is defined by the four quantum numbers $n,l,m_l,m_s$ and contains a single electron as required by the principle of exclusion.
	\end{enumerate}
	Let us summarize in in the form of diagrams that we have seen so far only for the first two main layers beginning with $K$ ($n=1$):
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/electronic_lower_orbit_n1_orgchart_with_spin.jpg}
		\caption{Decomposition of a low orbit with spin}
	\end{figure}
	Thus, the Pauli exclusion principle provides that  $2$ electrons in the layer $K$ are authorized.

	And for $K$ ($n=2$):
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/electronic_lower_orbit_n2_orgchart_with_spin.jpg}
		\caption{Decomposition of a less lower orbit with spin}
	\end{figure}
	Thus, the Pauli exclusion principle provides that  $8$ electrons in the layer $L$ are authorized.
	
	And so on ... We notice that we have in adequation with what we have proved earlier that every layer can contain a number of electrons equal to $2n^2$.
	
	Under the form of a table this gives:
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/electron_configuration_summary_table.jpg}
		\caption{Modern quantum number notation (source: Aco Z. Muradjan)}
	\end{figure}

	Under the form of Bohr diagram according to the naive Bohr model, this gives (the "groups" refers to columns of the periodic table of elements we will see further below):
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/bohr_diagram.jpg}
		\caption{Example of Bohr Diagram (source: ?)}
	\end{figure}
	With a notation consistent with that of chemists, the basic configurations of some elements are then written:
	
	Which is a condensed form of the following corresponding table:
	
	As we said earlier this is not an accurate way as the layers anywas overlap. This is why there exist another way to fill-in the layer (this other way works very well for the ground states of the atoms for the first $18$ elements, then decreasingly well for the following $100$ elements).

	\pagebreak
	\textbf{Definition (\#\mydef):} The "\NewTerm{Aufbau principle}\index{Aufbau principle}" (from the German Aufbau, "building up, construction") states that a maximum of $2$ electrons are put into orbitals in the order of increasing orbital energy: the lowest-energy orbitals are filled before electrons are placed in higher-energy orbitals such that:
	\begin{enumerate}
		\item Orbitals are filled in the order of increasing $n+l$

		\item Where two orbitals have the same value of $n+l$, they are filled in order of increasing $n$
	\end{enumerate}
	This gives the following order for filling the orbitals:
	
	and can be illustrated as following:
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/electronic_configuration_aufbau_principle.jpg}
		\caption{Aufbau Principle}
	\end{figure}
	In this list the orbitals in parentheses are not occupied in the ground state of the heaviest atom now known so far (Uuo, $Z = 118$).
	
	The Aufbau principle can be applied, in a modified form, to the protons and neutrons in the atomic nucleus, as in the shell model of nuclear physics and nuclear chemistry.
	
	And finally so far this is the only image of a nice periodic table that I have been able to found for this book but there exist much more complete and technical one but the quality is awful...:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/atomistic/periodic_table.jpg}
		\caption{Periodic (Mendeleev) table (source: European Synchrotron)}
	\end{figure}
	
	The form of the periodic table is closely related to the electron configuration of the atoms of the elements:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.53]{img/atomistic/periodic_table_structure.jpg}
		\caption{Periodic (Mendeleev) table structure (source: Wikipedia)}
	\end{figure}
	However, although the relativistic model of Sommerfeld is a amazing accurate and consistent model with experimental observations, it does not explain some important phenomena that we observe at the scale of the atom. Thus, this model is unable to explain the disintegration of the elements, the dual (complementary) behavior of matter between wave and particle, the annihilation of matter and antimatter, the split of spectral lines when in presence of a magnetic field and many more.

	These are much more complex developments and at the same time (hopefully!) consistent with what we saw which will be developed in the next section dealing with Wave Quantum Physics that gives the opportunity to explain in a very satisfactory way a large  number of unexplained phenomena that were unexplained at the atomic size (but sill not all as we will see after that we will need Relativistic Quantum Physics).
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{95} & \pbox{20cm}{\score{5}{5} \\ {\tiny 81 votes,  100.00\%}} 
	\end{tabular} 
	\end{flushright}
	
	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Wave Quantum Physics}
	\lettrine[lines=4]{\color{BrickRed}D}aughter of the old quantum theory (\SeeChapter{see section Corpuscular Quantum Physics}), the wave quantum physics known also simply as "\NewTerm{quantum mechanics}\index{quantum mechanics}" is the mainstay of a set of physical theories that we group under the general heading of "\NewTerm{quantum physics}\index{quantum physics}".
	
	This denomination is opposed to that of Classical Physics, this latter failing in its description of the microscopic world (atoms and particles) and in that of certain properties of electromagnetic radiation (typically see the experiences of Young slits in the section of Wave Optics) or semiconductor (typically see the Hall Effect in the section of Electrokinetics).
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The relativistic extension of Qave Quantum Physics is Relativistic Quantum Physics (see section of the same name).
	\end{tcolorbox}
	
	Quantum physics took over and developed the idea of wave-particle duality introduced by De Broglie considerating all material particles (even the atoms) not only as point particles, but as waves, with some spatial extent (\SeeChapter{see section Corpuscular Quantum Physics}). These two aspects wave/corpuscule of particles ("\NewTerm{quanton}\index{quanton}"), mutually exclusive, can not be observed simultaneously as far as we know. If we see a wave property, corpuscular appearance disappears and vice versa.
	
	To this date, no contradiction could be found between the predictions of quantum physics and associated experimental tests. This success has unfortunately a price: the theory is based on a rather abstract mathematical formalism, which makes it quite difficult at first. Hopefully until we not deal with the spin we can be dispensed Heisenberg's matrix formalism to focus on formalism of Schrödinger's wave equation that is much simpler and which to a certain level can get a mental picture of what is happening.
	
	What is even much more difficult is that it is very difficult if not impossible to introduce this field of physics in a linear pedagogical way ... This has the consequence that many books about this subject (including this text), addressed to specialists or not, see their explanations or texts subject to many interpretations criticism, proofreading and supplements.
	
	To find a plausible solution it is favorable to take as basis the "\NewTerm{principle of objectivity}\index{principle of objectivity}" due to Werner Heisenberg, which is the basis of "\NewTerm{standard quantum physics}\index{standard quantum physics}": exists only what is experimentally observable and reproducible. We could also align with the conviction of Max Born to rebuild corpuscular quantum physics that is objectively a potpourri of quantum and classical physics rules to give the place to a new coherent theory based only on y few postulates (Max Born was inspired by the elegance of Albert Einstein's theories that made use of a few well-defined assumptions). Wolfgang Pauli also felt it was essential to cease to sate arbitrary ad hoc hypotheses each time experiments produced data that were disagree with the theory.
	
	The duality principle is accepted by the majority of physicists, but not all. Is an electron in several places at the same time? For this to be admissible, we must have an experience that found them several places at the same time, which is not possible (problem of clock sync) then we are not required to answer the question! Say that it is in several places before we observe is not admissible in physics: principle of objectivity. In general, we'll also give up the notion of trajectory and movement, which will allow to remove the contradiction of radiation by breaking/Bremsstrahlung (\SeeChapter{see section of Electrodynamics}): because there is no more movement in the classical sense. The concepts of speed and acceleration lose all sense at this scale!
	
	A minority of physicists denies this principle and founded a non-standard quantum physics with classical quantities this is why we can find especially in popular scientific papers presentations that deviate from the standard quantum physics (that of the most physicists). This non-standard version gives the same predictions for any feasible experience, so this is a possible model.
	
	Finally quantum physics is a theory considered by the current majority of physicists as unfinished and in which many points still remain rather obscure.
	
	Before we tackle the mathematical part, we would like to indicate to the reader that we will limit ourselves only to theoretical developments made between about 1910 and 1935 (beyond the complexity of theories requires too many pages to a general book as ours). Indeed, we very much believe that Wave Quantum Physics, as Relativistic Quantum Physics and Nuclear Physics are not finished "products", but rather a work in progress. They have developed historically, they continue to be simplified, clarified, expanded and applied through the hard work of physicists who see these theories from different angles. While we present in this book all these theories in a more or less linear fashion, we attempt to provide multiple viewpoints whenever possible.
	
	\subsection{Postulates}
	Unlike most books on the subject, we are pedagogically (not technically!) very unconvinced about the impact of the presentation of the postulates of quantum physics at the beginning of its study in classrooms. We allow ourselves to present our reasons (experiment done):
	
	\begin{enumerate}
		\item They can be deduced from simple mathematical and logical reasoning (elementary algebra and probabilities) based on the postulates of corpuscular quantum physics and the principle of complementarity and therefore can be deduce from an evolution of this latter. Even if the process is rigorously false at least it is pedagogical!
		\item These assumptions are indigestible or even incomprehensible if quantum physics (its formalism and vocabulary) was not initially apprehended by a given number of exercises or a regular use.
	\end{enumerate}
	We can then consider that the only non-provable elements theoretically (to our knowledge) that have their place as assumption would be: De Broglie complementarity principle (we'll talk about it later), the Planck-Einstein relation (already seen in a previous section) and the measurement of an observable.
	
	Nevertheless..., in the objective of the respect of tradition, and especially in respect of scientific methodology, we have chosen to present these assumptions at the beginning of this section but without overemphasize them. However, we strongly recommend the non-initiated reader, to read them without spending too much time to try to understand but just think about them a come back to them regularly for during the read of this section. Afterwards, everything will probably be clear and light will be...

	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We will see practical cases, in this section itself, of quantum theory for later use in the sections of quantum field theory and nuclear physics. However, we advise the reader to read at the same time the sections of Quantum Computing and of Quantum Chemistry and Molecular Chemistry which it seem help greatly in the understanding of some little too theoretical topics presented further below.
	\end{tcolorbox}
	
	\subsubsection{1st Postulate: Quantum State}
	The state of a classical quantum system is specified by the generalized coordinates $(q_1,q_2,...)$ (\SeeChapter{see section Analytical Mechanics}) and is fully described by a finite function differentiable everywhere and denoted in all generality:
	
	named "\NewTerm{state function}\index{state function}" or "\NewTerm{wave function}\index{wave function}", whose squared modulus (multiplication of the function by its complex conjugate) must give the probability density to find instantly the system in the configuration $(q_1,q_2,...)$ at time $t$ (if the system is time dependent):
	
	that we will justify later!
	
	The above relation is named the "\NewTerm{Born rule}\index{Born rule}" or "\NewTerm{Born interpretation}\index{Born interpretation}". The Born rule is one of the key principles of quantum mechanics. There have been many attempts to derive the Born rule from the other assumptions of quantum mechanics, with so far inconclusive results...
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	\textbf{R1.} The fact that we speak of "wave" instead of "particle" comes from the brilliant postulate and in fact quite logical of De Broglie that we name "complementarity principle" (which we will detail later too) and which associates with any material particle, a wave and vice versa.\\
	
	\textbf{R2.} The fact that we deal with probabilities and that they are proportional to the square of the wave function module comes from the Heisenberg uncertainty principle that we will prove further below later and mainly from the experience of Young slits experiment with electrons (\SeeChapter{see section Wave Optics}).
	\end{tcolorbox}
	As a corollary, the particle being necessarily located somewhere in the entire space, we have the normalization condition that the integral over all space is:
	
	to a given phase factor. In other words $\Psi$ must be normalized, what we traditionally name the "\NewTerm{De Broglie normalization condition}\index{De Broglie normalization condition}" (even if a posteriori the concept seems to come from Max Born).
	
	Indeed the Born rule was formulated by Born in a 1926 paper. In this paper, Born solves the Schrödinger equation for a scattering problem and, inspired by Einstein's work on the photoelectric effect,concluded, in a footnote, that the Born rule gives the only possible interpretation of the solution. In 1954, together with Walther Bothe, Born was awarded the Nobel Prize in Physics for this and other work.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	\textbf{R1.} Note that even normalized, $\Psi$ is determined to a given phase factor. In addition, it is preferable for $\Psi$ to be differentiable, because differential operators act on it to get the theoretical predictions on measurable properties, and also finished to be normalizable...\\
	
	\textbf{R2.} When the integral given above provides a finite amount, we say it is "\NewTerm{square integrable}\index{square integrable}". Otherwise, you have to normalize it so that the theoretical model corresponds to reality! We will also discussed it in more detail late (with proofs!). 
	\end{tcolorbox}
	
	Let us recall that a "\NewTerm{phase factor}\index{phase factor}" is a complex constant factor having a unitary module. We can write it (depending on what we have studied in the section on Numbers during our study of complex numbers) $e^{\mathrm{i}\delta}$, where $\delta$ is any angle, named the "\NewTerm{phase}\index{phase}" (\SeeChapter{see section Wave Mechanics}). We will also proved further below rigorously why it has no influence.
	
	We can express this postulate in a little more formal way, because as we will see in several examples, the wave function is often a complex polynomial which can then be expressed in the Hilbert space of polynomials. This gives therefore in the language of Dirac bra-ket formalism (see further below for details) the following definition:
	
	\textbf{Definition (\#\mydef):} The state vector "\NewTerm{ket}\index{ket}" represented by $\Ket{\Psi(t)}$ belonging to the vector space $\mathcal{H}$ (Hilbert space) defines the state of the quantum system at time $t$. This state vector has all the mathematical properties required by quantum physics and especially the dot product of the vector $\Ket{\Psi(t)}$ by its dual vector (complex conjugate) "\NewTerm{bra}\index{kat}" $\Bra{\Psi(t)}$ must satisfy the functional scalar product:
	
	
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The bra-ket notation was introduced by Paul Adrien Maurice Dirac Dirac to facilitate (at least it's supposed to...) the notation of quantum physics equations, but also to highlight the potential vector aspect of the object representing a state quantum. What is specific to quantum physics is that the vectors are not drawn with arrows but with ket and bra (not the bra you can think about...), but it is only a matter of notations and brings nothing mathematically new. Moreover, you should not imagine that we wrote explicitly in the calculations these vectors as column vectors (think to complex numbers.... it is rare that we write them in vector form in fact)!
	\end{tcolorbox}
	
	To summarize these last paragraphs, the two relations:
	
	and:
	
	are therefore equivalent!
	
	\subsubsection{2nd postulate: Time evolution of a quantum state}
	If the system is not disturbed, the evolution (supposed non-relativistic!) of his state is governed by the Schrödinger's equation of evolution (so time dependent!):
	
	This relation simply means that it is the operator "total energy" or "Hamiltonian" $H$ of thesystem, which is responsible for the system to evolve over time. Indeed, the form of the equation shows that, in applying the Hamiltonian to the wave function $\Psi$ of the system, we obtain the derivative with respect to time, that is to say: how it varies over time!
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We will prove further below this relation in details (it will not be trivial unfortunately... but it is possible (!) and so it eliminates the need to define it as a postulate).
	\end{tcolorbox}
	In the latter relation, $H$ is the Hamiltonian operator (total energy) of the system that we will prove further below as having for value in a particular and simple case:
	
	In the case where the potential $\vec{V}(q_1,q_2,...q_n)$ is independent of time (corresponding to a conservative system in classical mechanics), there are (we will see it in various  examples later) a set of independent particular solutions time independent and satisfactory (relation which we will prove the origin later):
	
	where the $\psi_k(q_1,q_2,...)$ are the "\NewTerm{eigenfunctions}\index{eigenfunctions}" (in analogy with the eigenvectors seen in the section Linear Algebra) of the Hamiltonian / operator $H$ with eigenvalue / observable $E_k$.
	
	These particular solutions then describe special states named "\NewTerm{stationary states}\index{stationary states}" (as independent of time...), which we will show later the properties and origin of the name, and which form an orthogonal basis.
	
	The previous eigenvalue equation is often named "\NewTerm{time independent Schrödinger equation}\index{time independent Schrödinger equation}". It defines the stationary states and has a sense of course only that if the system is conservative.
	
	This is especially the time independent Schrödinger equation that concerns quantum chemistry and molecular chemistry (topics we cover in the Chemistry chapter of the book). Indeed, we seek in these fields to get the wave functions describing the stationary states, and especially the state of lowest energy, "\NewTerm{fundamental state}\index{fundamental state}" of atoms and molecules. The transitions observed in spectroscopy are done between these stationary states (as we will prove it further below), their determination is thus a prerequisite for the study of spectroscopy. However, we must remember that it is the Schrödinger evolution equation, which is (at first ...) the fundamental equation of nonrelativistic wave quantum physics: it plays the same role as the Newton equation in Classical Mechanics, or that of an equation of motion (see the proof of the  Ehrenfest's theorem further below).
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	In fact, we will prove (\SeeChapter{see section Relativistic Quantum Physics}) that the Schrödinger evolution equation is only a special case of what we name the "free Klein-Gordon equation" which itself is a special case of the "generalized Klein-Gordon equation", which itself is a limited model relatively to the "linearized Dirac equation" ... in short we have not finished to do maths....
	\end{tcolorbox}
	
	\subsubsection{3rd postulate: Observables and operators}
	For each measurable physical property (an "observable") of a system denoted for example by:
	
	where $q_k$ are the generalized coordinates and $p_k$ the generalized momentum according to notations adopted in the section of Analytical Mechanics, corresponds a linear operator (so it can also be a matrix!) named "\NewTerm{Hermitian operator}\index{Hermitian operator}", frequently noted with a hat such that for for the chosen example above it will be denoted by:
	
	which always intervene in the theoretical calculation of a physically measurable property (see the section Linear Algebra for a recall of what is an Hermitian matrix).
	
	In other words (but this is more related to the postulates that will follow): An observable value is represented by a quantum operator which operates on the wave function to predict the measurement value of the latter observable.
	
	To make ... a Hermitian operator in quantum physics is a mathematical expression as such that if we take its complex conjugate (or his adjoint matrix if the mathematical expression is a matrix!) then the theoretical calculation of the measurable value is always given by the same expression.
	
	Here are the best known examples for which we will prove in details the origin in this section and of this of Relativistic Quantum Physics:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\	
	E1.	Coordinates:
	
	for which we will see a practical example with the Ehrenfest theorem in this section.\\
	
	E2. Momentum:
	
	for which we will also see several practical examples (including one with the Ehrenfest theorem).\\
	
	These two examples are sometimes referred to under the name "\NewTerm{correspondence principle}\index{correspondence principle}".\\
	
	E3. Kinetic momentum:
	

 	E4. Pauli matrices (that as we will see later correspond to the spin operators):
	

	E5. The operator of evolution of energy of a quantum state:
	
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} This may seem to fall from the sky ... but we will see that it just comes naturally when we will later the development sof some very concrete examples or when reading the section of Quantum Computing.\\
	
	\textbf{R2.} As part of this boke, we write indifferently, the operators and observable without the hat symbol (that's the reader to know what we are working with... without being confused...).
	\end{tcolorbox}
	We will see also that some operators are not commutative and obey to what we name "\NewTerm{anticommutation relations}\index{anticommutation relations}" (which are the source of the Heisenberg' principles of uncertainty).
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\	
	Here is an example of such a relation that we will prove later:
	
	then we say that the two components of the two observable position and momentum are "\NewTerm{canonically conjugate}\index{canonically conjugate}".
	\end{tcolorbox}
	We will also see using a practical case that two observables $A$, $B$ whose respective operators commute such as:
	
	possess a common eigenvectors base. We then say that they are simultaneously measurable (the precise determination of one does not preclude the other's) with accuracy (otherwise we have an uncertainty... of Heisenberg). The two quantities $A$, $B$ can then be named "\NewTerm{compatible observable C.O.}\index{compatible observable}" This then means that the two operators should have the same specific eigenfunctions (so they are compatible only if the associated operators admit such common functions).
	
	The set of all attached C.O. to a physical system is a "\NewTerm{complete set of compatible observables C.S.C.O.}\index{complete set of compatible observables}".
	
	\subsubsection{4th postulate: Measure of a property)}	
	The consequence of the previous postulate is that the measurement of $\mathcal{O}$ therefore always gives an eigenvalue of the associated Hermitian operator $\hat{\mathcal{O}}$. In other words, the only observable values of the property $\mathcal{O}$ are the eigenvalues (denoted for example: $o$) of the operator $\hat{\mathcal{O}}$!
	
	
	The eigenvectors and eigenvalues of an operator values have special meaning: the eigenvalues are the values that can result from an ideal measure of this property, the eigenvectors are the quantum states of the system during the measurement.
	
	It is because of this assumption that it is important to ensure that any physical property is represented by a Hermitian operator. 
	\begin{theorem}
	In other words, the hermiticity of $\hat{\mathcal{O}}$ ensures that its eigenvalues (so denoted by example: $o$) are real.
	
	Let's us first do the proof with the usual algebraic notation and then with another approach using the Dirac notation.
	\end{theorem}
	\begin{dem}
	Since an operator is associated with a specific eigenfunction and an eigenvalue by (\SeeChapter{see section Linear Algebra}):
	
	Using the property proved during in our study of complex numbers in the section Numbers like what the complex conjugate product of two complex numbers is the product of two conjugate numbers we have:
		
	If we multiply the first relation by the complex conjugate of $\varphi_k$ and integrate over all space, we have:
	
	and same with the prior previous relation:
	
	As by definition $\hat{\mathcal{O}}$ is Hermitian (therefore equal to its own conjugate complex), both left sides of the previous two relations are equal. Then we have:
	
	Therefore it comes:
	
	and as the integral is physically of square integrable and therefore not zero, this requires that:
	
	and therefore:
	
	and this is only possible if the eigenvalue is real.
	
	Now, with the Dirac notation and with an approach a little bit different:
	
	Since an operator is associated with a specific eigenfunction and eigenvalue by (\SeeChapter{see section Linear Algebra}):
	
	We then have if the eigenfunction is normalized in Dirac notation:
	
	And if the operator $hat{\mathcal{O}}$ is a Hermitian operator (therefore equal to its complex conjugate in the case of a function and equal to the transposed complex in the case of a matrix), we have:
	
	So if the operator is indeed Hermitian we have:
	
	and this can only be satisfy if the eigenvalues are real.
	\begin{flushright}
		$\square$  Q.E.D.
	\end{flushright}
	\end{dem}
	Thus, own hermetic operator values are always real numbers (hopefully ...).
	
	\subsubsection{5th postulate: Average of a property)}	
	This postulate is the less intuitive and more difficult to prove (we will prove with an example when studying the Ehrenfest theorem). His statement is as follows:

	The mean value (expected) of a physical property $\mathcal{O}$, when the system is in the state described by the normalized function $\Psi$ is given by (do not confuse the notation of the average of an operator with that of the complex conjugate of an eigenvalue !!!):
	
	An equivalent expression quite difficult to readis the following: the probability of finding the eigenvalue $o_k$ (of the operator $\hat{\mathcal{O}}$ predominantly Hermitian), when measuring the property $\mathcal{O}$ at time $t$ on a quantum system prepared in state described by the function $\Psi$ is given by the square of module of the projection of the function $\Psi$ on the eigenfunction $\varphi_k$ associated to the eigenvalue $o_k$ (and its operator):
	
	where the "\NewTerm{projection}\index{projection}" (or "\NewTerm{representative}\index{representative}") is defined by:
	
	the index $k$ being here to indicate that there may be some for some operators several eigenvalues and eigenvectors.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We come back on this formalism and relations further below. They are also several practical examples provided in the section of Quantum Computing and a single and very nice example at the end of the section of Quantum Chemistry (for the mean radius and the mean angular momentum).
	\end{tcolorbox}
	For example, in one dimension and for a system that is time dependent, we use the operators presented in the previous postulate (but on which we will also return in detail):
	
	For which we will see practical examples in the sections already mentioned!
	
	\pagebreak
	\subsection{Classical principles of uncertainty}
	Before attacking directly quantum physics in front and the corresponding mathematical tools in-deep (and the pseudo-proofs of the five postulates), we must first introduce a simple classic example in which appears a special type of phenomena: the intrinsic presence of uncertainty in any measurement (as it is intrinsic we don't speak of statistical uncertainty as we have already study in the section Statistics!).
	
	This study in classical form is fundamentally not very rigorous, will normally help the average reader to better understand the quantum uncertainty (at least we hope so...) that we will study and determine further below and that will need no previous experimental considerations! However it say that it way also really one of the approaches used by Heisenberg himself!
	
	Imagine that we would like to measure through a microscope the abscissa $x$ of a particle and the components of its momentum $\vec{p}$. For the measurement of $x$ to be possible, there must be a monochromatic light beam (to simplify...) parallel to the $x$-axis comes illuminate the particle, and that at least one photon collides the particle and reaches the eye of the observer: 
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/classical_incertitude_experiment.jpg}
		\caption{Heisenberg's microscope}
	\end{figure}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Heisenberg's microscope exists only as a thought experiment, one that was proposed by Werner Heisenberg, criticized by his mentor Niels Bohr, and subsequently served as the nucleus of some commonly held ideas, and misunderstandings, about Quantum Mechanics. In particular, it provided an argument for the uncertainty principle on the basis of the principles of classical optics. While the act of measurement does lead to uncertainty, the loss of precision is less than that predicted by Heisenberg's argument when measured at the level of an individual state. The formal mathematical result remains valid.
	\end{tcolorbox}
	Once $x$ measured, we can imagine any method for measuring the linear momentum.

	Let us put $\alpha$ as the angle that do the direction of the photon after collision with the $z$-axis. Let us suppose to simplify the calculations that the particle has a relatively high mass so that we can neglect the energy change of the photon. We see that after the collision, the components of the linear momentum of the scattered photon following respectively the $x$-axis and $z$-axis are:
	
	Indeed, let us recall that the relations between the electromagnetic waves, the mass-energy equivalence and the linear momentum (\SeeChapter{see section Special Relativity}) are:
	
	It follows that the particle will see its amount of linear momentum altered. The components of the variation are then (remember that initially it was zero following the $z$-axes) those of the variation of the photon following:
	
	between it initial and final linear momentum.

	The only information we have about the angle $\alpha$ is that it is, in absolute value less than or equal to the opening angle $u$ of the microscope objective (technical restrictions).

	This implies that:
	
	
	\subsubsection{First classical uncertainty relation}
	When we will have measured the linear momentum $\vec{p}$ at the end of the experiment, we just saw that we will have to make the corrections:
	
	of the linear momentum of the photon to know the true value of $\vec{p}$ of the particule just before the start of measurement.

	In these corrections, there is an unknown portion corresponding to measurement errors on $p_x$ and $p_z$ of the photon. It is possible to establish that the maximum error $\Delta p_x$ and $\Delta p_z$ on the initial linear momentum is given by the $x$ component of the "\NewTerm{first classical Heisenberg's uncertainty relation}\index{first classical Heisenberg's uncertainty relation}":
	
	as we have $|\sin(\alpha)|<\sin(u)$. So it is like a worst error!
	
	\subsubsection{Second classical uncertainty relation}
	Let us now see what we can say about the measurement of the position of the particle.

	Let us recall now that (\SeeChapter{see section Wave Optics}) for a rectangular aperture we have by putting $n=1$:
	
	where $\theta$ (in Wave Optics) is the angle to clearly distinguish two diffraction minima (and therefore clearly an object emitting radiation between the same two points). Conversely, from the viewpoint of diffraction, the opening width $e$ is given by:
	
	The value of $e$ may also be seen as the vision area (orthogonal projection of the rectangular aperture on the $x$ axis) of width $x=e$ of the particle. Therefore:
	
	Just as the maximum error of the linear momentum is given by the condition $|\sin(\alpha)|<\sin(u)$, we can also write  $|\sin(\theta)|<\sin(u)$, which leads us to write:
	
	If we multiply the following relation we just proved before:
	
	with the prior-previous one, we get:
	
	After simplification we get the "second classical Heisenberg's uncertainty relation" also named "\NewTerm{classical spatial uncertainty}\index{classical spatial uncertainty}":
	
	which thus represents the maximum experimental error of a small rectangular aperture opening of a microscope. Many quantum physics books show that we fall back on exactly the same expression in many situations.
	
	In his celebrated 1927 paper, "\textit{Über den anschaulichen Inhalt der quantentheoretischen Kinematik und Mechanik}"), Werner Heisenberg established this expression as the minimum amount of unavoidable momentum disturbance caused by any position measurement, but he did not give a precise definition for the uncertainties $\Delta x$ and $\Delta p$. Instead, he gave some plausible estimates in each case separately.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The reader will easily verify that this relation applied to for a macroscopic object (size of the order of the centimeter), which position would be measurable with a precision of the micrometer gives a ridiculously low uncertainty of linear momentum and thus for the speed. But (!) the same relation applied to the mass of a particle such as that of an electron with a position measurement accuracy assumed of a tenth of a nanometer give an uncertainty about the speed of about $1,000\;[\text{m}\cdot\text{s}^{-1}]$!!\\

	Thus, if we try to locate a particle with an accuracy always bigger, its momentum reaches extreme values. At some point, the magnitude of the linear momentum can be so big that the corresponding energy is sufficient to produce a particle-antiparticle pair. In other words, if we try to confine a particle in an increasingly small box on the one hand, we know less and less its linear momentum and from a certain threshold, we do not even know not how many particles there are in the box!
	\end{tcolorbox}
	However (!), We will during our study of commutator applied to the quantum physics theory that the true uncertainty relation (whose value differs from the one above) only appears naturally from mathematical properties and the definition of the linear momentum.

	More generally, for a particle in a volume  of dimensions $x, y, z$, a vector state is characterized by six quantities $(x,y,z,p_x,p_y,p_z)$ in the phase space (phase space which is therefore of $6$ dimensions) and the quantum state occupies the "cube" of volume:
	
	What is remarkable in this simplistic approach is that the Planck constant naturally emerges as the unit of minimum universal measurement of uncertainty of experimental physics through the wave-particle dualism! Heisenberg wrote that this result establishes the ultimate failure of causality in quantum physics.
	
	\subsubsection{Third classical uncertainty relation}
	In special relativity, we saw that $(x, y, z, ct)$ are the components of a space-time four-vector and also $(p_x,p_y,p_z,Ec^{-1})$ those of a vector of a four energy-momentum vector.

	It is therefore natural to complete the three spatial relation of the type $\Delta x \Delta p_x=h$ by extension:
	
	We obtain thus roughly the "\NewTerm{third classical uncertainty relation}\index{third classical uncertainty relation}" also named "\NewTerm{classic temporal uncertainty}\index{classic temporal uncertainty}":
	
	However (!), we will see during our study of commutators applied to the theory of quantum physics, that this uncertainty relation (whose value differs from the one above) also appears naturally from only the mathematical properties and the definition of the linear momentum.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We will come back later on the implications of this temporal uncertainty whose implications are the basis of quantum cosmology (and the creation of our Universe... only this!) and also of quantum field theory and in particular regarding to the Yukawa potential (\SeeChapter{see section of Quantum Field Theory}).
	\end{tcolorbox}
	The established classical uncertainties relation will allow us to better understand the "real" uncertainties relations in their modern quantum form. For this, among others, we will have to make use of the necessary mathematical artillery. However, for the sake of clarity, we wanted to present the wave quantum physics in the simplest and least formal way possible as always in this book. This presentation may perhaps take the reader to many misinterpretations, he must therefore remain cautious as long as it has not seen the rigorous proof!
	
	Let us now come back as promise in the section of Magnetostatics on the Wilson bubble chamber that obviously seems to be incompatible with Heisenberg' inequalities: 
	\begin{figure}[H]
		\centering
		\includegraphics{img/electromagnetism/bubble_chamber.jpg}
		\caption[]{Wilson Bubble Chamber (source: CERN)}
	\end{figure}
	The issue of having the possibility to track a particle with such precision and on such a long distance that seems to violate both: Heisenberg's inertitude principle and Wave-Particle duality is named in generl the "\NewTerm{Mott problem}\footnote{So a Mott problem is a paradox that illustrates some of the difficulties of understanding the nature of wave function collapse and measurement in quantum mechanics}\index{Mott problem}".

	The problem was first formulated in 1929 by Sir Nevill Francis Mott and Werner Heisenberg, illustrating the paradox of the collapse of a spherically symmetric wave function into the linear tracks seen in a bubble chamber.

	In practice, virtually all high energy physics experiments, such as those conducted at particle colliders, involve wave functions which are inherently spherical. Yet, when the results of a particle collision are detected, they are invariably in the form of linear tracks (see, for example, the illustrations accompanying the article on bubble chambers). It is somewhat strange to think that a spherically symmetric wave function should be observed as a straight track, and yet, this occurs on a daily basis in all particle collider experiments.

	A related variant formulation was given in 1953 by Mauritius Renninger, and is now known as "\NewTerm{Renninger's negative-result gedanken experiment}\index{Renninger's negative-result gedanken experiment}". In this formulation, it is noted that the absence of a particle detection can also constitute a quantum measurement; namely, that a measurement can be performed even if no particle whatsoever is detected.

	In the original 1929 formulation by Mott and Heisenberg, the spherical wave function of an alpha ray emitted from the decay of a radioactive atomic nucleus was considered. It was noted that the result of such a decay is always observed as linear tracks seen in Wilson's cloud chamber. Intuitively, one might think that such a wave function should randomly ionize atoms throughout the cloud chamber, but this is not the case. Mott demonstrated that by considering the interaction in configuration space, where all of the atoms of the cloud chamber play a role, it is overwhelmingly probable that all of the condensed droplets in the cloud chamber will lie close to the same straight line. What is uncertain is which straight line the wave packet will reduce to; the probability distribution of straight tracks is spherically symmetric. 
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Rigorously the "Heisenberg's incertitude principles" should be named "Heisenberg's indetermination theorem" as they can be proven (therefore it is not a principle!) and they are not a limit of the instrumentation of a limit of precision of Nature itself (hence the "indetermination" in place of the "incertitude").\\
	
	But this is a detail as many many statements in physics are provable  and keep the name "principle" because at the time they were discovered they were not provable!
	\end{tcolorbox}

	\pagebreak
	\subsection{Quantum algebra}
	Under uncommon and unofficial term of "quantum algebra" (so do not to abuse it!) we wish to introduce and recall to the reader some mathematical tools  that will be very useful to solve certain equations of Quantum Physics. It is therefore of prime importance to understand (or to have understand  regarding ot the reminders) at best what will follow!
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Purists may climb the walls when reading what follows ...
	\end{tcolorbox}
	
	\subsubsection{Linear functional operators}
	\textbf{Definition (\#\mydef):} The "\NewTerm{linear operators}\index{linear operators}" are mathematical entities acting on functions or vectors (\SeeChapter{see section Vector Calculus}).

	The functions on which can operate on these operators can be functions of a single variable $x$, such as $f (x)$, or of three coordinates of a point $x, y, z$ such as $f (x, y, z)$ or written briefly $f(\vec{x})$.

	We will have to write integrals of these functions, which are often extended to the entire space. In the case of a function of the three-dimensional coordinates of a point, we will adopt the following notation that we already know:
	
	These notations, that are quite essential to simplify the expressions that we will meet in quantum physics being established, we can go back to our operators.

	Starting from a function $f$, if we associate to it a function $g$ of the same kind, that is to say dependent on the same variables, we can say that $g$ is the result of the action of an operator $\alpha$ on $f$ and write this symbolically as a single product:
	
	So $\alpha$ multiply, or better "acts on", $f$.
	
	But we introduce immediately a fundamental restriction: only interest us linear operators (as as in Linear Algebra...), that is to say, such as:
	
	whatever the coefficients $\lambda_1$ and $\lambda_2$

	A very simple category of operator consists simply of the numbers (real or complex scalars). Thus, in the relation:
	
	$\chi$ is a function that depends linearly of $f$, trough a linear operator that we write $\lambda$, $\lambda$ being a number (typically the position operator in quantum physics).

	There are two important special cases to consider:
	\begin{enumerate}
		\item Zero operator: $\lambda=0$ where $\chi=0\cdot f=0$ will be obviously a function equal to zero everywhere...

		\item Unit operator (or identity operator): $\lambda=1$ where $\chi=1\cdot f=1$ (which is just as simple...)
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The "Nabla" operator $\nabla$ is also a functional linear operator (we will see that a bit later) that in quantum physics is found in the energy operator.
	\end{tcolorbox}
	We also check easily for functional operators that they are (see the sections of Set Theory and Linear Algebra if necessary):
	\begin{itemize}
		\item Commutative with respect to the addition
	
		\item Associations in relation to the addition and multiplication
		
		\item Bistributive over addition left and right
	\end{itemize}
	So far, nothing distinguishes operators algebra of that of numbers. But there are however two properties that we must always keep in mind to not to make mistakes when we make calculation with operators:
	\begin{enumerate}
		\item Two operators do not commute in general compared to the multiplication (as in linear algebra ...), that is to say that in general given two functional operators $\alpha$ and $\beta$:
		

		\item If we encounter an expression such as $\alpha_1\beta+\beta\alpha_2$, so we do not have the right to do in general a factorization (there is therefore a particular structure group that is non-commutative)!
	\end{enumerate}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	A simple and important example, as useful for what will follow (very close to a practical case that we will see later), of two operators that do not commute with a function of a single variable is as follows (where f is any) . 

	Let us consider the operator $\mathrm{d}/\mathrm{d}x$ acting on $xf(x)$:
	
	simplifying by $f$:
	
	So we have above an example of two operators that do not commute since:
	
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} If an operator can switch anyhow with another operator is that it is a number (it joined with the concept of measurement which we have mentioned in the postulates).\\
	
	\textbf{R2.} When a state (a mathematical function in the formal sense) is unchanged by an operator, the sate is then named "\NewTerm{eigenstate}\index{eigenstate}" or "\NewTerm{eigenvector}\index{eigenvector}\index{eigenvector}" of the system (we will see practical examples below). The state is then perfectly measurable and is assimilated to the classical observable.
	\end{tcolorbox}
	Before going to the next subject, to close this introduction on linear functional operators, consider that we know the three-dimensional Schrödinger equation (which we will prove further below):
	
	or written differently (that is more aesthetic ...) with the Laplacian of $\vec{\nabla}^2=\Delta$ of a scalar field (\SeeChapter{see section Vector Calculus}):
	
	or after rearranging:
	
	or after simplification:
	
	Then the total energy operator  (the Hamiltonian $H$ in other words ...) is expressed as:
	
	or in Hamiltonian notation:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Here we fall back naturally on the second expression given in the second postulate but the notation $V$ for the potential energy can be sometimes confusing with the electrical potential for beginners that have never deal too much with Analytical Mechanics.
	\end{tcolorbox}
	On the other hand, we know that:
	
	The last two expressions must be identical. The only way to meet these conditions is to put:
	
	which are the "\NewTerm{Hermitian operators of linear momentum}\index{Hermitian operators of linear momentum}" in wave quantum physics (expressed here in Cartesian coordinates) and that need to be remembered throughout this section of the book!
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Here we fall back naturally one one of the operators listed in the third postulate.
	\end{tcolorbox}
	We can check the legitimacy of these operators by re-injecting them into the expression of the kinetic energy:
	
	Moreover, it is easy to verify that this development is remain correct if we take the complex conjugate of the linear momentum operator!!!

	Thus, the total energy operator $H$ (Hamiltonian) is, also, well Hermitean! This result is very important to check such calculations using the orthogonality property of eigenfunctions that we will see further below.
	
	\paragraph{Hermitian and Self-adjoint operators}\mbox{}\\\\\
	Caution!!!! Reading the following lines could be quite abstract... However, if you do not understand much this is not very important because often everything will become evident during the study and the developments of concrete examples that will be give later, after which you will be able to reread the following and understand it at the same time.
	
	Let us consider the two following integrals extended to the whole space (inside the integral it is a multiplication of functions and operators) without trying to understand their utility for the moment:
	
	where let us we recall that the notation $\bar{f}$ is the complex conjugate of $f$. It should be noted that in these two integrals, $\alpha$ and $\beta$ represent operators.
	
	We observe in the developments of quantum physics that these two integrals are equal (and for $Q$ and $\bar{Q}$ to be equal we must therefore have $Q\in\mathbb{R}$) and that there is a one-to-one correspondence between the operators $\alpha$ and $\beta$, we say then that $\beta$ is the "\NewTerm{adjoint operator}\index{adjoint operator}" of $\alpha$ (the transpose of the conjugate) or that it is the "\NewTerm{hermitian}" of $\alpha$ (both terms are customary) and we write:
	
	if the two previous integrals are satisfied.

	From this definition, we deduce the following important identity:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider the operator:
	
	Then by integration by parts:
	
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We will prove the relation above in a concrete but particular example in our study of Quantum Field Theory (see the next section) and we will return to this in a more rigorous way in our presentation of the Dirac formalism in the section Relativistic Quantum Physics.
	\end{tcolorbox}
	
	The adjoint operator has several properties, the only ones that will interest us in this section are:
	\begin{enumerate}
		\item[P1.] $(\alpha^\dagger)^\dagger$ which it is unnecessary to proved, since this relation comes from the very definition of the adjoint operator.
	
		\item[P2.] $\alpha$ being considered as a complex number (special case of operator such as the linear momentum one that we have seen before) then we have $\alpha^\dagger=\bar{\alpha}$.
	\end{enumerate}
	An extremely important category of operators is thus constituted by the "\NewTerm{self-adjoint Hermitian operators}\index{self-adjoint Hermitian operators}", or simply "\NewTerm{self-adjoint operators}" equal by definition to their adjoint:
	
	It must be known to the reader that sadly some teachers and physicists say	that "self-adjoint" and "hermitian" are equivalent concepts. This is an abusive language used by physicists that you will never found by mathematicians. 
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Remember that these operators can also be matrices!
	\end{tcolorbox}
	We also notice that if we take a Hermitian operator (such as that of the linear momentum to make a simple example) and that we multiply that latter by the unitary imaginary number $\mathrm{i}$ then it becomes non-Hermitian (being not in $\mathbb{C}$ anymore) but at the same time it becomes self-adjoint as it will be a real operator (belonging to $\mathbb{R}$).
	\begin{theorem}
	Any operator, that we will denote by $\xi$, can be decomposed in a unique way into Hermitian (self-ajdoint) $\alpha$ and non-Hermitian (non-self-adhoint) $\mathrm{i}\beta$ parts, that is, we can write:
	
	where $\alpha$ and $\beta$ are hermitians (self-adjoints).
	\end{theorem}

	\begin{dem}
	If:
	
	then:
	
	as it is a simple complex number, then:
	
	The sum or the difference of the operator and his adjoint is therefore a self-adjoint operator (the sum or subtraction between self-adjoints operators, hence remains self-adjoints).
	\begin{flushright}
		$\square$  Q.E.D.
	\end{flushright}
	\end{dem}
	In general, it is quite obvious that the product of two self-adjoints operators $\alpha$ and $\beta$  is not necessarily itself self-adjoint operator, that is:
	
 	since we can verify that the condition for which the product of two self-adjoint operators is itself self-adjoint is that the two operators "commute" (see below):
 	\begin{theorem}
	The product of two self-adjoint operators is self-adjoint if and only if the two operators commute.
	\end{theorem}

	\begin{dem}
	First remember that by definition and explicitly:
	
	Therefore (\SeeChapter{see section linear Algebra}):
	
	Therefore we see that:
	
	if and only if $\alpha^\dagger$ and $\beta^\dagger$ commute or equivalently, if and only if
	
	That is to say if and only if:
	
	\begin{flushright}
		$\square$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\pagebreak
	\paragraph{Commutators and Anticommutators}\mbox{}\\\\\
	\textbf{Definitions (\#\mydef):}
	\begin{itemize}
		\item[D1.] The "\NewTerm{commutator}\index{commutator}" of two operators $\alpha$ and $\beta$, is written:
		
		\item[D2.] The "\NewTerm{anticommutator}\index{anticommutator}" of two operators $\alpha$ and $\beta$, is written:
		
		Which is generally written in the following form:
		
	\end{itemize}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} As the commutator is much more common in developments than the anti-commutator, if there is no possible confusion, we denote it simple $[\alpha,\beta]$.\\
	
	\textbf{R2.} Concrete and trivial examples of these commutators in our study of Qantum Physics will be presented in the text that follows.
	\end{tcolorbox}
	Let us mention some obvious properties of commutators (those that we will use the most in this book):
	\begin{enumerate}
		\item[P1.] Anticommutativity:
		

		\item[P2.] Left-linearity:
		

		\item[P3.] Right-linearity:
		
	\end{enumerate}
	where $\lambda_1$, $\lambda_2$ are any number (proofs are made - if necessary - during the development of practical examples further below).
	
	Let us look for the adjoint of $[\alpha,\beta]$:
	
	Hence a very simple result:
	
	The following relation is very useful in practice (trivial, but as usual if necessary we can add the detailed proof on request):
	
	We also have:
	
	We will prove later with a concrete case that if two operators do not commute, then it is impossible to have a state having a precise and unique value for both operators at the same time (in quantum physics there is an experimental configuration where the first operator represents the linear momentum and the second the spatial coordinate), this is why the operators are often referred to as "observables".
	
	Let us take a moment to look at a concrete example of commutators and whose one of the results is absolutely fundamental!

	We have proved earlier above the relations:
	
	Consider the relation (simple usual mathematical differential):
	
	If we divide by $\psi$ on both sides of the equality and then multiply by $\hbar/\mathrm{i}$, then we get:
	
	what give us:
	
	therefore it comes that the commutator of $x$ and $p_x$ is equal to $\mathrm{i}\hbar$ and therefore that the quantities do not commute. We therefore have the following non-commuting relation:
	
	that we often find in literature in the form (with the symbol of Kronecker):
	
	Thus (in the context of the second postulate), the two observables $x$ and $p_x$, whose operators do not commute, do not have a common eigenvector basis. They are therefore not simultaneously measurable with precision and therefore constitute an Heisenberg uncertainty (see proof after the remark)!
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} The abbreviation (cycl.) Mmeans that the letters ($x$, $y$, $z$) can be circularly changed and the result remains the same.\\
	
	\textbf{R2.} Although this result may seem surprising, it is none the less extremely correct as it results from mathematical reasoning. We can not make it more simpler and rigorous.\\
	\end{tcolorbox}
	\begin{theorem}
	Operators that do not commute, do not have a common eigenvector (eigenfunction) basis or conversely operators that do commute, have a common eigenvector (eigenfunction) basis
	\end{theorem}

	\begin{dem}
	Let us consider two operators $A$ and $B$ which represent observables and are therefore Hermitian. If there is a $\psi$ such that:
	
	then $\psi$ is a simultaneous (common) eigenfunction (eigenvector) of $A$ and $B$, belonging to eigenvalues $a$ and $b$, respectively. Hence:
	
	and therefore:
	
	\begin{flushright}
		$\square$  Q.E.D.
	\end{flushright}
	\end{dem}
	Let us now also consider the relation:
	
	and proceeding in the same way as before, we get:
	
	The two relations:
	
	can be summarized as follows:
	
	using the coordinates and generalized moments (\SeeChapter{see section Analytical Mechanics}) and are remarkable from several point of views:
	\begin{enumerate}
		\item First, because from purely theoretical and mathematical considerations we fall back in Quantum Physics an equivalent (but not equal) uncertainty to that obtained during our study of the Heisenberg principles of uncertainty seen earlier above (which we recall had been obtained From a classical practical case).

		Indeed, if we take the module of the left commutator, then we get the "\NewTerm{spatial uncertainty relation of Heisenberg}\index{spatial uncertainty relation of Heisenberg}":
		
		which for recall, can also be written in the form:
		
		The Planck constant being extremely small, this explains why this effect is impossible to detect on our macroscopic scale. By cons, the mass of the electrons being extremely small too, the above fraction becomes noticeable for an electron and the effect of this uncertainty is then important!
		
		Finally, by the commutation of the components of the linear momentum quadrivector (\SeeChapter{see section Special Relativity}), we get the "\NewTerm{temporal uncertainty relation of Heisenberg}\index{temporal uncertainty relation of Heisenberg}":
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		With the used of advanced mathematics we can prove that in fact a factor $1/2$ is missing in front of the $\hbar$.
		\end{tcolorbox}
		There is a quite fantastic consequence from the uncertainty about time and energy and relativity. Imagine the most complete vacuum (quantum vacuum) and suppose that we look at what happens at a given point in space for a very short time. Then the principle of temporal uncertainty tells us that the energy of this state (the vacuum!) is very imprecise. But Special Relativity says that energy is also mass (and therefore also a field), and therefore particles. So, during this very short time particles can spontaneously appear from the vacuum! We name them "\NewTerm{virtual particles}\index{virtual particles}" because they disappear very quickly and are generated by the "\NewTerm{vacuum quantum fluctuations}\index{vacuum quantum fluctuations}". We will come back on this later in the section Quantum Field Theory.
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.4]{img/atomistic/quantum_fluctuation.jpg}
			\caption{Conceptual representation of vacuum quantum fluctuations (source: ?)}
		\end{figure}
		However (!!!) the above "\NewTerm{time-energy uncertainty relation}\index{time-energy uncertainty relation}" (and other time-"observable" uncertainty relations that can be constructed) is (considered) not to have same meaning as previous "\NewTerm{canonical uncertainty relations}". Meaning uncertainty relations constructed from canonical dynamical variables/observables (in the Hamiltonian sense), like position and momentum, since time parameter is not an observable and also not an operator in Quantum Mechanics or Quantum Field Theory formalisms.
	
		In fact, there are various approaches and interpretations of time-energy uncertainty. For example:
		\begin{itemize}
			\item Energy-dispersion ($\Delta E$) of a state and lifetime ($\Delta t$) of the state itself
			\item Energy exchange ($\Delta E$) and time-frame ($\Delta t$) during which this can happen
			\item Energy measurement ($\Delta E$) and time ($\Delta t$) it needs for accuracy (although this is rigorously disputed)
		\end{itemize}
		..other similar or specialized formulations of the above.
		
		\item Secondly, these relations are remarkable because uncertainty is a complex value. This leads us to consider that the set $\mathbb{C}$ of complexes number is inherent to the real structure of our Univers (space-time) at the level of the quantum world (as confirm it the Casimir effect where the sum to infinity of integers prolongation with the Riemann Zeta function is equal to $-1/12$ as seen in the section of Sequences and Series). The quantum world is therefore a world of complex uncertainty. And this probability does not seem to be a consequence of our vagueness or ignorance but seems to be an intrinsic property of Nature!
	\end{enumerate}
	
	It also interesting to notice that time plays an unusual and subtle role in quantum physics. Unlike position, time is not usually treated as an operator; rather it is a simple parameter!
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The relations and properties of commutators and anti-commutators will be indispensable to develop the quantified theory of angular momentum and spin further below.
	\end{tcolorbox}
	
	\paragraph{Representatives}\mbox{}\\\\\
	Let us now introduce contemporary quantum notations, which we will consider for now as abbreviations of integrals relating to wave functions, we will write (for the future purpose of calculating probability densities):
	
	since it is a complex functional dot product (\SeeChapter{see section Functional Analysis and Vector Calculus}).

	With this notation, the relation that we presented earlier in our study of operators:
	
	becomes (it is lighter already ... but less pedagogical):
	
	That said, the set $E$ of the functions $\psi$ that interest us in Wave Quantum Physics constitutes a linear functional space. Indeed, in Quantum Physics, the differential equations that we have to solve (Schrödinger's equation) to describe the behavior of a particle are such that the general solution can be very often decomposed into the sum of the particular solutions (we will prove this later!) . In mathematics, we say that "the states are linear", that is, every combination of states is still a state!
	
	Thus, the state of a particle is, as will be proved later, represented by a "\NewTerm{quantum state}\index{quantum state}" or a "\NewTerm{state vector}\index{state vector}" denoted $|\Psi\rangle$ which also corresponds to a mathematical function describing it completely.

	For example, if $|\Psi_1\rangle$ and $|\Psi_1\rangle$ are two possible states, then:
	
	is also a possible state for the system (due to the property of linear functional  spaces).

	Let us now return to our linear functional  space (or "\NewTerm{linear space of states}\index{linear space of states}"). The fact that the set $E$ of the functions $\psi$ that interest us constitutes a linear functional space means that if $\psi\in E$, $\chi\in E$ we also have:
	
	regardless of the coefficients $\lambda$ and $\mu$ (\SeeChapter{see section Vector Calculus}).
	
	where for recall, $\delta_{ij}$ is the Kronecker symbol (\SeeChapter{see section Tensor Calculus}).
	
	\textbf{Definition (\#\mydef):}
	The base is name a "\NewTerm{complete base}\index{complete}" if of course any $\psi\in E$ can be develop in series of eigenfuctions such as:
	
	where $c_i$ is an arbitrary number (it is in part here that we have to return back to the $4$ and $5$ postulates of Wave Quantum Physics).

Let us now calculate the functional scalar product (\SeeChapter{see section Functional Analysis}):
	
	This last relation shows that we have identically (we change the notation of the indices):
	
	Thus, in a complete orthonormal basis $(\varphi_i$, an function $\psi$ will be well described by the data of the coefficients $c_i$. It will often be useful to put them in the format of the representative matrix of $\psi$ in the basis $(\varphi_i)$:
	
	Let us consider now an operator $\alpha$ such that:
	
	But we can also write (notice the apostrophe in the relation!):
	
	Let us multiply this last relation by $\bar{\varphi}_j$ and calculate the functional scalar product:
	
	To be compared with (obtained above):
	
	By noting $A_{ij}$, the "\NewTerm{representative matrix}" of $\alpha$ in the basis $\varphi_i$, we can thanks to the relation:
	
	write finally:
	
	
	\paragraph{Eigenvalues and Eigenfunctions}\mbox{}\\\\\
	Given an operator $\alpha$ (hermitian or not). The number $a$ is named the "\NewTerm{eigenvalue of the operator}" of $\alpha$, if there exists a function $\varphi$ not identically zero such that (for a reminder of similar notions see the section of Linear Algebra):

$\varphi $is then say to be the "\NewTerm{eigenfunction}" (in analogy with the "eigenvectors") of $\alpha$, associated with the eigenvalue of $a$. Notice that $a$ may very well be equal to zero (you will understand this better when we move on to the study of concrete cases).
	
	In more physical terms, this means that when a state (a mathematical function in the formal sense such as $\varphi$) is unchanged by an operator, then the state is named an "\NewTerm{eigenstate}\index{eigenstate}" or "\NewTerm{eigenvector}" of the system.

	Let us consider the set $E_a$ of eigenfunctions associated to $a$ and $a$ a functional linear space which we will name "\NewTerm{sub-eigenspace}" associated to $a$. The number of dimensions of $E_a$ is named the "\NewTerm{multiplicity}" or "\NewTerm{degeneracy order}\index{degeneracy order}" of the eigenvalue $a$, and we denote it $g$.

	Let us consider now $a$ a being a simple eigenvalue, or non degenerated: $g=1$. This means that there is only one eigenfunction associated with $a$, with a non-zero multiplicative coefficient.

	If $g=2$ (double eigenvalue), we can find two non-proportional eigenfunctions (unbounded) associated with $a$, etc.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us see a particular example of an eigenfunction with an eigenvalue other than the classical case using the Energy.\\
	
	Given:
	
	with $\alpha=-\mathrm{i}\hbar\dfrac{\partial}{\partial x}$ (operator that we have already seen previously) and $a$ an eigenvalue.\\

	The equation becomes:
	
	which is easily verified if:
	
	which is indeed an eigenfunction of the previously mentioned operator and which will be most useful to us in the following developements.
	\end{tcolorbox}
	
	\subparagraph{Orthogonality of eigenfunctions}\mbox{}\\\\\
	Two eigenfunctions (eigenvectors) $\varphi$ and $\varphi'$ associated with two different eigenvalues of the same Hermitian operator are orthogonal, that is to say:
	
	\begin{dem}
	Let us start this time first with the Dirac notation with two eigenfunctions and two associated eigenvalues:
	
	with $a\neq a'$.
	
	We multiply the two previous relations by $\bar{\varphi}'$ and $\bar{\varphi}$ and we integrate to get the functional scalar product:
	
	Let us recall to continue that we have proved earlier above that:
	
	So if the operator $\alpha$ is self-adjoint (which is the case of the Hamiltonian as we have shown it), that is to say that $\alpha=\alpha^\dagger$, we have:
	
	Hence, by subtracting from the relation $($\ref{orthgonalyeigen}$a)$ the conjugate complex of relation $($\ref{orthgonalyeigen}$b)$, the eigenvalue $a$ being assumed real (or an integer...), we have:
	
	which proves indeed that:
	
	since $a\neq a'$.
	
	Let us now make the same demonstration proof with the classical algebraic writing and a different one approach. We start from:
	
	and as we have proved earlier above that:
	
	So if the operator $\alpha$ is self-adjoint (which is the case of the Hamiltonian as we have shown it), that is to say that $\alpha=\alpha^\dagger$, we have:
	
	It follows that the two relations $($\ref{orthgonalyeigenclassic}$a)$ and $($\ref{orthgonalyeigenclassic}$b)$ are equal (do not forget that the eigenvalue is assumed to be real!). As we can write:
	
	We can then write:
	
	So if the eigenvalues are not equal it is forced that it is the integral that is zero (the functional scalar product) and therefore that the two eigenfunctions are orthogonal!
	\begin{flushright}
		$\square$  Q.E.D.
	\end{flushright}
	\end{dem}
	The same proof, but with the traditional and more pedagogical notation gives:
	
	If we multiply the first equation on the left by $\bar{\Psi}_2$, and the second equation by $\bar{\Psi}_1$, and that we integrate over the totality of space, we get the following two expressions (corresponding to mean values):
	
	If we take the case of real functions, we can write:
	
	Since the operator $H$ is Hermitian (self-adjoint) as we have proved it above, we have:
	
	and as $E_2$, $\Psi_1$, $\Psi_2$ are admitted as real functions, we also have:
	
	Therefore:
	
	can be written:
	
	Therefore it comes:
	
	which shows well that $\Psi_1$, $\Psi_2$ are orthogonal according to the definition of the functional scalar product.
	
	\paragraph{Dirac formalism}\mbox{}\\\\\
Dirac has conceived a very practical general formalism, world-wide used by physicists, of which we will give the essential elements. The notations used have already been partially introduced in the preceding paragraphs.

	We will use the Dirac formalism for two points, the first being to better understand what has been seen so far when introducing to functional operators, the second being to introduce a notation and a method of solving that we can found in some textbooks. Moreover, in this book by simplification of writing, we will sometimes use this formalism.

	\subparagraph{Kets and Bras}\mbox{}\\\\\
	We consider a vector space $E_n$ with $n$ dimensions where $n$ can very well be infinite (Hilbert space). A vector is defined by $n$ components $x_i$ with $i=1,2,\ldots,n$ which we can store in column to form a column-matrix:
	
	We will say that this matrix describes the "\NewTerm{right vector}" or the "\NewTerm{ket $| x\rangle$}\index{ket}" (this must remind the "representatives"). It is possible to associate with the column-matrix the adjoint matrix (conjugated transpose for recall...):
	
	where the $\bar{x}_i$ are the conjugate complexes of the $x_i$. We will say that the adjoint line matrix describes the "\NewTerm{left vector}" or the "\NewTerm{bra $\langle x|$}\index{bra}" (this must also remind you the "representatives").
	
		The addition and multiplication by an number $\lambda$ are evident. Notice that if.
	
	we have trivially:
	
	
	With two vectors of components $x_i$ and $y_i$, we can then form the following quantity, named "\NewTerm{Hermitian scalar product}\index{Hermitian scalar product}" or "\NewTerm{Hermitian inner product}\index{Hermitian inner product}":
	
	We agree to write it $\langle x | y \rangle$. Notice that:
	
	the Hermitian scalar product is therefore not simply commutative!
	
	The product $\langle x|y \rangle$ depends linearly on $\langle x |$ and $|y\rangle$. Conversely, if a number $Q$ depends linearly on a ket $|x\rangle$, there exists a bra $\langle a |$ such that:
	
	In quantum physics, $\langle x|y\rangle$ is associated to the "amplitude" of being in the state $x$ if the system is in the state $y$. This Hermitian scalar product will be interpreted as the probability that the physical system is projected into the state $x$ if it is in the state $y$.

	An orthonormal basis of the studied space is constituted by $n$ vectors $|i\rangle$ such that (\SeeChapter{see section Vector Calculus}):
	
	where for recall, $\delta_{ij}$ is the Kronecker symbol (\SeeChapter{see section Tensor Calculus}).

Any vector (or function if we generalize vector space to function space) $|x\rangle$ of $E_n$ can be developed on this basis according to (\SeeChapter{see section Vector Calculus}):
	
	where the $x_i$ are the components of $|x\rangle$ in the chosen base. We can easily verify that (already seen many times in the section of Vector Calculus):
	
	If a ket $|y\rangle$ depends linearly on a ket $|x\rangle$, we write symbolically:
	
	where $\alpha$ is a linear operator. 

	Given a linear operator defined by the preceding relation and a bra $\langle u|$, the Hermitian scalar product:
	
	is a number $Q$ which depends linearly on $|x\rangle$. From what has been seen above, there is a bra $\langle v|$ such as $Q=\langle v|x\rangle$. Then $\langle v|$ obviously depends of $\langle u|$ linearly. We agree to put:
	
	Using this convention we can write:
	
	If $|y\rangle=\alpha|x\rangle$, depends linearly on $\langle x|$. By definition, we will write:
	
	where for recall $\alpha^\dagger$ is the adjoint of $\alpha$.
	
	Let us form with a bra $\langle u|$ the Hermitian scalar product:
	
	and we can write (as we have proved it previously):
	
	Hence the relation of the first importance which we have already met several times without really explaining its origin:
	
	We simply recall with this relation that a Hermitian (self-adjoint) operator is an operator equal to his adjoint.

	Thanks to Dirac's formalism, what was before abstract definitions has now become evident evidence.

	To summarize:
	\setlength\extrarowheight{10pt}
	
	\setlength\extrarowheight{0pt}
	So the reader has probably noticed that Dirac bra-kets and their algebra are very much like the Linear Algebra.

	A ket is like a vector, a bra is like the conjugate transpose of a vector, a bra-ket is like a complex inner product, a ket-bra is like an outer product, and operator is like a matrix, and operator acting on a state is like matrix product. Even we speak about eigenstates which are very much like eigenvectors.

	So how is it happened, that we invented a new notation for Linear Algebra instead of sticking to the usual notation of vectors and matrices as usual?

	So in fact it is not that it's "like Linear Algebra" but it is exactly Linear Algebra!!!! At the time of Dirac Linear Algebra was not taught to all undergraduates\footnote{In the first decade (when they were all students) of 20th century matrix multiplication was not taught to students on a regular basis in the best European universities.}. So he invented his own notation. Physicists found them convenient for the questions they consider. The clear evidence of this is that Heisenberg had to invent matrix multiplication when he created quantum mechanics... Actually there is probably a causal relation between the invention of quantum mechanics and the spread of linear algebra as a part of the standard curriculum.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Again, an excellent practical example of the application of the Dirac formalism is proposed in the section on Quantum Computing.
	\end{tcolorbox}
	
	\subsection{Schrödinger Model}
	Experiments (Compton effect, photoelectric effect, Young's slits, geometric / wave optics, etc.) have shown that waves can be treated as corpuscles (and vice versa). It is these observations which led Niels Bohr to state his "\NewTerm{principle of complementarity}\index{principle of complementarity}" which says that, according to the experiments carried out, we must consider matter either as a wave or as corpuscles. These two aspects complement each other.
	
	\subsubsection{de Broglie associated wave}
	In 1924, the French physicist Louis Victor De Broglie suggested that particles (electrons, protons, and others, and even atoms and molecules) could also, in some cases, exhibit wave properties in the same way as the light! De Broglie then expressed the idea that there existed between the associated fictitious wavelength of a particle of matter and its momentum a relation similar to that of a photon, that is to say ($\nu$ is the notation for the frequency for recall...):
	
	so we can write using the relation established in the section of Wave Mechanics:
	
	hence the ratio:
	
	De Broglie then made the following hypothesis: For a corpuscle of mass $m$ and velocity $v$ we have:
	
	where $\lambda$ is named the "\NewTerm{associated De Broglie wavelength}\index{associated De Broglie wavelength}".
	
	The moving matter would therefore have an associated wavelength!? It is an extremely small wavelength for masses of the order of a kilogram. If the speed is for example of the order of $1\;[\text{m}\cdot\text{s}^{-1}]$ then $\lambda\cong 10^{-34}$ [m].

	As we have seen it, the phenomena of interference and diffraction are important only when the size of the objects or slits is not much greater than the wavelength. It is therefore impossible to detect the wave properties of everyday objects. It is not the same for elementary particles, the electrons in particular.
	
	The electrons can therefore have wavelengths of the order of $10^{-10}$ [m] which corresponds to the spacing of the atoms of a crystal. C. J. Davisson and L. H. Germer carried out a crucial experiment: they scattered electrons on the surface of a crystal and in early 1927 observed that the ejected electrons were distributed in regular peaks. When they interpreted these peaks as diffraction peaks, they found that the wavelength of the diffracted electron was exactly that predicted by De Broglie!
	
	But then what is an electron?? The illustrations that show an electron as a minuscule negatively charged sphere are only convenient but inaccurate images. In fact, we have to use the corpuscular or wave model, the one that works best according to the situation so that we can understand what is happening. But we must not conclude that an electron is a wave or a particle. Rather, we should say that an electron is "all of its measurable properties". Some physicists still use the term "\NewTerm{quanton}\index{quanton}" to describe any system behaving either as a wave or as a particle.
	
	De Broglie was then able to suggest that each quantified electron orbit (according to Bohr's quantization postulate) is then a stationary wave and that an electron can only occupy orbits that can accept an integer number of wavelengths of its associated fictitious wave (today we assimilated this rater to the fact that the electron is trapped in a potential wells). If there was no exact coincidence, there could be no stationary wave, and therefore no stationary orbit either.
	
	Consequently, as for the resonant modes of a string (standing waves), only the waves whose circumference of the circular orbit contains an integer number of $\lambda$ exist, or (the amplitude will be calculated by particular techniques which will be seen further):
	
	with $n\in\mathbb{N}^{*}$.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/de_broglie_associated_wave.jpg}
		\caption{Approach to the wave aspect of orbits by De Broglie (source: ?)}
	\end{figure}
	By replacing $\lambda$ by $h/mv$, we get:
	
	That is indeed the quantum condition proposed by Bohr (\SeeChapter{see section Corpuscular Quantum Physics}). The orbits and the quantized energy states of the Bohr model are due to the wave nature of the electron and to the fact that only resonant stationary waves persist. This supposes that the wave-corpuscle duality is the basis of the structure of the atom.

	The wave's notion of the particle then allowed the physicist Erwin Schrödinger to develop a "wave equation" to describe the wave properties of particles.

	Let us do a small sympathetic interlude... since the associated wave of De Broglie known and given the result seen during our study of the Viriel theorem in the section of Continuum Mechanics, we can put relate together:
	
	Thus, for a fluid (liquid), we can get the value of the associated "\NewTerm{De Broglie thermal wave}\index{De Broglie thermal wave}". Which gives us:
	
	We will come back to this relation during our study of superfluids in the section of Continuum Mechanics.
	
	\pagebreak
	\subsubsection{Classical Schrödinger Wave Equation}
	The physicist Peter Debye found the De Broglie model a bit far fetched. He argued that the physics of waves, from the sound waves to the electromagnetic waves, even the waves propagating on a string requires an equation that describes them. There was no wave equation for De Broglie's atomic model (for the simple reason that the latter had never tried and Albert Einstein either). The physicist Erwin Schrödinger then took it upon himself to find the missing equation and did it brilliantly.

	Let us recall the one-dimensional form of the equation of wave (\SeeChapter{see section Wave Mechanics}):
	
	To simplify, let us look for a particular solution of the form (see the section on Wave mechanics or the section on Electrodynamics for the analogy):
	
	$\Psi(x)$ is the amplitude of the field associated with the particle. It is important to notice that the periodic part does not contain any displacement parameter $k$ (as is the case in Electrodynamics, for example) because the function must describe "static" solutions (be careful not to take this literally).
	
	For historical reasons this amplitude is as we know commonly named "wave function" although this name is misleading. It might be better to name it simply "amplitude field associated with matter".
	
	It is the search for the expression of this function that will lead us during the study of a special case (see much further below in text) to the well-known expression of the ionisation energy of an electron of given main quantum number $n$ and for its atom of given atomic number $N$.
	
	If we introduce \ref{univariatewaveequationparticularsolution} into \ref{univariatewaveequation}, we get:
	
	We have also:
	
	However, the Heisenberg model described particles as Schrödigner describes waves. The advantage of having two different formalisms - but equivalent - in quantum physics quickly became evident. For most of the problems physicists encounter, wave mechanics offers the easiest way to the solution. However, for others, such as those involving spin, Heisenberg's matrix approach proves its value (\SeeChapter{see section Relativistic Quantum Physics}).
	Hence:
	
	if we introduce \ref{variablechangeschrodinger} into \ref{wavesolutioninwaveequation} then we get the "\NewTerm{classical one-dimensional Schrödinger equation}\index{classical one-dimensional Schrödinger equation}" (in the absence of magnetic field ...):
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The potential energy could be gravitational as well as electric or both combined (therefore of any kind). But gravity is assumed to be so small at such scales compared to electrostatic forces that it is neglected.
	\end{tcolorbox}
	We can rewrite the preceding equation by generalizing it to a three-dimensional system. What ultimately gives us:
	
	where for recall $\Delta$ is the Laplacian of a scalar field (\SeeChapter{see section Vector Calculus}):
	
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} This equation is not a Lorentz invariant given that it was established from the classical expression of energy (and not the relativistic one).\\
	
	\textbf{R2.} The plane wave function that we have taken at the beginning does not have a physical significance since it carries an infinite energy. A better solution is to consider a wave packet. However, the wave packets generally employed consist of a superposition of plane waves (see proof further below). Hence, by studying its effects on one of the plane waves, we can still accept the physical conclusions which we can deduce from them...
	\end{tcolorbox}
	However as simple as it may seem, it took all the skill and experience of Schrödinger to be the first to write this wave equation and it was the foundation upon which he built the mathematical edifice of Wave Quantum Physics in the following months. But first it had to prove that it was indeed the wave equation sought by applying it to the hydrogen atom for which some results were well known thank to Sommerfel results (\SeeChapter{see section Corpuscular Quantum Physics}). His model eliminated all the successive and empirical tinkering of Corpuscular Quantum Physics.

	Some physicists judged Schrödinger's model to be pure madness (in particular Sommerfeld), but changed his mind shortly afterwards... so much so that the model was effective and replaced advantageously the horrible abstract matrix model of Heisenberg on which even Pauli had failed for the experimental results of the hydrogen atom. Soon after Max Born described the wave model as the deepest form of quantum laws, which obviously did not please his close friend Heisenberg...
	
	However, the Heisenberg model described particles as Schrödigner describes waves. The advantage of having two different formalisms - but equivalent - in quantum physics quickly became evident. For most of the problems physicists encounter, wave mechanics offers the easiest way to the solution. However, for others, such as those involving spin, Heisenberg's matrix approach proves its value (\SeeChapter{see section Relativistic Quantum Physics}).
	
	\paragraph{Schrödinger Hamiltonian}\mbox{}\\\\\
	The Schrödinger equation can also be written as (after a few small elementary factorizations) as follows:
	
	We write this in quantum physics in the form:
	
	Where $H$ is therefore the Hamiltonian of the system (or total energy) and constitutes a functional operator and where the total energy $E_\text{tot}$ is its eigenvalue.
	
	The Schrödinger equation is therefore an equation with partial derivatives of the second order, homogeneous linear. Whatever the total energy, it admits solutions (phew!), but we show that in general these solutions grow very rapidly (exponential growth) when we go to infinity in certain directions and are therefore physically unacceptable. There are only particular values of the total energy which give rise to physically acceptable solutions and in general all of these values include discrete values (trigonometric functions at the source with integer parameters) which are the "\NewTerm{linked levels}" of the system (because their eigenfunction decreases rapidly to infinity) and a continuum of values which are the "\NewTerm{unbound levels}" (their eigenfunction remaining finite at infinity). More precisely, if $W$ is the lower limit of the potential energy values at infinity, the bounded levels lie below $W$, while the values greater than $W$ constitute the continuum of the unbound levels.

	For example, in the study of the harmonic oscillator (one of the most difficult practical cases in terms of formalism) that we will do later, we will prove that we have:
	
	with $W=+\infty$. So there are only linked levels.

	For the hydrogen atom:
	
	with $W=0$. Linked levels are negative, therefore all positive energy values will be unbound levels.
	
	This having been said, let us also consider as an example (very important) how to determine the Hamiltonian $H$ of the Schrödinger equation of a non-relativistic charged particle in an electromagnetic field!

	We have seen in the section of Analytical Mechanics that the (classical) Lagrangian was defined by the subtraction of kinetic and potential energy according to the relation:
	
	We have proved in the section of Electrodynamics that the Lagrangian of the relativistic field-current interaction was given by:
	
	where for recall $\phi$ is the vector potential of the electric field (whose gradient is the electric field $\vec{E}$) and $\vec{A}$ is the vector potential of the magnetic field (whose curl is the magnetic field $\vec{B}$).

	If we add an electric field (and therefore an electrostatic potential $U$) in addition to the electromagnetic field, the Lagrangian is then written (since the potential is subtracted according to the Lagrangian definition):
	
	In the classical (non-relativistic) approximation we know that we have (\SeeChapter{see section of Special Relativity}):
	
	As we restrict ourselves to the non-relativistic case, we can eliminate the constant term of energy of the mass at rest such that:
	
	Still in the section of Analytical Mechanics, we showed that the Hamiltonian was given by:
	
	Therefore we have
	
	Moreover, we have seen in the section of Analytical Mechanics that:
	
	It therefore comes that:
	
	Finally:
	
	Either after simplification:
	
	$H$ therefore contains the kinetic energy and total potential energy. There is no magnetic term because the Laplace force, as we proved it in the section of Magnetostatic, does not work (there are some who are lucky...). $H$ is indeed the total energy of the classical system, but the preceding relation is not really adapted to Hamilton's formalism because the conjugate moments do not appear. But it is very simple to introduce them from the result obtained previously which was:
	
	Therefore:
	
	If we go into Quantum Physics, we have to replace the $p_i$ by their respective operators:
	
	of which we have already proved the origin earlier above. Thus, we have that:
	
	which must be written in the general case (as we do not know if the vector potential commutes with the linear momentum operator, we will assume that it does not commute):
	
	What is traditionally written in the form (sic!)
	
	This last relation is very often found in the following simplified form in textbooks in the absence of potential $U$ and by explicating $V$:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	In the section of Relativistic Quantum Physics we will prove the relativistic form of this Hamiltonian associated with the generalized Klein-Gordon equation or that of Dirac which includes the $1/2$ spin!
	\end{tcolorbox}
	Fortunately, we will not discuss examples where we will have to find solutions to the Schrödinger equation with such a Hamiltonian in this book...
	
	In the academic case, however, when application premises are addressed ..., we cancel either the vector potential (ie the particle is not immersed in a magnetic field) or the scalar potential (ie the particle is not immersed in an electric field). Moreover, when one of the fields is chosen as being non-zero, we take the case where it is constant and in a single dimension... Thus, if we want a constant electric field along a single axis (for example the $x$-axis), we take as scalar potential:
	
	since its gradient will give a constant according to $x$. In the case of a constant magnetic field along a single axis (for example the $z$-axis), we will arrange to take the vector potential:
	
	whose rotational (curl) gives a constant magnetic field according to $z$.
	
	\paragraph{De Broglie normalization condition}\mbox{}\\\\\
	In general, in a given dynamic state, the particle (if it is a one-particle system) described by the resolution of the Schrödinger equation for well-defined parameters is badly localized because $x$, $y$ and $z$ are not well defined by Heisenberg's principle of uncertainty. It is therefore necessary to define a probability $\mathrm{d}P$ of finding the particle in the volume element $\mathrm{d}x\mathrm{d}y\mathrm{d}z$ surrounding a point $(x, y, z)$, hence the existence of a distribution function of the coordinates such that:
	
	where $\rho(x,y,z)$ is therefore an essentially positive or zero quantity (probabilities obliged!) which must be expressed by means of the Schrödinger equation function $\Psi(x,y,z)$. We have, moreover, such very detailed examples in this section and at the end of that of Quantum Chemistry.
	
	Analogies with Classical WavePhysics, more precisely with Electrodynamics, have led us to admit that since the density of energy of an electromagnetic wave is proportional to the square of its amplitude (\SeeChapter{see section Electrodynamics}), the volumic density probability must be proportional to the square of the intensity of the associated field such that:
	
	where we use the Schrödinger function module as an amplitude analogy and where the constant is a real number. In the framework of Quantum Physics, it is much more common to find this last relation in the following obvious form:
	
	which highlights the necessary normalization of the Schrödinger equation.
	
	Finally, it is important to know that during mathematical developments, the huge majority of physicists have the habit of keeping the same notation for the non-normalized Schrödinger function as for the normalized one (which can lead to some confusion) such as:
	
	where $\lvert\Psi(x,y,z)\rvert^2$ then represents the density probability of finding the particle at a certain point in space.
	
	It is evident then that with this way of writing things we have then over all space:
	
	We can now consider the physical significance which may be attached to the intensity of the field associated with matter. Since this field describes the motion of a particle, we can say that the regions of space in which the particle is most likely to be found are those in which the $\lvert\Psi(x,y,z)\rvert^2$ is maximum.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/atomistic/atom_evolution.jpg}
		\caption{Atom model evolution (source: ?)}
	\end{figure}
	Let us also inform you that the prior-previous relation is written using the notation Dirac ket-bra in the very refined (and very common...) way:
	
	or the following (it is simply the square root of the previous one):
	
	where the module in the denominator disappears, since, for recall, the integral is a real number. It should never be forgotten that physicists, for the vast majority, note in a similar way the non-normalized and normalized Schrödinger function as this last relation reminds us.
	
	As we have already said, we will see many detailed examples of this normalization in this section with one-dimensional spaces and in the context of volumes in the section on Quantum Chemistry.
	 \begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	A ball is constrained to move along a line inside a tube of length $L$. The ball is equally likely to be found anywhere in the tube at some time $t$. What is the probability of finding the ball in the left half of the tube at that time? (the answer is $50\%$, of course, but how do we get this answer by using the probabilistic interpretation of the quantum mechanical wave function?)\\
	
	The first step to the answer is to write down the wave function. The ball is equally like to be found anywhere in the box, so one way to describe the ball with a constant wave function:
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/ball_in_a_tube.jpg}	
		\caption[]{Wave function for a ball in a tube of length $L$ (source: OpenStax)}
	\end{figure}
	The normalization condition can be used to find the value of the function and a simple integration over half of the box yields the final answer.\\
	
	The wave function of the ball can be written as:
	
	where $c^{te}$ is a constant, and  $\Psi(x,t)=0$ otherwise. We can determine the constant $C^{te}$ by applying the normalization condition (we set  $t=0$  to simplify the notation):
	
	This integral can be broken into three parts: (1) negative infinity to zero, (2) zero to$ $L, and (3) $L$ to infinity. The particle is constrained to be in the tube, so $C^{te}=0$ outside the tube and the first and last integrations are zero. The above equation can therefore be written:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	The constant $C^{te}$ does not depend on $x$ and can be taken out of the integral, so we obtain:
	
	Integration gives obviously:
	
	To determine the probability of finding the ball in the first half of the box $( 0\le x\le L)$, we have:
	
	The probability of finding the ball in the first half of the tube is $50\%$, as expected. Two observations are noteworthy. First, this result corresponds to the area under the constant function from $x=0$ to $L/2$ (the area of a square left of $L/2$). Second, this calculation requires an integration of the square of the wave function. A common mistake in performing such calculations is to forget to square the wave function before integration.
	\end{tcolorbox}
	
	Let us say one more thing about normalization. If you observe the Hamiltonian expression of the Schrödinger equations seen so far, then if $\lambda$ is a real or complex constant we always have:
	
	If we assume that $\Psi$ is a solution of the Schrödinger equation, we see that $\lambda\Psi$ is also a solution of the equation. Indeed, we get:
	
	Taking into account the fact that the the function $\lambda\Psi$ is normalized, we then get:
	
	hence (some books restrict themselves to these solutions for pedagogical reasons):
	
	Or rigorously we have more generally:
	
	where $\theta$ is a real number. This is what we name the "\NewTerm{phase arbitrariness}" which we had already mentioned at the beginning of this section without proof. We will also come back on this in the section of Quantum Field Theory for our study of Gauge theories applied to Quantum Physics.

	These solutions are normalized and correspond to the same energy value $E$ as the same probability density. This shows that it is not useful to look for the meaning of a negative or complex value of $\Psi$ (if we take the particular pedagogical case of $\lambda=-1$), because $|\Psi|^2$ is real and is not negative (but nothing prevent us that the fact we require a result in $\mathbb{R}$ is wrong...). Only the square of a wave function, which corresponds to the probability density, seems to be interesting from a physical point of view.
	
	\paragraph{Bound and unbound states}\mbox{}\\\\\
	Let us suppose that $\Psi(x,y,z)$ decreases rapidly to infinity, so that the integral:
	
	converge. It is then possible to take advantage of the arbitrariness prevailing on the wave function (the fact that $\Psi$ and $\lambda\Psi$ describe the same state) to make this integral equal to unity. We then say that $\Psi$ is a "\NewTerm{standardized field state function}":
	
	We notice that there is still an arbitrary on $\Psi$ by a complex number of module $1$, $e^{\mathrm{i}\theta}$, without the condition of normalization being altered. We know that this is the "phase arbitrariness" and we have already introduced it just before!
	
	Such a dynamic state is named "\NewTerm{bound state}\index{bound state}" or "\NewTerm{bound level}", because the particle manifests itself in a limited region of space because of a potential. When, for example, the hydrogen atom is located on a fundamental level, it is in a bound state. We know that there is no chance of finding the electron at more than a few angstroms of the proton, treated as infinitely heavy and originally placed as we have seen in the study of the Bohr model. Here is a good schematic view of the thing (bound state):
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/bound_state.jpg}	
		\caption{Pictorial representation of a bound state (source: Pour la science)}
	\end{figure}
	An example of an "\NewTerm{unbounded (default) state}\index{unbounded state}" is the free particle that can propagate indefinitely in all directions of space (by the way, for the latter example it is a little more complicated ... but we will deal with it later).
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	It is perhaps good to know that these bound state concepts have classical analogues. Thus, the bound levels of the hydrogen atom correspond to the elliptic orbits, the unbound (positive energy) levels correspond to the hyperbolic orbits.
	\end{tcolorbox}
	
	\subsubsection{Classical Shrödinger equation of evolution}
	We know that in Classical Mechanics the dynamic state of a system evolves, in general, in time. This means that the position and the linear momentum (for example) are a function of time. For a given Hamiltonian system, knowledge of the initial dynamic state makes it possible to predict exactly the future evolution of this system due to the well-known properties of Hamilton's equations.
	
	In Quantum Physics, dynamic states will generally evolve over time. The wave function describing a dynamic state will then not only depend on the coordinates of the particles constituting the system, but it will also depend on time and be written:
	
	It is quite natural to assume, by analogy with Classical Mechanics, that for a given Hamiltonian system the knowledge of the initial dynamic state at the instant $t_0$ allows us to predict what will be the dynamic state of the system at a later time $t>t_0$.
	
	On the way let us notice that this is equivalent to say that an initially "pure set" remains a pure set in the during the subsequent evolution of the systems which constitute it without external action. It would therefore cease to be true if all the systems of the whole had not exactly the same Hamiltonian.
	
	Let us indicate that there are at least two simple possible approaches to determine the time-dependent functions:
	\begin{itemize}
		\item The first one, common in many fields of application of Quantum Physics, consists in using an "evolution operator" and makes it possible to make explicit the Schrödinger equation of evolution. We will begin with this one even if it is the most complicated or abstract approach in our point of view.

		\item The second, which is widely used for educational purposes, allows the time-dependent functions to be obtained by means of the technique of separation of the variables of differential equations (\SeeChapter{see section Differential and Integral Calculus}) but requires to admit the Schrödinger equation of evolution as a postulate.
	\end{itemize}
	
	\paragraph{Operator of evolution}\mbox{}\\\\\
	Given $\Psi(t)$ the normalized wave function describing the dynamic state of the system at time $t$ (we do not write the other variables on which equation depends for simplicity, ie the spatial coordinates of the system's particles!). From the above, if $\Psi(t_0)$ is known, $\Psi(t)$ is also known. Then we have the correspondence:
	
	and we will admit that this dependence is linear! There is therefore an operator $T(t,t_0)$, named "\NewTerm{evolution operator}\index{evolution operator}", such that:
	
	The function $\Psi(t+\mathrm{d}t)$ depends also linearly on $\Psi(t)$. Then the same applies to:
	
	Therefore we assume that it exist a linear operator $K$, such that:
	
	where the pure imaginary complex number $i$ simply comes from the fact that we intuitively guess that the result will be a complex wave function. What also led the physicists to write the latter equality thus were the known results of the wave equation describing a dynamic state according to the idea of De Broglie. We will then show why writing this equality is justified.
	
	We have to determine $K$. Since the knowledge of the Hamiltonian $H$ controls the evolution of the system, so $K$ must probably depend on $H$. To define the law that binds $K$ to $H$, we will examine a particular case, that of the free particle (for which e will make a detailed study later). In this case, $H$ is identifies with kinetic energy only.
	
	According to De Broglie's ideas it is natural to assume that the wave function describing a dynamic state in which the linear momentum is well-defined, ie $p=\hbar k$ (relation proved during the study of the free particle), and where the total energy is thus also well defined, ie equal $E=\hbar\omega$, is a plane wave of the classical form:
	
	where as we already know, and for recall, $\vec{k}$ is the wave vector of the wave and $\vec{r}=(x,y,z)$ its spatial coordinates.
	
	It is common in Quantum Physics to take a wave that propagates in the $+X$ direction. Therefore (\SeeChapter{see section Wave Mechanics}):
	
	But we have the relation between the Hamiltonian operator and the corresponding eigenvalue:
	
	The two preceding equations lead us to write:
	
	By comparing this last relation with:
	
	We are led to put:
	
	Physicists assume that this relation between $K$ and $H$ is general. Then the equation:
	
	In which $K$ is replaced by its expression:
	
	Therefore becomes:
	
	This equation constitutes the "\NewTerm{Schrödinger's classical evolution equation}\index{chrödinger's classical evolution equation}" which allows to study systems evolving over time, in particular the processes involving the absorption and the emission of radiation and the diffusion of the radiation by the atoms.
	
	In particular, for a spinless particle subjected to a potential energy $E_p$, by always maintaining that the relation between $K$ and $H$ is general, the equation of evolution is then written:
	
	where the terms in parentheses correspond to the expression of the Hamiltonian.

	It is now necessary to solve the Schrödinger differential equation of evolution. For this, we will use the condition of normalization of De Broglie.

	Let us recall that this condition is written:
	
	and let us generalize this condition to a multidimensional and temporal study such that (according to the properties of complex numbers):
	
	Let us now perform the the derivative with respect to time of the previous integral. We therefore necessarily have:
	
	and let use the Schrödinger equation of evolution:
	
	What gives us for our integral after substitution:
	
	Let us now prove that we can write:
	
	This would then be equivalent to the proof that $H$ can act identically "backwards" such that:
	
	where $H$ can be (or "contain" if you prefer) an operator (differential one for example).
	
	This equality can be proved if and only if $\Psi$ is a decreasing function  towards infinity and whose derivative tends towards zero towards the infinite (typical of bounded states)!
	
	Let us prove this on a particular case (but frequent in physics) and to see how this can be done, let us consider in $H$, a particular term of the following form (we don't care of the constant corresponding the potential energy as it doesn't change anything in the result below):
	
	Which leads us to write:
	
	By integration by parts (\SeeChapter{see section Differential and Integral Calculus}) on the differential operator term $\partial_q (g\Psi(t,q))$, we get:
	
	But, since $\Psi$  is a function decreasing towards infinity by hypothesis (physically necessary for bound states!), we will have the first term which will always be zero. It remains to us therefore:
	
	So it makes no difference to consider that the operator differentiates everything that is on the right or whatever what is on the left, so far as it must be accepted that the latter case involves a change of sign and that the considerate states are bounded. It is customary to name this result sometimes the "\NewTerm{condition of hermicity}" ....
	
	So we are indeed authorized to write:
	
	Which also leads us to write (this highlight why the constant related to the potential energy was ignored in the proof above as it is ubvious that the constants cancels each other):
	
	This can only be satisfied if:
	
	And in the mathematical field dealing with operators we saw that we should note this equality:
	
	Which brings us to write:
	
	Or using the representative (ket-bra) notation:
	
	To return to the resolution of:
	
	It is obvious that a possible solution is then:
	
	which is therefore a made of purely spatial (independent of time) component and a time-dependent complex exponential. Let us check this:
	
	This is what had to be proved (...).

	Let us also notice that once the purely spatial solutions are determined, the time- and space-dependent solutions are easily obtained.
	
	Similarly, thanks to the relation $H^\dagger=H$ that we have demonstrated before, we can write:
	
	Finally, the relation:
	
	becomes:
	
	with the "\NewTerm{Heisenberg operator (of evolution)}\index{Heisenberg operator (of evolution)}" defined by:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	It may very well happen that $X$ is sometimes a simple constant (we will see an example further below).
	\end{tcolorbox}
	 One way is to think of the operator $X$ as time-independent and to consider all of the time dependence of its expectation value as coming from the state vector. This point of view is known as the "\NewTerm{Schrödinger picture}\index{Schrödinger picture}". Alternatively, we may think of the operator as evolving in time, while the state vector stays constant. This is known as the "\NewTerm{Heisenberg picture}\index{Heisenberg picture}". 
	 
	 To sum up, the time evolution of a state vector following Schrödinger picture is given by:
	
	or written more generally:
	
	or in representative notation:
	
	and that of an operator in the Heisenberg picture is:

	The Heisenberg picture is useful because we can see a closer connection to classical physics than with the Schrödinger picture. In classical physics, we describe the evolution of a system in terms of the time evolution of the observables, such as position or angular momentum, as dictated by the classical equations of motion. Classical mechanics does not include the concept of state vectors, as quantum mechanics does.
	
	\paragraph{Schrödinger picture by separation of variables}\mbox{}\\\\\
	Let us also see an interesting mathematical manipulation and somewhat similar to the previous one of Schrödinger's equation of evolution. This manipulation will allow us to see that the separation of variables works very well with the equation of evolution and that it will allow us to fall back on a result obtained previously (it is always pedagogically to see several approaches).

We have therefore in a particular case:
	
	Rewritten in traditional form (according to the literature) and in one dimension, for a constant potential in time, this relation is then written:
	
	Let us suppose now that the wave function can separate into two functions of which it is the product such that:
	
	We would then have:
	
	What injected into the one-dimensional evolution equation yields:
	
	Which gives after simplification:
	
	The left term depends only on $t$, the right one on $x$. Since they are equal, they are necessarily also equal to a constant which has the dimension of an energy ($U(x)$ is a potential energy for recall).

	So for the left term:
	
	then:
	
	And for the right term:
	
	Which can be written:
	
	After factorization:
	
	Either with the notations of this book (after having rearranged a bit):
	
	we thus fall back on the classical one-dimensional Schrödinger  equation which is not bad at all as result!

	Now, since we have put:
	
		then we have finally:
	
	Which we can write under the notations of the preceding paragraphs:
	
	We find also this last relation in several different forms in the literature of which few samples:
	
	
	\paragraph{Linear combination of states (quantum superposition)}\mbox{}\\\\\
	We must notice before we move on to another subject something very important that we had just mentioned in the second postulate!

	Indeed, any equation of the following form seen previously:
	
	Is therefore a solution of Schrödinger's evolutionary equation and as in quantum systems the Hamiltonian can take (or be associated with) several discrete eigenvalues traditionally denoted by $E_n$, we then have, as mentioned at the beginning of this section, by the principle of linear combination of differential equations (\SeeChapter{see section Differential and Integral Calculus}) the following more general solution:
	
	of which we shall have several practical examples (of the discretization of the states of energy and that these are in infinite number) in this section and in that of Quantum Chemistry.

	If we write the normalization constant of $\Psi_n(x)$ of the previous relation, then we have:
	
	This last relation would be written in the following traditional ket-bra form:
	
	where the constant coefficient $c_n$ is obviously assimilated to $A_n$ (admit that it notation is much simpler isn't it?).
	
	We then say that the state $|\Psi\rangle$ is a linear combination of elementary states $|\psi\rangle$. Therefore $|\Psi\rangle$ also represents a wave particle as being simultaneously in several different sub-states.

	It is interesting to notice that each solution:
	
	Describes a "\NewTerm{steady state}" or "\NewTerm{stationnary state}". Let us see (at last!) rigorously what it is.

	Indeed, we have:
	
	which is therefore independent of the time from which the origin of the name "\NewTerm{stationary state}" (we promised to define its origin at the beginning of section... so that is done!).

	The functions being normalized we therefore have:
	
	The calculations above have showed us (we had proved it in two different ways) that the eigenfunctions have the following properties:
	
	When $k=n$ and:
	
	when $k\neq n$. That is to say with a more general notation:
	
	It is this property that has led us in the third postulate to speak of "\NewTerm{orthogonal basis of the stationary eigenfunctions}".
	
	Let us continue our calculation which can be written using the Kronecker symbol (\SeeChapter{see section Tensor Calculus}):
	
	We can then interpret the term $|A_k|^2$ as the weight of the eigenfunction $\Psi_k$ in the quantum state $k$, the probability of actually being in the eigenstate $\Psi_k$ is then equal to $|A_k|^2$ the normalization then imposes:
	
	We must therefore remember that any quantum state can always (as far as we know) be interpreted as a linear combination of eigenstates. The coefficient $A_k$ of an eignenfunction/eigenstate $\Psi_k$ is then associated with a probability $|A_k|^2$.
	
	It is this mathematical result, very important (£££), which is at the origin of the paradox of "\NewTerm{Schrödinger's cat}\index{Schrödinger's cat}" (among others ...) and of many debates.

	To close this small subject, let us notice one thing:

	If the coefficients $A_n$ are not the coefficients already normalized but not normalized, the physicists then denote their normalization as follows:
	
	because very often they use the same notation for the normalized coefficient and the non normalized one in their developments ...

The writing of the last relation is easily justified because let us recall that we must have:
	
	and we have indeed after rearranging:
	
	Let us notice that with the traditional ket-bra notation, the relation:
	
	is often written in some specialized textbooks:
	
	which therefore always gives the probability of finding the state $n$ at the position $x$.
	
	\paragraph{Continuity equation}\mbox{}\\\\\
	Let us consider now the important example of the equation of evolution for a free particle, that is to say with $E_p=0$. Therefore we have:
	
	The probability of finding the particle in a volume $V$ is, as we have seen, given by:
	
	Hence:
	
	Taking into account the equation of evolution of the free particle, the second term of equality is written:
	
	Where we have put:
	
	According to the Ostrogradsky theorem (\SeeChapter{see section Vector Calculus}), it therefore comes:
	
	where the integral of the right is carried out on the surface $S$ which limits the volume $V$. The preceding relation therefore expresses indeed that the variation per unit of time of the probability of finding the particle in $V$ is equal to the flux passing through the surface $S$ and the vector $\vec{j}$ can be interpreted as a probability current density which satisfies the continuity equation as we have determined it in the section of Thermodynamics:
	
	Hence:
	
	In Quantum physics, it seems therefore that there is conservation of the particles flow: there is no creation, neither destruction of particle, but in the Nature (experimental observations) we observe such phenomenons of destruction and creation (but in fact the particle must be in a non-zero potential field)... In other words it means that a particle can not appear or disappear in a given volume $V$, there must be a particle flow in the walls of $V$ for particles entering or leaving $V$.
	
	That latter relation is more often written:
	
	Intuitively, the above quantities indicate this represents the flow of probability. The chance of finding the particle at some position $r$ and time $t$ flows like a fluid; hence the term probability current, a vector field. The particle itself does not flow deterministically in this vector field.
	
	\subsubsection{Implications and Applications}
	The different definitions and tools that have been seen before, sometimes very abstract, will allow us to study some fundamental cases that lead to quite splendid results.

	In a first step, we will see how to treat the case of the free particle (unbound state) and what are the problems of this simple configuration.

	Then we solve the Schrödinger equation with a spin-free particle in a potential well with rectilinear infinite walls and show that with the formalism of quantum physics we will find the same results as the Bohr model (even more generalized!).

	After this, we will introduce the study of the harmonic oscillator by revisiting briefly the resolution of the Schrödinger equation of a free particle. This example constitutes a form of introduction to the theoretical study of quantum atomic systems. It is in this example that we will use all the power of functional linear operators. It will therefore be important not to read it in diagonal (if possible and if the reader is still motivated).

	We will also have to study another famous phenomenon, the "tunnel effect"! Obviously, we decided to introduce a particular case so that the reader can see the reasoning that led to the discovery of this astonishing (but logical) phenomenon. Again, this example will support the validity of quantum theory and demonstrate the value of the decay constants of many nuclear isotopes!

	With respect to relativistic cases, with or without spin, we refer the reader to the sections of Relativistic Quantum Physics, and as regards the simple atomic model, we refer it to the section on Quantum Chemistry.

	Enjoy!
	
	\paragraph{Free particles (zero potential)}\mbox{}\\\\\
	Curiously, the resolution of the Schrödinger equation for a free particle (where the potential is zero) is the simple ... most complex case ... mathematically speaking because the integration bounds of normalization are infinite. Let us see this!

Let us recall first that we have proved in a simplified way in the section of Sequences and Series that the Fourier transform of a function $f$ and its inverse were given by:
	
	Either in a one-dimensional form:
	
	Let us now proceed to the change of variable which connects the wave number $k$ to the linear momentum (relation introduced at the beginning of this section):
	
	Which gives us:
	
	Let us now return to the Schrödinger's equation of evolution:
	
	If the particle is free, there is no potential and at one dimension we then have:
	
	This differential equation admits monochromatic plane wave solutions of the type (\SeeChapter{see section Electrodynamics}):
	
	with obviously the small nuance that we have to use the relation (otherwise it does not work by cons!):
	
	without forgetting that (this will be useful afterwards):
	
	The curve of the energy $E$ as a function of the wave vector $k$ is sometimes named a "\NewTerm{dispersion curve}" and it is a parabola (since $k$ is squared) for a free particle!

	Obviously, the probability density of this solution is equal to:
	
	but this can not correspond to reality because! Indeed, a monochromatic planar wave of constant norm will have to carry an infinite energy and this is not possible!

	In fact the solution comes from the fact that the true solution uses the principle of superposition of all the monochromatic waves of all the frequencies such as:
	
	and we fall back here a relation very similar to an inverse Fourier transform (\NewTerm{see section Sequences and Series}). Such a superposition of plane waves is named "\NewTerm{one-dimensional wave packet}\index{one-dimensional wave packet}".
	
	What we can rewrite:
	
	But, we see immediately that we will not be able to normalized following:
	
	Therefore there is no longer a general solution. It is necessary to give a carrier envelope to the waves imposing a possible normalization. This carrier envelope can be a Dirac or a Gaussian or other functions of more or less complex distributions. Then physicists must use a property of the Fourier transforms which naturally reveal the true Heisenberg incertitudes. Thus, the latter are a condition for the normalization of free particles using Fourier transforms.

	To date, as already mention during our study of classical Heisenberg incertitudes, we have no pedagogical and simple proof to propose on this last point. It will come in a near of far future.

	On the other hand, we can take as a trivial solution the eigenmodes of the particle such that:
	
	Indeed:
	
	This is what we will use as a relation for our study of the harmonic oscillator further below.

	Before studying the particular case of the quasi-monochromatic wave packet, we will recall some results concerning the sum of two plane waves.

	Let us start by summing two monochromatic plane waves of neighboring frequencies:
	
	with:
	
	and:
	
	Notice that we therefore impose:
	
	The resultant wave is the expressed as:
	
	Either by using the remarkable trigonometric relations (\SeeChapter{see section Trigonometry}):
	
	which is a plane wave propagating along the positive $x$ (\SeeChapter{see section Wave Mechanics}) with the pulsation $\omega_0$ and the mean wave vector $k_0$, and therefore at the phase velocity:
	
	The cosine term is then interpreted as the slowly variable amplitude of this plane wave ("amplitude modulation" as we have seen in the section of Wave Mechanics).

	Notice a rather important point !: The phase velocity is not consistent with the velocity we get by using the kinetic energy of a free particle. Indeed:
	
	Therefore the phase velocity does not represent the velocity in the usual conventional sense but that of the wave traveling at the group velocity:
	
	where we thus fall back on the classic formulation of the velocity from kinetic energy (not bad...)!

	We can easily represent all this with Maple 4.00b:
	
	\pagebreak
	\texttt{>restart:with(plots):\\
	>lambda[0]:=1; T[0]:=1; k[0]:=2*Pi/lambda[0]; w[0]:=2*Pi/T[0];\\
	>delta\_k:=k[0]/8: k[1]:=k[0]-delta\_k; k[2]:=k[0]+delta\_k;\\
delta\_w:=w[0]/10: w[1]:=w[0]-delta\_w; w[2]:=w[0]+delta\_w;\\
	>P1:=animate(cos(k[1]*x-w[1]*t)+cos(k[2]*x-w[2]*t), x=0..1*2*Pi/delta\_k, \\
	t=0..2*Pi/delta\_w, numpoints=200, frames=15, color=red):\\
	>P2:=animate({2*cos(-1/2*k[1]*x+1/2*w[1]*t+1/2*k[2]*x-1/2*w[2]*t), \\
	-2*cos(-1/2*k[1]*x+1/2*w[1]*t+1/2*k[2]*x-1/2*w[2]*t)}, x=0..1*2*Pi/delta\_k,\\
	 t=0..2*Pi/delta\_w, numpoints=100, frames=15, color=blue):\\
	>display(P1,P2);}
	
	Which gives:
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/phase_velocity_vs_group_velocity.jpg}	
		\caption{Representation of the concept of phase velocity and group velocity}
	\end{figure}
	Unlike the harmonic plane wave, this wave does not have a constant norm: its norm is zero in certain zones. On the other hand, it always extends over an infinite distance, and therefore has a infinite norm (the sum of the probability over all space). It therefore has no physical meaning (at least as far as we know).

	The preceding study can be extended by summing an increasing number $N$ of plane waves in the neighborhood of $\omega_0$ and $k_0$. Such a superposition leads to an increasingly localized function in certain areas of space (in particular near $x=0$, for example, for $t=0$), the distance between these zones increasing proportionally with $N$. At the limit $N\rightarrow +\infty$, then only the zone near $x=0$ remains, the others being rejected to infinity. The transition to this limit  $N\rightarrow +\infty$ is done by replacing the discrete sum on plane waves by a continuous summation ie by an integral of the form:
	
	with:
	
	with therefore:
	
	Such a packet is therefore named a "\NewTerm{quasi-monochromatic wave packet}\index{quasi-monochromatic wave packet}".

	This expression can be rewritten:
	
	It is important to understand that $\omega$ is a function of $k$ given by the dispersion equation. We will compute this expression using the fact that $\delta k/k_0\ll 1$. This condition implies that $\delta \omega/\omega_0\ll 1$. It is possible to carry out a limited Talyor development in the neighborhood of $\omega_0$:
	
	Therefore:
	
	Let us put $y=k-k_0$:
	
	Let us calculate the integral:
	
	with:
	
	Therefore:
	
	The last term is again interpreted as a plane wave moving at the phase velocity:
	
	The amplitude of this plane wave is given by a sinus cardinal function. At $t=0$, this $\mathrm{sinc}$ function has important values only in the zone where:
	
	It is therefore a well-localized function. Consequently, $\Psi$ is a summable square function. The calculation gives:
	
	The function can therefore be normalized by putting:
	
	We have therefore succeeded in obtaining a function satisfying both the Schrödinger equation and the normalization condition, by using an infinite sum of harmonic waves. The example we have dealt with is only one particular case. Other types of wave packets can be obtained by taking other distributions for the amplitudes of the plane waves that make up the packet (we assumed here that they all had the same amplitude). Consequently, the group velocity is conventionally associated with the velocity of the particle of mass $m$ and linear momentum $p$.

	Thus, the wave packet moves globally at the group velocity, which is identified with the velocity given by Classical Mechanics.

	Uncertainty relationships have already been introduced at the beginning of this section in two different ways. But in the example of the wave packet studied in the previous paragraph, we have seen that the function is located in a zone of range (half-height width):
	
	We therefore have the relation:
	
	We fall back here an expression of the uncertainty. The numerical coefficient could be slightly different according to the definition chosen for $\delta x$ and $\delta k$, or the type of packet. In particular, it could be significantly larger in some cases. We thus have in fact an inequality of the type:
	
	In quantum physics, these inequalities are expressed as a function of the linear momentum $p$, connected to $k$ as we have prove earlier by $p=\hbar k$. We therefore have:
	
	Thus, the more precisely the linear momentum of a wave packet is defined (implicitly the wavelength), the less it has components and the more it is spread, which increases the uncertainty relative to its position and respectively the more its position is known, the less its wavelength will be precise.

	It is therefore not a question of uncertainties in the sense of measurement, and which would be limited by the measuring devices, but of an intrinsic fundamental property, linked to the quantum representation of a particle according to the proposed mathematical model. The model of the Bohr atom have therefore to be rejected for the energy levels which are close to this equality.

	\pagebreak
	\paragraph{Infinite (rectangular) potential well}\mbox{}\\\\\
	Let us take as a first example, very important for the Nuclear Physics section and for semiconductor specialists, the resolution in the classical form of the infinite potential well with straight walls, also named "infinite rectangular well" (this example is really very important, take your time to understand it and to master it at best) or "particle in a box problem".

	This is the simplest example of a potential function $E_p(x)$, zero within the well and infinitely big on the walls, distant by a length $L$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/potential_well_infinite_rectangular.jpg}	
		\caption{Infinite potential well illustration}
	\end{figure} 
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	When $U=+\infty$ we say sometimes that the walls are "perfectly reflective".
	\end{tcolorbox}
	We assume a particle trapped in this well. It can not escape because the walls (ie the potential $U$) have an infinite height. But inside, it is free to move without interacting with the walls.

	This configuration is reflected by the boundary conditions where the potential electrostatic energy is denoted $U$:
	
	and:
	
	There exist at least two ways to approach this problem. Let us consider the two types of treatments because the first one allows to have a simplistic approach whereas the second one allows to have a more general approach which will be useful to us later on in our study of the Tunnel effect:
	
	\paragraph{First approach}\mbox{}\\\\\
	The (classical) Schrödinger equation :
	
	has therefore a simple particular solution respecting the initial conditions in one dimension, of the type:
	
	whose second derivative is:
	
	Introduced in the Schrödinger equation, we get, after some elementary algebra simplifications:
	
	So finally the solution is written:
	
	to which the boundary conditions must be applied (the cosine solution is in every aspect similar).

	If we want to be able, later, to make a parallel with one or more electron(s) trapped in the potential well of an atom (which is him not rectangular and not infinite), we are taken to the following considerations:

	The stability of the atoms suggests the existence of an electronic standing wave in the well. In addition, experimental observation shows that only certain energy levels appear to be allowed in the latter.

	If we make a similarity with the vibrating strings, the wave function of the electron must be such that:
	\begin{enumerate}
		\item For $x=0$ and $L=0$ there must be a vibration node, ie $\Psi(0)=\Psi(L)=0$

		\item The wave function $\Psi$ must have an integer value of half wave length along the length $L$

		\item In the box $U=0$ therefore $E_p=0$

		\item If at the ends ($x=0$ and $x=L$) $\Psi=0$ then the argument of the sine equals $n\pi\; \forall n\in\mathbb{N}^{*}$
	\end{enumerate}
	Therefore we must have:
	
	hence since the potential energy is zero in the well:
	
	The total energy of the particle thus ranges over a discrete sequence of values, the only ones permitted. The value of $L$ is determined using the Bohr or Sommerfeld model as depending on the situations (\SeeChapter{see section Corpuscular Quantum Physics}).

	The total energy of the above particle is composed of the eigenvalues of the energy in the potential well.

	Thus the Schrödinger's equation allows us to ignore Bohr's third postulate in the sense that it directly explains the notion of quantification of the levels by by integer (discrete) values of the boundary conditions of a potential well considered as perfect.

	The corresponding wave functions in the well where $U=0 \equiv E_p=0$ are therefore:
	
	Either after simplification:
	
	This is the expression of one of the solutions of the differential equation for the ideal rectangular potential well. Thus, there exists a discrete sequence of solutions wave functions. These are the obviously the eigenfunctions of the particle.

	The constant $\Psi_0$ in this expression is determined by the De Broglie normalization  condition (about which we spoke at the beginning of this section), that is to say by the condition:
	
	We then find (normally integration calculus that should be obvious to the reader but on request we can as always give the details):
	
	and the final expression of the wave function associated with the eigenvalue $E_n$ thus reads:
	
	Some physicists typically write this in a complex form by obviously considering only the real part of the following expression (we use the "Euler formula" seen when introducing the complex numbers in the section Numbers):
	
	with:
	
	We then say that we have "\NewTerm{quantification conditions}" on $k$ imposed by the boundary conditions.

	This notation is sometimes useful and we will use it when studying the tunnel effect in the section of Nuclear Physics.

	We can deduce from the expression obtained the main properties of the wave functions describing the stationary states of the particle in a box:
	\begin{enumerate}
		\item[P1.] The figure below shows the $\Psi_n$ (left) and the probability density functions $|\Psi_n|^2$ (right) for the first energy levels $E_n$:
		\begin{figure}[H]
			\centering
			\includegraphics{img/atomistic/infinite_potential_well_functions_plots.jpg}	
			\caption[]{Representation of the wave functions and density for some energy levels}
		\end{figure}
		We notice that (obviously we could analyze this analytically and not graphically if we wanted to), in addition to the points $x=0$ and $x=L$, $\Psi_n$ has $n-1$ zeros located in:
		
		These points, where the wave function and the probability density are zero, are named "\NewTerm{nodal points}" or simply "\NewTerm{nodes}" of the wave function. The number of nodes increases when $n$ increases, that is to say when one moves to more and more excited states. 

		The wave function $\Psi_1$ of the ground state with $n = 1$ and therefore with:
		
		has no node, that of the first excited state $\Psi_2$ of energy:
		
		has one nodal point, that of the second excited state $\Psi_3$ has two nodal points, etc.
		
		The variation of the nodal properties of the wave functions when $n$ varies translates the orthogonality of the stationary states of different energy. Indeed, we easily check that $\langle \Psi_n|\Psi_m\rangle$ is zero when $m\neq n$:
		
		where we used one of the remarkable trigonometric identity demonstrated in the section of Trigonometry.
		
		\item[P2.] As we can see it in the previous figure, the probability density associated with any stationary state of the particle is symmetric with respect to the midpoint $x=L/2$.

		We therefore expect that the mean value of $x$ will be exactly equal to $L/2$ in such a state. Indeed, we have seen in the section of Statistics that the expected mean (average) of a probability event $P(x)$ is defined by:
		
		where $x$, $\text{E}(x)$ and $P(x)$ have no units (caution! as we will do a dimensional analysis!).

		Now, in quantum physics $\text{E}(x)$ and $x$ are identical dimensional quantities. This means that the dimensions of $P(x)$ must cancel those of $\mathrm{d}x$. Thus, we guess following the study of the De Broglie normalization conditions that:
		
		is a linear probability of the presence of the particle in [m$ ^{-1}$].
		
		The integration domain being $[0, L]$ we have finally:
		
		
		\item[P3.] Also we have for the linear momentum:
		
		
		\item[P4.] We can also verify without too much difficulty that what we saw in the statement of the second postulate of Quantum Physics is well verified  in this example. That is to say, the eigenfunctions of the wave are connected to the Hamiltonian operator via the eigenvalues of the energy:
		
		Indeed, in our example, this gives:
		
		
		\item[P5.] Our analysis of the quantum particle in a box would not be complete without discussing "\NewTerm{Bohr's correspondence principle}\index{Bohr's correspondence principle}". This principle states that for large quantum numbers, the laws of quantum physics must give identical results as the laws of classical physics. To illustrate how this principle works for a quantum particle in a box, we see that in the plot given earlier above of $|\Psi|^2$ when $n\rightarrow +\infty$ that the an number of "peaks" will increase to infinite and therefore result in a uniform (constant) probability to found the particle between $0$ and $L$.
	\end{enumerate}
	That's it... for the first approach of the problem. Let us now look at the second:
	
	
	\paragraph{Second approach}\mbox{}\\\\\
	We thus have the Schrödinger equation in the one-dimensional case:
	
	In regions outside the box where the potential is infinite, we have:
	
	Therefore:
	
	which gives:
	
	Let us consider now the case of the well where, since the electrostatic potential is zero, the Schrödinger equation reduces to:
	
	It is therefore a linear differential equation of order $2$ with constant coefficients, equation which is relatively easy to solve in the general case (\SeeChapter{see section Differential and Integral Calculus}). Given the equation:
	
	Using the results we get during the treatment of the particular solution, let us suppose that the function $y$ that satisfy this differential equation is of the form $y=e^{Kx}$. We have then:
	
	provided, of course, that $e^{Kx}\neq 0$. This last relation is therefore the auxiliary quadratic equation of the differential equation (characteristic polynomial). It has two solutions / roots (it is a simple resolution of a polynomial of the second degree) that we will denote in the general case $K_1$, $K_2$. Which means that:
	
	is satisfied for both roots. If we make the sum since the both are equal to the same constant:
	
	Thus, it is immediate that the general solution of $y$ is of the type:
	
	where the reader should normally be able to verify that the addition of the constants $A$ and $B$ does not change the developments of the preceding paragraphs.

	In the present case that interest us:
	
	The quadratic equation is:
	
	thus:
	
	So finally the general solution is of the form:
	
	Let us put now:
	
	We then have:
	
	with:
	
	It is now necessary to determine $A'$ and $B'$ using the boundary conditions. Thus, at $x = 0$ and $x = L$ we should have $\Psi=0$ and we have for $x = 0$:
	
	The coefficient $A'$ must therefore be equal to zero. And in $x = L$ we should have:
	
	But in this case, $B'$ must be different from zero. Indeed, if it were zero, the wave function would be zero throughout the whole well, which is contrary to the physical reality of the problem. It is therefore necessary that the sinus to be zero, or that its argument be equal to a multiple of a non-zero integer number of angle $\pi$ such that:
	
	Therefore:
	
	We thus find exactly the same result as the preceding method!

	It remains to determine $B$ and the method is exactly identical to the first method of resolution which we have seen above. Thus, we have:
	
	What is especially important in this method is to remember for a moment the general form of the solution:
	
	
	\pagebreak
	\paragraph{Fermi Energy}\mbox{}\\\\\
	The "\NewTerm{Fermi energy}\index{Fermi energy}", $E_F$ is a concept in Quantum Physics that designates the energy of the highest quantum state occupied in an idealized system in which all the layers are filled successively and without discontinuities, that is to say in practice when a system is at absolute zero: $0$ [K] (\SeeChapter{see section Statistical Mechanics}). It is an important concept in Solid states physics as we have seen it in our study of semiconductors in the section of Electrokinetics.
	
	The rectilinear infinite potential well is an excellent teaching tool to introduce practically the Fermi energy level and then to extend it to other particular cases.

	Let us recall that we have just obtained in two different ways:
	
	Then the total energy of an entire system composed of N particles can all have the same fundamental state $n = 1$ and thus that violate the principle of Pauli will be:
	
	But if we apply the Pauli's exclusion principle of electrons (fermions), each level can then take only two states (spin oppositions) in this model (which does not contain sublayers or other subtleties). Henceforth, each fundamental level can be occupied by only two states, and so it is for each level. The total energy is then in reality:
	
	Using the sum of the squares demonstrated in the section of Sequences and Series and writing as it is usual:
	
	we have then:
	
	The average energy per particle is then:
	
	By definition, the energy of the last level to be occupied is that of Fermi and therefore given by:
	
	Therefore:
	
	We also notice that we have whatever the value of $N$:
	
	The three-dimensional isotropic case is known as the "Fermi sphere" as already introduced in the section of Electrokinetics during our study of semiconductors.
	
	Let us now consider a three-dimensional cubical box that has a side length $L$. This turns out to be a very good approximation for describing electrons in a metal. The states are now labeled by three quantum numbers $n_x$, $n_y$, and $n_z$. The single particle energies are (where $m$ is the mass of fermion (electron in this case)) as we have seen in the section of Electrokinetics during our study of semiconductor:
	
	where $n_x$, $n_y$, and $n_z$ are positive integers. There are multiple states with the same energy, for example $E_{211}=E_{121}=E_{112}$. Now let's put $N$ non-interacting fermions of spin $1/2$ into this box. To calculate the Fermi energy, we look at the case where $N$ is large.

	If we introduce a vector $\vec{n}=\{n_{x},n_{y},n_{z}\}$ then each quantum state corresponds to a point in "$n$-space" with energy:
	
	with $||\vec{n}||^2$  denoting the square of the usual Euclidean length (norm). The number of states with energy less than  $E_{\vec{n}}$, defined as the "Fermi energy", is  is equal to the number of states that lie within a sphere of radius $R=||\vec{n}_F||^2$ in the region of $n$-space where $n_x$, $n_y$ and $n_z$ are positive. In the ground state this number equals the number of fermions in the system:
	
	the factor $2$ is once again because there are two spin states, the factor of $1/8$ is because only $1/8$ of the sphere lies in the region where all $n$ are positive. Therefore we get:
	
	so the Fermi energy is given by:
	
	Which results in a relationship between the Fermi energy and the number of particles per volume (when we replace $L^2$ with $V^{2/3}$):
	
	
	Confusingly, the term "Fermi energy" is often used to refer to a different but closely related concept, the "Fermi level" (also named "electrochemical potential\index{electrochemical potential}"). There are a few key differences between the Fermi level and Fermi energy, at least as they are used in this book:
	\begin{itemize}
		\item The Fermi energy is only defined at absolute zero, while the Fermi level is defined for any temperature.

		\item The Fermi energy is an energy difference (usually corresponding to a kinetic energy), whereas the Fermi level is a total energy level including kinetic energy and potential energy.

		\item The Fermi energy can only be defined for non-interacting fermions (where the potential energy or band edge is a static, well defined quantity), whereas the Fermi level (the electrochemical potential of an electron) remains well defined even in complex interacting systems, at thermodynamic equilibrium.
	\end{itemize}
	Since the Fermi level in a metal at absolute zero is the energy of the highest occupied single particle state, then the Fermi energy in a metal is the energy difference between the Fermi level and lowest occupied single-particle state, at zero-temperature.
	
	
	\paragraph{1D-Harmonic oscillator}\mbox{}\\\\\
	The study of the harmonic oscillator corresponding to that of a wave function stuck in a well of parabolic potential. This is roughly equivalent to the atoms where the walls of the potential well are naturally not rectangular and infinite ... The study that follows is therefore what is closest to what is available in Nature at the atomic level and this is why it is used a lot in Quantum Chemistry and also in Nuclear Physics.

	In the case of a free particle in rectilinear displacement, we have seen that the potential energy is zero ($E_p=0$) and the Schrödinger equation then becomes:
	
	However, for a free particle (in the absence of a potential field) the total energy is therefore equal to the kinetic energy:
	
	But we have:
	
	The ratio:
	
	being the associated De Broglie wavelength. By introducing the wave number $k=2\pi/\lambda$ (\SeeChapter{see section Wave Mechanics}), we have:
	
	named "\NewTerm{De Broglie's relation}\index{Broglie's relation}". Finally:
	
	Therefore, Schrödinger's equation can be written:
	
	We see by direct substitution that this differential equation admits for solutions the wave functions:
	
	These two different solutions represent the displacement of the same particle once in the $+x$ direction and the other in $-x$. If $A=B=1$ we have:
	
	The fact that this result is equal to unity means that the probability of finding the particle is the same at all points as we don't need any special normalization. In other words, $\Psi(x)=e^{\pm\mathrm{i}kx}$ describes a situation in which the uncertainty on the position is total. This result is in agreement with the principle of uncertainty since $\Psi(x)=e^{\pm\mathrm{i}kx}$  describes a particle of which we know with precision the linear momentum $p=\hbar k$: that is to say that $\Delta p=0$, which implies $\Delta x\rightarrow +\infty$.

	In the section of Differential and Integral Calculus we showed that the most general solution of a differential equation is the sum of these solutions. In other words, in our example:
	
	with:
	
	By the way, we can notice that if $E_p\neq 0$ then the result is the same with the difference that we will have:
	
	When the particle of interest is in a well of potential described by the function (parabola):
	
	we then speak of "\NewTerm{1D-harmonic oscillator}\index{harmonic oscillator (1D)}".

	This system is very important because the Hamiltonian of the equation intervenes in all the problems involving oscillations such as molecular and crystalline vibrations (\SeeChapter{see section Quantum Chemistry}).

	Let us first take as an example the classic harmonic oscillator which consists of a body subjected to move along an axis and subjected to a return force proportional to the distance to a point situated on this axis.

	The equation of this body is governed by the equation of dynamics:
	
	We have seen in the section of Classical Mechanics that the general solution of this equation is:
	
	with for pulsation:
	
	The total energy of the system being the classical Hamiltonian, we write:
	
	It follows from the expression of the potential energy that the constant we had in the initial relation which defined the well of parabolic potential:
	
	is simply $k$ and therefore:
	
	but which we will write for the next development and by tradition in the following form:
	
	Now let us come back to our quantum framework. From this point of view we have for Hamiltonian (or total energy):
	
	where for recall, following what we have just seen:
	
	Using what we define as a "reduced writing", we write:
	
	where the "reduced operators" of linear momentum and positions are therefore respectively:
	
	where by a stupid tradition wee keep the constant instead of replacing by by $\omega_0^2m$.
	
	It is more or less easy to obtain the relation of commutation:
	
	\begin{dem}
	Remember the relations below that we saw in our study of functional linear operators at the beginning of this section:
	
	Let us consider the properties of the commutators with the linear momentum. We have also proved above earlier above the relation:
	
	By multiplying the latter by $\hbar/\mathrm{i}$, it comes:
	
	That we can also write:
	
	If you remember the definition of the of commutators ($[\alpha,\beta]=\alpha\beta-\beta\alpha$), we have:
	
	We therefore have for our oscillator:
	
	Let's write the definition of the commutator:
	
	Therefore:
	
	that's was what has to be proved...
	\begin{flushright}
		$\square$  Q.E.D.
	\end{flushright}
	\end{dem}
	We now have an interest for solving the differential equation to use the non-Hermitian operators equation defined by\footnote{This approach of solving this problem is sometimes named "Ladder operators\index{ladder operators} for the quantum harmonic oscillator"} (this is a definition so do not try to understand!):
	
	What thus defines us the operators (by temporarily putting $m=1$):
	
	which are linear combinations of the position and linear momentum operators and are trivially not Hermitian (not equal to its own conjugate for recall!).
	
	We see these two operators very frequently in Quantum Physics, especially in Quantum Field physics, and physicists then speak of "\NewTerm{creation operator $a^\dagger$}\index{creation operator}" or "\NewTerm{raising operator $a_{+}$}\index{raising operator}" and of the "\NewTerm{destruction operator $a$}\index{destruction operator}" or "\NewTerm{lowering operator $a_{-}$}\index{lowering operator}" (also named "\NewTerm{annihilation operator}\index{annihilation operator}"). The reasons of these names will be more clear later (it seems that it is Dirac himself that introduce them for the first time in the framework of the harmonic oscillator)!

	Given the commutators relation, we check (we need that for the next developments):
	
	\begin{dem}
	First:
	
	and:
	
	\begin{flushright}
		$\square$  Q.E.D.
	\end{flushright}
	\end{dem}
	And on the other hand we also need:
	
	\begin{dem}
	
	and therefore by dividing by $2$ on both sides of the equality, we get:
	
	\begin{flushright}
		$\square$  Q.E.D.
	\end{flushright}
	\end{dem}
	Now let us come back to:
	
	and using:
	
	where it appears that $N$ can be seen as an operator, named "\NewTerm{counter operator}\index{counter operator}" (hermitic) since:
	
	The same reasoning and assumption bring us to:
	
	with:
	
	It is then sufficient to quantify the counter operator to know the eigenvalues and the eigenstates of $H$ since these two elements are then linked by:
	
	Therefore in general using the other notation:
	
	therefore the Schrödinger equation can be written:
	
	We now do the assumption that $\Psi$ is an eigenfunction of $N$ associated with the eigenvalue $n$ such that:
	
	This assumption is very important because we will use it as an induction principle to find all the eigenfunctions starting from the fundamental one!

	Let us now establish commutator relations between $N$ and $N'$ and the operators $a$ or $a^\dagger$. For this, let us first multiply $aa^\dagger-a^\dagger a=1$ by $a^\dagger$, then we get:
	
	Similarly, by multiplying the $aa^\dagger-a^\dagger a=1$ by $a$, we get:
	
	Since according to our assumption $\Psi$ and $n$ are respectively eigenfunction and eigenvalue of $N$, we can write:
	
	But as we have:
	
	which multiplied on both side of the equality by the wave function $\Psi$ gives the relation:
	
	This equation has the following consequences:
	\begin{itemize}
		\item We have $a\Psi=0$ such that $Na\Psi=na\Psi$
		\item Or we have $a\Psi$ that is an eigenfunction of $N$ for the eigenvalue $(n-1)$
	\end{itemize}
	The same reasoning would establish that $a^\dagger\Psi$ is an eigenfunction of $N'$ for the eigenvalue $n' + 1$, if it is not zero (we shall see later that $a^\dagger\Psi$ is never zero):
	
	With the notations above it was still not clear for one of our read with the operator were named creation and annihilation operators. Let us see this with another approach before we continue! The following development was more explicit to explain their name (we keep the other notation of these operators for obvious reasons):
	
	By the same argument, we can show that $a_{-}\Psi$ is a wave function with energy $E-\hbar\omega_0$. For this reason $a_{+}$ and $a_{-}$ are called the raising and lowering operators, respectively:
	
	Now, since the harmonic oscillator potential is parabolic, we would expect no upper limit to the energy levels, but we would expect a lower limit, so that applying the lowering operator to the ground state should give us zero.
	
	Therefore we know that there exists a lower eigenvalue $n_0$ smaller all the others corresponding to the fundamental level (according to the Bohr-Sommerfeld model this eigenvalue always exists).
	
	Necessarily, its eigenfunction equation obeys the relation (the reader will be able to check with the results further):
	
	as they can not be , as we have already mention it, a level lower that the lower limit by applying the lowering operator!
	
	Otherwise $n_0-1$ would a eigenvalue and there would a physical contradiction.

	By multiplying this last relation by $a^\dagger$ we get:
	
	Which shows that the minimum eigenvalue $n_0$ is zero. We therefore know the fundamental level of the oscillator:
	
	named "\NewTerm{zero-point energy}\index{zero-point energy}".
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	It should be noticed that the oscillator is never in a state of rest (put $n = 0$ in the expression of the energy) which also means that the absolute zero can not be accessible since the temperature "fix" the atomic agitation and that a rest state does not exist!
	\end{tcolorbox}
	To get the corresponding eigenfunction, we need the explicit expression of $a$. According to:
	
	Then we have:
	
	which gives us:
	
	Hence:
	
	But according to $a\Psi_0=0$:
	
	hence:
	
	thus (resolution of a simple differential equation):
	
	where $A$ is the constant of integration. Thus, explicitly:
	
	We can normalize this function using the standard Gauss integral (\SeeChapter{see section Statistics}):
	
	so we get:
	 
	Therefore:
	
	So finally:
	
	It remains to build the other eigenfunctions and to normalize them. Indeed, if $\Psi_n$ is a normalized function associated with the level $E_n$, we have seen above that $a^\dagger\Psi_n$ is an eigenfunction associated with the level $n+1$ (the operator of creation $a^\dagger$ increase the energy as we have show it...), but there is no reason to normalize it again since it is precisely associated with an eigenfunction already normalized!

	Therefore we can write:
	
	Where $\alpha_n$ is a coefficient to be determined. Let us express the fact that $\Psi_n$ is already normed:
	
	Considering the relation $aa^\dagger-a^\dagger a=1$ we have:
	
	Let us recall that $N(a^\dagger\Psi)=(n+1)(a^\dagger\Psi)$ therefore:
	
	We have just verified on the way that $a^\dagger\Psi_0$ is never zero (fact that we had supposed above).

	All the functions $\Psi_n$ (except the already $\Psi_0$ already fixed) have an arbitrary phase factor (concept that we have seen during the definition of bounded and unbound states), independently of each other, the argument $\alpha_n$ therefore remains available to us and we will choose $\alpha_n$ as being a fixed positive real number. This fixes all the $\Psi_n$ according to the previous development:
	
	By iterating this relation on the wave function, we obtain easily (elementary algebra but that we can detail on request):
	
	by taking into account the following relations (which we have already proved just earlier above):
	
	We then have:
	
	This relation takes a simpler form, relying on the relation:
	
	Let us check this:
	
	hence, in the language of operators:
	
	Therefore:
	
	We thus obtain the expression of $\Psi_n$:
	
	Moreover, in the mathematical theory of families of orthogonal polynomials with the weight (with respect to the weight function\footnote{If you don't remember what is the weight function, you can refer of the section of Function Analysis} $w(x)=e^{-x}/2$), we encounter the "\NewTerm{Hermite polynomials}\index{Hermite polynomials}" defined by (physicist versions):
	
	They are polynomials of degree $n$, even or odd ($H_0=1$, $H_1=2Q$, $H_2=4Q^2-2$):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/atomistic/hermite_physicists_polynomials.jpg}	
		\caption{The first five (physicists) Hermite polynomials (source: Wikipedia)}
	\end{figure}
	By using these polynomials, the notation of:
	
	is simplified and becomes:
	
	Thus fully explicitly:
	
	These polynomials therefore constitute an orthonormal basis of the global quantum state and thus appear naturally in the general expression of the functions / eigenstates.

	Finally we have:
	
	With the no less famous graphic representation with on the left the eigenfunctions associated $\Psi_n$ and on the right the probability of presence $|\Psi_n|^2$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/harmonic_oscillator_plot_first_levels_energy_density_probability.jpg}	
		\caption{Plot of first eigenfunctions and density functions of some energy levels of the $1$D harmonic oscillator}
	\end{figure}
	We can easily get the left part with Maple 4.00b (the example below is not normalized from hence the abscissa which is a bit special compared to the figure above!):
	
	\texttt{>m:=1;omega:=1;h:=1;\\
>plot([(m*omega/(Pi*h))\string^(-1/8)*exp(-m*omega*x\string^2/(2*h)),-((m*omega/(Pi*h))\\
\string^(1/4)*2)\string^(-1/2)*2*sqrt(m*omega/h)*x*exp(-m*omega*x\string^2/(2*h)),((m*omega/(Pi*h))\\
\string^(1/4)*8)\string^(-1/2)*(4*m*omega/h*x\string^2-2)*exp(-m*omega*x\string^2/(2*h)),-((m*omega\\
/(Pi*h))\string^(1/4)*48)\string^(-1/2)*(8*(m*omega/h)\string^(2/3)*x\string^3-12*sqrt(m*omega/h)*x)\\
*exp(-m*omega*x\string^2/(2*h))],x=-6..6);}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/atomistic/harmonic_oscillator_plot_maple.jpg}	
		\caption{Plot of first eigenfunctions of the $1$D harmonic oscillator with Maple 4.00b}
	\end{figure}
	By analyzing these wave functions, we fall back on many classical results: the particle in the potential well has a wider probability of presence if it has a higher energy (a ball at the bottom of a well will rise higher on The edges if it has more energy), the particle is more likely to find itself on the positions distant from the center of the well when its energy is high.

	For all calculations where particles are in a potential well, the harmonic approximation is very interesting. For example, if we wish to study a two-dimensional "harmonic trap\index{harmonic trap}", ie 2D  Bose-Einstein condensate (\SeeChapter{see section Statistical Mechanics}), we can set the following Hamiltonian to begin the study (in analogy with the one at $1$ dimension used above):
	
	
	
	\paragraph{Tunnel Effect (barrier potential)}\mbox{}\\\\\
	The "\NewTerm{tunnel effect}\index{tunnel effect}" or "\NewTerm{quantum tunneling}\index{quantum tunneling}" refers to the property of a quantum object to cross a potential barrier, which can not be explained by Classical Mechanics. Generally, the wave function of a particle, whose square of the module represents the amplitude of its probability of presence, does not cancel at the level of the barrier but attenuates within the barrier (see the previous plot of the harmonic oscillator!), practically exponentially for a fairly wide barrier as we will prove it soon. If at the output of the potential barrier the particle possesses a non-zero probability of presence, it can thus cross this barrier.

	The theoretical study of this phenomenon is of crucial importance in the theory of semiconductors (\SeeChapter{see section Electrokinetics}) and disintegration in nuclear physics (\SeeChapter{see section Nuclear Physics}). It is therefore necessary to pay special attention to it!

	The quantum barrier of width $L$ separates in simple cases the space in three (in the $1$ dimensional case), the left and right parts of which are considered to have constant potentials up to infinity. The intermediate part constitutes the barrier, which may be complicated, revealing a soft profile, or on the contrary formed by rectangular or possibly other type of barriers/wells.

	Let us now consider the case of systems where the potential energy $E_p$ (implicitly the relative potential) tends to finite boundaries, not necessarily equal when $x\rightarrow \pm \infty$. It is therefore a problem of unbounded states.

	First, we define a region \texttt{I} far to the left where $E_p(x)$ will be denoted:
	
	A region \texttt{III} far to the right where $E_p(x)$ will be denoted:
	
	By limiting ourselves to the simplest situations, there are three possibilities relative to the relations given above: 
	\begin{itemize}
		\item potential well (a)
		\item potential step (b)
		\item potential barrier (c)
	\end{itemize}
	as represented in the same order in the figure below:
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/type_potentials_tunnel_effect_study.jpg}	
		\caption{Classical types of potential variation for Tunnel Effect study}
	\end{figure}
	Now, let us write Schrödinger's equation:
	
	In the regions \texttt{I} and \texttt{III} of the potential barrier, the idea is that $E_{\text{tot}}-E_p$ is constant and positive so the differential equation can be written in one dimension:
	
	We thus get very simply the analytic expression of $\Psi$ in these regions in general form:
	
	We fall back on these two expressions that are identical to those of our study of the potential well with rectangular walls, with the difference that we have written more general solutions of the differential equation (\SeeChapter{see section Differential and Integral Calculus}) without having determine the coefficients (because we are interested here in a generalization).

	Thus, in our study of the well with rectangular walls earlier above we had already determined that:
	
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} We see that the wave numbers $k$ are therefore proportional to the root of the kinetic energy. And since the kinetic energy is proportional to the square velocity of the particles, then the velocity is proportional to the wave number (and vice versa)!

	\textbf{R2.} In some textbooks, in order to simplify the notations, the potential in regions \texttt{I} and \texttt{III} is assumed as a reference and therefore equalized to $0$. The term $E_p$ therefore disappears from the two preceding expressions and this has the effect of equalizing the both wave numbers $k_{\texttt{I}}$, $k_{\texttt{III}}$ which are then simply denoted $k$.
	\end{tcolorbox}
	In region \texttt{II}, the idea is that $E_{\text{tot}}-E_p$ is negative and constant so the differential equation can be written in one dimension:
	
	and as we have seen it in our study of the infinite rectangular potential well according to the second approach, the solution is then of the form:
	
	with:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The parenthesis under the root of the preceding relation must therefore be positive. This would mean that the kinetic energy of the particle is negative. To overcome this problem within the framework of this simplified model, it is said that the particle has no right to exist in the barrier and that the particle borrow of energy from vacuum. But there are other more complex models that do not require this kind of "fantasies".
	\end{tcolorbox}
	We thus get very simply the analytic expression of $\Psi$ in the three regions in general form:
	
	Let us suppose now that we have at $-\infty$ (region \texttt{I}), a source of particles (which sends them to the right), with a kinetic energy obviously equal to $E_{\text{tot}}-E_{p}^{\texttt{I}}$.
	
	Thus, these particles have an energy $E_\text{tot}$ and the wave function that describes them obeys the Schrödinger equation. In Region \texttt{III}, it will be assumed that there can be only particles going to the right (no source at $+\infty$ by hypothesis).

	The region \texttt{III}, as well as region \texttt{I}, is of infinite extension, so the principle of uncertainty allows us to speak in theory of a perfectly determined linear momentum which we denote by $p'$.

	We know that (this is Classical Mechanics!) in the region \texttt{III} we then have:
	
	If $E_\text{tot}>E_p^{\texttt{III}}$ then $p'$ is positive, so thanks to the preceding relation and to the De Broglie relation we have:
	
	Therefore:
	
	Since the wave numbers are now known formally, let us return to the interpretation of solution \texttt{III}:
	
	The assumption that the particles come from the left imposes us that $B'=0$ so that the solution only describes particles that go to the right. Then, it is possible, for those coming from the left, to take $A'=1$. The region \texttt{III} is therefore relatively simple to analyze...
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The conditions and assumptions used previously are sometimes referred to as "scattering conditions\index{scattering conditions}".
	\end{tcolorbox}
	The constants $A$ and $B$ of the region \texttt{I} will be completely determined by carrying out the connection of the solutions from one region to the other.

	Let us now turn to the interpretation of the equation in Region \texttt{I}:
	
	It is evident that $Ae^{\mathrm{i}k_{\texttt{I}}}$ describes particles which, in region \texttt{I} are directed to the right, when $Be^{-\mathrm{i}k_{\texttt{I}}}$ describes particles which in this same region are directed to the left. As we know, the first are the incident particles, the second are the reflected particles.
	
	What we ask of Quantum Physics now appears in a clear way: a particle arriving from the left (incident) can either:
	\begin{enumerate}
		\item Continue to the right, that is, cross the region \texttt{II} and become a transmitted particle

		\item Go back to the left and become a reflected particle.
	\end{enumerate}
	We are led to define a "\NewTerm{transmission coefficient $T$}\index{transmission coefficient}"  assimilated to the probability that the incident particle has to cross the region \texttt{II} and a "\NewTerm{reflection coefficient $R$}\index{reflection coefficient}", the probability that the incident particle will be reflected. We must obviously have:
	
	In the case of a potential barrier, $T$ is also named the "\NewTerm{barrier transparency}\index{barrier transparency}".

	To calculate $R$ and $T$, we will define the current fluxes of the various categories of particles (incident, transmitted, reflected).
	
	For example, since the incident particles are described by $A^{\mathrm{i}k_\texttt{I}x}$, the average number of these particles per unit length in region \texttt{I} must certainly be proportional to a given factor with $|A|^2$.
	
	Let $v_{\texttt{I}}$ be their velocity, we see that the flow of the incident particles denoted $j_i$, is then proportional to a given factor to $|A|^2v_{\texttt{I}}$ (through dimensional analysis). Thus, the coefficient of proportionality being of the same nature for the three categories of particles (incident $i$, reflected $j$, transmitted $t$) and from the fact that $v_{\texttt{I}}$ and $v_{\texttt{III}}$ are proportional to $k_{\texttt{I}}$ and $k_{\texttt{III}}$, it follows that $j_i$, $j_r$ (incident flow and reflected flow) and $j_t$ (transmitted flow) are respectively proportional (hence always to a given dimensional factor) ti $k_{\texttt{I}}|A|^2$, $k_{\texttt{I}}|B|^2$, and $k_{\texttt{III}}$ (since we recall that for region \texttt{III} we have found $A'=1$ and $B'=0$).
	
	We deduce from here simply, by a simple ratio, the expressions of the coefficients of reflection $R$ and transmission $T$:
	
	and as in our special case $k_{\texttt{I}}=kk_{\texttt{III}}$ it comes:
	
	Another way of writing things is to say that since the incident wave is summarized to:
	
	and the transmitted wave to:
	
	then:
	
	In all these situations, quantum theory leads in general to small but not zero values of $R$ and $T$!
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us determine the explicit expression of transparency for the example of our rectangular barrier.\\ 

	For this we know that we must impose the continuity of $\Psi$ in $x=0$ and $x=L$, as well as the continuity of $\mathrm{d}\Psi/\mathrm{d}x$ in $x=0$ and $x=L$.\\

	Let us first recall that we have the three relations (by putting the reference of the potential at $0$):
	
	wither therefore:
	
	We then have for the continuity of $\Psi$ in $x=0$ and $x=L$:
	
	as well as the continuity of $\mathrm{d}\Psi/\mathrm{d}x$ in $x=0$ and $x=L$:
	
	Since $B'$ is zero we have a system of $4$ equations with $5$ unknowns:
	
	We will choose to express all the constants from $A$. To do this we multiply the first line by $\mathrm{i}l$ and we sum it to the second line. We then get:
	
	and then we multiply the third line by $-\mathrm{i}k$ and the sum it to the fourth line. We have then:
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	Therefore we have the two relations:
	
	or by putting $\alpha=k/K$:
	
	From the second relation, it comes:
	
	And injected into the first one:
	
	Therefore:
	
	Then we have:
	
	and if we put:
	
	Therefore it comes:
	
	Similarly, re-starting from:
	
	From the second relation, it comes:
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	And injected into the first one:
	
	Therefore:
	
	We then get:
	
	and let us still put:
	
	Therefore it comes:
	
	Notice that we also have:
	
	\end{tcolorbox}
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	We can now express the constants $A'$ and $B$ as a function of $A$ with the help of the preceding relations. Let us first begin with $B$:
	
	and:
	
	Therefore:
	
	So finally we have:
	
	and therefore:
	\end{tcolorbox}
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	using the properties of the complex module (\SeeChapter{see section Numbers}):
	
	It remains for us to calculate:
	
	Therefore:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	We have therefore:
	
	But as (\SeeChapter{see section Trigonometry}):
	
	If $KL\gg 1$ (so at the atomic scale it is rather $K$ which is huge relatively to $L$) we have:
	
	Therefore:
	
	relation that we can found in many textbooks (but without detailed proof as above). Below we have plotted $T$:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/atomistic/transmission_coefficient_tunneling_matlab.jpg}	
		\caption{Plot of the transmission $T$ coefficient with Matlab 5.0.0.473 for tunneling effect}
	\end{figure}
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	following the relation:
	
	\end{tcolorbox}
	
	We thus find in the (special) above example that the coefficient $T$ is very sensitive (exponentially) to a small variation of the width of the barrier, $a$, when the potential of this barrier is small. We can thus visualize atomic sites, for example in Silicium, using a point very close to the material (or "\NewTerm{scanning tunneling microscope (STM)}\index{scanning tunneling microscope}") to be observed. This is the principle of the tunneling microscope where, by approaching a very finely cut conductive point (a few atoms) to a proximity of about $5$ Angstroms of a conductive surface, and imposing a potential difference of a few [mV], we measures a current of a few nanoamperes. The number of electrons passing through the potential barrier (here the vacuum between the two conductive electrodes) decreases exponentially with the width of the barrier.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/scanning_tunneling_microscope.jpg}	
		\caption{Scanning tunneling microscope concept (source: OpenStax)}
	\end{figure}
	By analyzing the signal of  the current passing through the circuit, one can access a very precise mapping of the measured surface of the order of $0.1$ Angstroms in vertical.
	
	We also notice according to the relation obtained that the not "heavy" particles like the electrons have a greater probability of making a tunnel effect than heavier particles because of the term of mass involved.
	
	STM images are in therefore gray scale, and coloring is added to bring up details to the human eye:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/stm_nanotube.jpg}	
		\caption{An STM image of a carbon nanotube (source: OpenStax)}
	\end{figure}

	Using the relation obtained previously, one can quite simply calculate the probability that a human being of mass $m$ crosses a wall with a height $h$ (it is then easy to calculate the potential energy) and a thickness $a$. The probability is of the order of $10^{-4\cdot 10^{30}}$....

	However, the most famous example of Quantum tunnelling that can be treated is that of the emission of $\alpha$ particles by radioactive heavy nuclei, the explanation of which was given by the Russian physicist G. Gamov in 1928.

	The proof is relatively simple, but since it constitutes a particular practical case, we have not detailed it in this section, but in that of Nuclear Physics. However, to solve this problem, we need to use an approximation method known as the "W.K.B. method" Named after the physicists Wentzel, Kramers and Brillouin.
	
	The results therefore give a transmission factor $T$ for the $\alpha$ "particle" of:
	
	For the Uranium atom $_{92}^{238}\mathrm{U}$. Moreover, in the semiclassical approximation, the particle $\alpha$ has, in the well, a velocity of the order of $10^7\;[\text{m}\cdot\text{s}^{-1}]$, and it goes back and forth in a nucleus whose radius is of the order of $10^{-14}$ [m]. It thus performs approximately $10^{21}$ oscillations per second where each time it has a probability $T$ to cross the potential barrier. This probability per unit time is thus determined by:
	
	Experimentally, we find:
	
	The model that we will see in detail in the section on Nuclear Physics gives therefore quite satisfactory results.
	
	And here is how looks like  a "typical" STM in this beginning of the 21st century:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/atomistic/stm_omicron.jpg}	
		\caption{Omicron Scanning tunneling microscope (source: Omicron)}
	\end{figure}
	Besides these technical examples, we encounter the phenomenon of quantum tunneling in a much more accessible and very pedagogical case: Thus, when under the condition of total reflection of a beam of light, we approach another prism (on the face of the prism where no ray of light comes out or re-enters) so as to produce a sufficiently thin air space, a small transmitted beam is observed. This is sometimes named "\NewTerm{optical tunneling}\index{optical tunneling}".
	
	In the table we summarize some of the properties of the systems studied in this so far. The table gives an abbreviated name for each idealized system, and an example of a physical system whose potential and total energies are approximated by the idealization. It also gives sketches of the forms of the potential and total energies, and corresponding probability density functions, for each system. If the particle is not bound, it is incident from the left. We have chosen one significant feature of each system to list in the table, but there are many other significant features that we have discussed, which are not listed. In fact, in this chapter we have obtained most of the important predictions of quantum mechanics for systems involving one particle moving in a one-dimensional potential. In the following chapters we shall obtain predictions from the theory for systems involving three dimensions and several particles.
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/barrier_potentials_types.jpg}	
		\caption[]{A summary of the systems studied so far (source: \cite{eisberg1985quantum})}
	\end{figure}
	
	\pagebreak
	\subsubsection{Superposition principle}
	At the beginning of the section we have already introduce the basics of quantum superposition during our study of the linear combination of quantum states. We would like now to deepen the subject a bit more to get some very important results.

	Let us recall that the notion of the dynamic state of a classical system plays a crucial role in Classical Mechanics.

	Is it possible to fall back on this concept when we are dealing with a quantum system, that is to say a system such as an atom, a nucleus or a molecule, in short a microphysics system?
	
	At first glance no! Because we know that one defines the dynamic state of a classical system by the data of the generalized coordinates $q_i$ and conjugate moments $p_i$ at a given instant (\SeeChapter{see section Analytical Mechanics}). However, the principle of uncertainty is opposed to this procedure as soon as we are in the field of microphysics, given the impossibility of accurately measuring the $q_i$ and $p_i$. This is particularly clear when the system is reduced to a single particle which we describe by its cartesian coordinates $q_i(x,y,z)$ and the components of its linear momentum $\vec{p}(p_x,p_y,p_z)$.

	Fortunately, there is another definition of the dynamic state of a system which applies indifferently to Classical and Quantum systems and which, in the case of the former, is identified with the usual definition. We will give this definition on the basis of a brief theory of the sets of identical systems.
	
	If we have a set $(E)$ of a very large number of identical systems, we will make a statistical survey to characterize this set as follows: we take a system of the set, we measure a dynamic variable (coordinate, linear momentum, kinetic energy, etc.) and we reject the system (which disturbed by the measure, must not be reincorporated to the initial set). We thus draw up a balance sheet which is translated by distribution functions of all the possible dynamic variables. This makes it possible to define unambiguously the notion of "identity":
	
	\textbf{Definition (\#\mydef):} Two physical sets are say to be "\NewTerm{identical physical systems}", if the results of the measurements are the same for the both.
	
	Let us now consider a unique set $(E)$. Is it possible to realize it by juxtaposition of two sets non-identical sets $(E_1)$ and $(E_2)$? This would allow us to write:
	\\
	If Yes, we will say that $(E)$ is a "\NewTerm{mixture of physical systems}". Conversely, by suitable sampling, a mixture can be broken down into two different sub-systems. If not, we will say that $(E)$ is a "\NewTerm{pure set}" or "\NewTerm{pure physical system}". Any sampling will decompose the pure set into two subsets identical to each other and necessarily also identical to $(E)$! We then agree to say that all systems of a pure set are in the same dynamic state and that two different pure sets give rise to different dynamic states. It goes without saying that the systems constituting a mixture will be in different dynamic states.
	
	Suppose now that the studied systems obey the laws of Classical Mechanics. If the systems of a set have different pairs $(q_i,p_i)$, we sort them by grouping them by systems all having a same pairs $(q_i,p_i)$. We check then that the new definition of the dynamic state coincides with the usual definition. Let us notice this obvious but important fact (as opposed to quantum systems): in a pure set of classical systems, that is to say for a given dynamic state, every dynamic variable is well determined. Indeed, in Classical Analytical Mechanics, such a variable is a function of the $q_i$ and $p_i$ and, therefore, has a unique value.
	
	Let us pass to quantum systems. It is now possible to define for them a dynamic state, but immediately we see a fundamental distinction with Classical Mechanics. Indeed, in a pure set of quantum systems, that is to say for a given dynamic state, a dynamic variable is not, in general, well determined as already mentioned. When we measure it on systems extracted from the pure set, we usually do not find a single value, but a distribution of values as a result.
	
	The incertitude which reigns over the value of a dynamic variable in a given dynamic state is therefore purely quantum in nature and it is well to distinguish it from the indeterminacy of statistical origin manifested in a mixture, whatever they are classical or quantum systems.
	
	The formalism of quantum physics can only be edified if we know how to describe mathematically dynamic states and dynamic variables. We have seen that we can not expect from this formalism a precise prediction as in Classical Mechanics, but simply the probabilities of obtaining this or that value when we measure a dynamic variable on a system whose dynamic state is given.
	
	The whole theory we have seen so far allows us to conclude that the dynamic states of a system of a spin-free particle are described by complex, wave functions non-zero everywhere.

	If we apply this condition to dynamic systems, then we have the following assumption:
	
	$\Psi$ is then a wave function describing a possible dynamic state of the system. What is often written  with the Dirac ket-bra notation in the following form:
	
	This postulate seems quite natural because of the wave aspect of the physics of microsystems. Indeed, in the wave phenomena of classical physics, wave equations are, in most cases, linear homogeneous, and it follows that the waves can be superimposed (\SeeChapter{see section Wave Mechanics}). Now, the great interest of this postulate is that it contains in germ the explanation of this fundamental fact which is quantum indeterminacy (also sometimes named "\NewTerm{quantum coherence}\index{quantum coherence}").
	
	Let us see it on a very simple case where we assume that a dynamic variable $A$ has a well-defined value $a_1$ in the dynamic state $\Psi_1$ and a well-defined value $a_2$ in the dynamic state $\Psi_2$ with $a_1\neq a_2$. This means that if we repeat the measurement of $A$ on systems all in the dynamic state described by $\Psi_1$, we will find each time $a_1$, same for $\Psi_2$ with $a_2$.
	
	A question comes naturally to mind: if we measure $A$ on systems all in the dynamic state $\Psi$ what are we going to get? A naive idea would be to believe that $A$ will take a well defined intermediate value between $a_1$ and $a_2$.

	These two assumptions are false and we know that! First, $A$ is not well defined in quantum physics (uncertainty) and second it is not necessarily mathematically located between $a_1$ and $a_2$. The correct interpretation is as follows:
	
	If we measure $A$ on the system in the dynamic state $\Psi$, we find as a result of measurement, sometimes $a_1$, with a probability $p_1$, sometimes $a_2$, with a probability $p_2=1-p_1$. Of course, $p_1$ and $p_2$ and will have to be calculated according to $\lambda_1$ and $\lambda_2$.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The reader must especially not confuse the pure set of systems described by $\Psi$ with the mixture that we would obtain by juxtaposing two pure sets of systems respectively $\Psi_1$ and $\Psi_2$.
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider the following superposed academic case:
	
	We can already easily verify that the system is normalized:
	
	Since the superposition is normalized, then it comes the probability of finding the system in for example in the state $\Psi_1$ which is then under the assumption of the orthonormality $\langle \Psi_i|\Psi_j \rangle=\delta_{ij}$:
	
	and if we do the same for each of the other two states the sum of the probabilities will always be equal to $1$. It should be pointed out that this probability is also the proportion of states that will be measured in the state $\Psi_1$ if the system is constituted of $N$ identical components.
	\end{tcolorbox}
	In fact, the interpretation given by De Broglie's theory (associate a wave function to a particle) with the principles of uncertainty is the most striking and well known example of superposition principle of states in quantum physics (Schrödinger's Cat case putted aside)! To see why, let us consider a De Broglie wave propagating in the direction of the $x$-axis, but limited to an interval $(-a,+a)$ at a given instant ($t=0$ if we want). Hence at $t=0$ the wave is written, dropping the multiplicative constant:
	
	If we measure the coordinate of the particle, we must find it there necessarily where $\Psi$ is not zero (otherwise we could not measure anything). We can say that $x=0$ with obviously an uncertainty $\Delta x=a$ (the interval where we are sure to find the particle with respect to the the origin divided by two: $d((-a,+a))/2$.
	
	If we measure $p$, what do we find? We must not find $\hbar k_0$ (a relation which we have already proved earlier above), as this would be true for an indefinite plane wave, which is not the case here. Then we will decompose the wave into plane waves by means of the Fourier transform (\SeeChapter{see section Sequences and Series}):
	
	How to interpret this relation? One of the elementary plane waves (which we can also interpret as a state):
	
	whose sum gives back $\Psi(x)$, leads to a value $p=\hbar k$ of the linear momentum. But, the values of $k$ form a continuum. We are led to say that the possible values of $p$ then form also continuum and that there is therefore an uncertainty about the value of $p$. To go further, one has to evaluate $a(k)$ (which must be considered as variable of the probability of presence of each plane wave coming from the decomposition of $\Psi(x)$) by means of the relation (according to the properties of the Fourier transform proved in the section Sequences ans Series):
	
	which gives (reduced to) here:
	
	Let us put $k_0-k=\Delta k$, then the integral becomes:
	
	Let us recall the plot of $\sin(u)/u$ (that is the "cardinal sine" as seen in the section of Trigonometry) that show that $a(k)$ takes values that can be considered negligible for $|u|>\pi$:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/geometry/sinus_cardinal_2d.jpg}
		\caption{2D plot of the sinc function with Maple 4.00b}
	\end{figure}
	It follows that in the integral:
	
	It is the $k$ at the neighborhood of $k_0$ that are effective, and more precisely the $k$ such that:
	
	since:
	
	It follows that the values to be retained of $p$ are those close to $\hbar k_0$, more precisely we have if we multiply the priore previous relation by $\hbar$:
	
	Thus:
	
	That is:
	
	Therefore:
	
	Finally (it is usage to replace $a$ by the letter $x$):
	
	So we fall back on a well know result already introduced at the beginning of this section but now with an inequality "$<$".
	
	Similarly, if we propose to determine the $x$-coordinate of an electron by passing it through a slit of width $2b$ pierced in a screen:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/electron_through_slot.jpg}
		\caption[]{Study configuration of the electron passing through a rectangular slot}
	\end{figure}
	The precision with which we know the position of this electron is limited by the size of the slit, ie $\Delta x=b$. On the other hand, the slit disturbs the associated wave. The result is a modification of the electron's motion, which is reflected by the diffraction pattern of the wave (which is in fact a representation of the linear superposition of its intrinsic states).

	The uncertainty on the dynamic component $p_x$ of the electron linear momentum is determined by the angle $\theta$ corresponding to the central maximum of the diffraction pattern. According to the theory of diffraction (\SeeChapter{see section Optical Wave}) produced by a rectangular slot, we have $\sin(\theta)=\lambda/2b$ since the intensity $I(\theta)$ is written:
	
	Thus $p_x$ is comprised between $p\sin(\theta)$ and $-p\sin(\theta)$, $p$ being the linear momentum of the incident electron. Thus the uncertainty $\Delta p_x$ is:
	
	This simple result is quite extraordinary if we put it in relation, \underline{in order of magnitude}, with the result we had obtained just above and that was for recall:
	
	We can draw several conclusions of the first importance:
	\begin{enumerate}
		\item The associated De Broglie wave  is closely related to the principle of uncertainty and quantum physics must take simultaneously in account these two properties.

		\item If we take into account that the distribution of the intensity is obtained from the counting of the electrons (or particles) as a function of the angle and that we get the same distribution regardless of the intensity of the monokinetic electron beam that arrives on the slot, even if the electrons are sent one by one. We observe then that the motion of particles is no longer deterministic but probabilistic. Thus, the wave equation of the electron can be considered as a linear superposition of the states defined each as we have done previously, that is to say by its possible spectral decomposition by the Fourier transform.
	\end{enumerate}
	The observation of interference patterns in double-slit experiments with massive particles is generally regarded as the ultimate demonstration of the quantum nature of these objects (\SeeChapter{see section Wave Optics}). Such matter–wave interference has been observed for electrons, neutrons, atoms, and molecules quite huge molecules (phthalocyanine) and, in contrast to classical physics, quantum interference can be observed when single particles arrive at the detector one by one. The build-up of such patterns in experiments with electrons has been described as the most beautiful experiment in physics!
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/atomistic/molecules_quantum-superposition.jpg}
		\caption{Comparison of interference patterns for $\mathrm{PcH}_2$ and $\mathrm{F}_{24}\mathrm{PcH}_2$ (source: \cite{juffmann2012real})}
	\end{figure}
	What can we conclude from all that we have seen so far:
	\begin{enumerate}
		\item The equations of Wave Quantum Physics give us a probability density to find a particle in a certain volume of space-time.

		\item The linear superposition of states can be interpreted as the fact that it is possible to find a particle at several points in space-time at a given instant, and with for each of these points a certain probability of finding it there (by possible decomposition of the wave equation).
	\end{enumerate}
	If point (1) has been widely studied so far on this site, point (2) is new and arises from a simple mathematical operation of decomposition or superposition.
	
	But what happens if we try to measure the energy of an atom that is in a superposition of states of energy? We will never detect this superposition, but only one of the energies that constitute it, the action of measuring eliminates the superposition of the states in favor of a single one - we speak then of "\NewTerm{quantum decoherence}\index{quantum decoherence}" (this is the Copenhagen interpretation which we mentioned implicitly at the very beginning of this section and to which we shall return later). But which one? Quantum physics can not simply answer this question for now.... It seems that this choice is made randomly! On the other hand, in the absence of predicting the precise state which will be measured among all those who constitute the superposition, quantum theory can give the probability that one has to measure each state (what we have already repeated many times, right here). If many measurements are made, we finally find the proportions predicted by the theory (even if each measure is unpredictable).
	
	Erwin Schrödinger, had emphasized the absurdity (according to him) of these superpositions by resorting to a thought experiment become famous: Imagine a cat enclosed in an airtight box. In the box is also a radioactive atom and a device capable of spreading poison. When the radioactive atom disintegrates, it triggers the deadly device: the poison spreads into the box and the cat dies.
	
	The absurdity of this experience (that we will in-deep in the section of Quantum Computing) is obvious ... but difficult to prove, at least until we understand what distinguishes a Cat from a Particle. Always the problem of the quantum-classical boundary ...

	It was not until the 1980s that the situation finally progressed, both on the front of experience and on that of theory. In 1982, Wojciech Zurek, a researcher at the Los Alamos National Laboratory in New Mexico, takes up a very simple but brilliant idea: in a measurement, what produces decoherence, is the interaction of the system with its environment. More generally, quantum objects are never completely isolated from their environment - we mean everything that interacts with the system: an apparatus, air molecules, light photons, etc. So that in reality the quantum laws must be applied to the whole consisting of the object and all that surrounds it! However, Zurek shows that the multiple interactions with the environment lead to a very rapid destruction of the quantum coherence of the superpositions of states (also called "\NewTerm{quantum interference}\index{quantum interference}" since mathematically one deals with wave functions). By destroying interferences, the environment suppresses state superpositions and quantum behavior, so that only simple states remain and classic behavior returns.
	
	In a macroscopic object - a cat for example ... - each of the atoms is surrounded by many other atoms that interact with it. All these interactions spontaneously cause a jamming of the quantum interferences which disappear very quickly. This is why quantum physics does not apply to our scale (the probability that it happens is too small): systems are never isolated!

	The speed of the decoherence increases with the size of the system: a cat that has $10^{27}$ particles, "decoheres" in $10^{-23}$ seconds, which explains why we have never seen zombie living-dead cat until today...!
	
	Quantum physics is therefore a theory:
	\begin{itemize}
		\item Non-deterministic (probabilistic) hence the fact that it is considered as a theory of information

		\item Non-local: quantum objects may simultaneously have several positions

		\item non-separable: several quantum objects can be superimposed so that they can not be considered separately.
	\end{itemize}
	Another excellent example of the linear superposition of states is a remarkable application to the principle of least action!

	To see this let us consider a quantum particle going from point $M_1$ at a moment $t_1$ to a point $M_2$ at a moment $t_2$. We know that the probability of finding a particle at a given point and at a given instant is related to the square of the module of the wave function associated with it. Let us put in the simplest case where the wave function of the particle is a undimensional plane wave $\Psi(x,t)$ given by the solution function of the Schrödinger equation of evolution:
	
	where $\lambda$ and $\nu$ are respectively the wavelength and frequency of the wave associated with the particle.

	The particle can take an infinite number of paths to get from $(M_1,t_1)$ to $(M_2,t_2)$. Let us choose any one of these paths which we shall denote by the letter $C$. We can cut the path $C$ into an integer number of lengths $\mathrm{d}t$.
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/path_decomposition.jpg}	
	\end{figure}
	After having traveled the first part of the path, the wave function has the following value:
	
	From this we get that:
	
	But, Planck and De Broglie have established the following relations as we have showed it earlier above:
	
	hence, by replacing $\lambda$ and $\nu$ in the preceding relation, we get:
	
	Applying the same technique for the following part we get:
	
	Proceeding like this from part to part along the path $C$ then we get the value of the wave function on $(M_2,t_2)$ for the particle coming from $(M_1,t_1)$ by following the path $C$:
	
	Now, let the take the limit of the duration $\Delta t$ of each trajectory part to zero such that $\Delta t\rightarrow \mathrm{d}t$. The quantity $\Delta x/\Delta t$ then tends to the instantaneous velocity $\mathrm{d}x/\mathrm{d}t$ of the particle that we will denote by $\dot{x}$. The preceding relation then becomes:
	
	In the chapter of Analytical Mechanics, we have shown that the quantity $p\dot{x}-E$ is equal to the Lagrangian $L$. By substituting the Lagrangian in the preceding relation, we get:
	
	where $S_C$ is the action of the particle that has traveled the path $C$.
	Notice (without proof but we can detail on demand) that the module $\Psi_C(x_2,t_2)$ takes the same value for:
	
	for all $n\in\mathbb{N}$. The Planck constant then finds a physical significance directly related to the action of the particle!

	Let us recall the De Broglie condition of normalization:
	
	which therefore gives the probability that the particle, starting from $x_1$ at the instant $t_1$, is in $x_2$ at the instant $t_2$, having traveled the path $C$.

	The total probability is thus:
	
	To calculate the probability that the particle going from $x_1$ at the instant $t_1$ to $x_2$ at the instant $t_2$ requires to makes the sum of the contributions of each path either (by applying the principle of linear superposition since we perform a sum of the wave functions):
	
	That latter equality is most of written as:
	
	where $\mathcal{D}x$ denotes integration over all paths.
	
	This integral was discovered by Richard Feynman and is named "\NewTerm{Feynman path integral}\index{Feynman path integral}" or sometimes "\NewTerm{sum over histories method}\index{sum over histories method}". In the first analysis it seems to diverge as there exist an infinity of possible paths between two points. Let's take a closer look at what's going on. Let us place ourselves in the case where the trajectory is macroscopic. The value of the action $S_c$ is then much greater than $\hbar$ and varies greatly from one path to another, except for the paths close to the classical physical path for which the variation is almost zero (application of the variational statement of the principle Of lesser action).

	Since the actions of paths intervene as a phase in the path integral above, their contributions are destructive and therefore tend to cancel out, except in the case of paths close to the classical physical path where contributions are added. It follows that the integral of path takes the value of the classical action, indicating that quantum physics makes it possible to find the laws of classical mechanics on a macroscopic scale.
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/feynman_paths.jpg}	
		\caption{Representation of the phases according to the type of path}
	\end{figure}
	The situation becomes very different at the quantum scale, that is, for values of the action $S_C$ whose order of magnitude is that of the constant $\hbar$. An infinity of paths then brings non-destructive contributions. Feynman was able to prove that the path integral converged but on the other hand, it is no longer possible to predict which path the particle will take to the point that the concept of path itself vanishes. Thus on the quantum scale the particle seems to seek its way among all those that are possible but on a macroscopic scale, this quantum trial and error seems to have allowed the particle to find the "right path".

	The path integral formalism is a very original way of approaching and interpreting quantum physics that has been added to those developed by Schrödinger.	 
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	A common question on Internet is: \textit{Are quantum entanglement and superposition principle the same}? In fact, both are different concepts. Superposition of two state means a quantum system is in two state at a time. But, entanglement says the correlation of two or more system in a ensemble. Which means even if two two system are spatially separated the measurement of any observable will be effected by the other.
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{Ehrenfest theorem}
	This theorem makes it possible to connect Newton's Classical Mechanics to Quantum Physics by establishing similar relations with respect to the linear moment and the force.

	For this, we start from the special case of a massive particle moving at a non-relativistic velocity in a potential. We then have the one-dimensional Schrödinger equation of evolution:
	
	from which we get (useful for further below):
	
	If we take the complex conjugate on both sides of the equality and multiply the two members by $\hbar$:
	
	from which we get (useful also for further below):
	
	Let us consider the temporal variation of the mean position of the particle (5th postulate):
	
	We have:
	
	hence:
	
	Let us use this last relation:
	
	Let us now use the relation:
	
	and let us inject it into the prior-previous relation:
	
	The first term to the right of the equality is easy to integrate ... (since there is no need to integrate it...):
	
	and since the wave function must be $0$ at $x=\pm \infty$  (otherwise the energy is infinite) then this last relation is equal zero. We then have:
	
	Thus:
	
	and finally:
	
	Which is the equivalent in Classical Mechanics of:
	
	and which reconfirms the existence of the mathematical being:
	
	as being the linear momentum operator, and which we determined earlier by falling back on Newton's second law.

	But we can do a little better at the level of the classic/quantum analogy by derivating:
	
	Which gives:
	
	hence:
	
	Using:
	
	It comes:
	
	Let us focus on:
	
	Let us integrate the first term twice according to the relation proved in the chapter of Differential and Integral Calculus:
	
	We then have (always considering $\Psi$ as a decreasing function to infinity):
	
	and once again:
	
	So finally:
	
	We then have:
	
	However, we have demonstrated in the section of Classical Mechanics that:
	
	It therefore comes that:
	
	This extraordinarily simple result constitutes the "\NewTerm{Ehrenfest theorem}\index{Ehrenfest theorem}". We thus find the fundamental law of Classical Dynamics in the sense of mean values of position and force, calculated using the probability of presence!
	
	\pagebreak
	\subsection{Angular moment and Spin}
	In classical physics, angular momentum is commonly divided into two types: orbital and rotational. In the case of the Earth, for example, the orbital angular momentum is associated with the revolution of the Earth about the Sun, while rotational angular momentum is associated with the Earth rotating on its axis. Rotational angular momentum is often known as spin, for fairly obvious reasons.

	In classical physics, of course, there is really only one type of angular momentum, since both the orbital and rotational types are associated with the rotation of masses about some axis. The division into the two types is purely a computational convenience.
	
	In quantum physics, the situation is quite different is quite different as we will see (it is important that the reader refers to the Corpuscular Quantum Physics section where we have proved the it is seems not acceptable ton consider the electron spinning on itself).

	Like the harmonic oscillator, the notion of angular momentum is of main importance in quantum theory and has many applications in all fields of physics: atomic and molecular physics, nuclear and subnuclear physics, physics of condensed state, etc. Thus, it plays an essential role in the study of the motion of a particle in a potential with spherical symmetry, as we shall see in the section of Quantum Chemistry (which is an excellent practical example). The angular momentum is also at the base of the group of rotations which satisfies the algebra of the angular momentum operators (\SeeChapter{see section Set Algebra}). Thus, it not only allows to construct the wave function of a given quantum system of symmetry, but also to predict if an optical transition is allowed and to determine its intensity (for example, during the study optical transitions between impurity states (in solid state), molecular states (quantum chemistry), nuclear physics, etc.).
	
	Finally, we will see that the algebraic method applied to the study of angular momentum will allow us to introduce quite naturally the notion of intrinsic angular momentum of a particle, the "spin", which has in fact no classical equivalent (so it's not an angular momentum!).

	The following developments may seem rather disconcerting in the sense that it is no longer necessary to rely on intuition but only on the properties and results of mathematics. As usual, if the reader need additional information, he must not hesitate to contact us.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	"\NewTerm{Spintronics}\index{spintronics}" (a portmanteau meaning "spin transport electronics"), also known as "spinelectronics" or "fluxtronics", is the study of the intrinsic spin of the electron and its associated magnetic moment, in addition to its fundamental electronic charge, in solid-state devices or in Quantum computers. The simplest method of generating a spin-polarised current in a metal is to pass the current through a ferromagnetic material. The most common applications of this effect involve giant magnetoresistance.  (GMR) devices. 
	\end{tcolorbox}
	To begin, let us recall that the angular momentum of a particle relatively to their origin is given by (\SeeChapter{see section Classical Mechanics}):
	
	Since the linear momentum is quantified (it is an eigenvalue related to energy in one way or another), the angular momentum is necessarily also quantified (the angular momentum is therefore an eigenvalue) and the experience has supported this result (Stern-Gerlach experiment).

	Given $z$ the component of the resulting vector product:
	
	This relationship being cyclic, we can change the indices to get the other coordinates.

	Since $x$ and $y$ commute (in the sense that their commutator is equal to zero) and that we have proved earlier above that:
	
	We then have:
	
	Which gives:
	
	Using the gradient (we will see again this relation in the section of Relativistic Quantum Physics during our study of the Pauli equation!!):
	
	and putting for the "\NewTerm{operator of the orbital angular momentum}\index{operator of the orbital angular momentum}":
	
	Which lead us to write:
	
	with:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Most often in the literature the orbital angular momentum is denoted $\vec{L}$ (we have already made this remark in the section of Classical Mechanics) but we avoided this notation here in order to differentiate the orbital angular momentum and the \underline{total} orbital angular momentum.
	\end{tcolorbox}
	We will establish some commutation relations concerning $\vec{l}$ which will play an essential role in our study of the spin. Using the following commutation relations (proved during our study of the Heisenberg classical uncertainty principles):
	
	and:
	
	We have the relation (it is traditional to do the analysis on the component $z$ of the projection of $\vec{l}$):
	
	Therefore:
	
	and proceeding in the same way:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We find analogous relations with the linear momentum:
	
	\end{tcolorbox}
	Let ow now evaluate the quantity (following the request of a reader, we put all the details):
	
	Either after simplification (it is rather annoying for the experience that this does not commute):
	
	By the way, at this stage, the reader that has already covered the Spinor Calculus section beforehand (if not already done we strongly recommend the read to go read it!), he will notice that the Pauli matrices satisfy the preceding relations if we put ourselves in natural units (the reduced Planck constant then being equal to $1$):
	
	This observation will be useful for our study of Relativistic Quantum Physics (see section of the same name). Indeed, we know from our study of Spinor Calculus that the $2\times 2$ matrices in $\mathbb{C}$ of determinant $1$ form the group of rotations in the space $\text{SU}(2)$, whose Pauli matrices are the generators. Basically, the origin of the spin comes from the link between $\text{SU}(2)$ and the group of rotations of our ordinary space, $\text{SO}(3)$ (\SeeChapter{see section Set Algebra}).
	Now, let us consider the norm:
	
	where we must consider the square of one of these operators in the following form:
	
	Let us study its commutation with a component (without having to explicit the thing!):
	
	Using the cyclic relation $[l_x,l_y]=\mathrm{i}l_z$ it comes:
	
	Therefore the norm of the orbital angular momentum commutes with its components:
	
	Conclusions of the results obtained so far: Since the commutator is equal to zero (the quantities commutes), it is therefore possible to simultaneously measure with precision a component as well as the square of the angular momentum (its squared norm), but it is impossible to do the same for two components!
	
	Let us also notice that finally the fact that we have explicitly:
	
	we can write (its quite an interesting relation):
	
	If we have a system of particles numbered by an the index $k$, each has an individual kinetic moment $\hbar\vec{l}^{(k)}$ and the total orbital angular momentum of the system $\hbar\vec{L}$ (not to confuse the notation of the $L$ with that of the Lagrangian that latter being anyway as scalar!!!) is then obviously given by:
	
	and if we simplify by $\hbar$ (or if we work in natural units):
	
	But $\vec{L}$ is not really the total angular momentum of the system! Indeed, a particle may possess an intrinsic angular momentum, or "\NewTerm{spin}\index{spin}". We can give a simple picture of the spin by saying that it translates an infinitesimal rotation of the particle on itself (caution! it is only an image because in fact the particle does not turn on itself!). As we saw in the section of Spinor Calculus, this corresponds mathematically to the limited development of the matrix of rotations in the vicinity of the identity matrix.
	
	Therefore we see that the usual approach to quantum spin is just to postulate that elementary particles have an intrinsic spin, and that this spin isn't due to any physical motion of the particle (again refer to the section of Corpuscular Quantum Physics); it just \textbf{\textit{is}}, in much the same way that particles have mass, charge and (in the case of the more esoteric particles) several other quantities such as strangeness. This is not terribly satisfactory from an intellectual point of view, but since I'm not qualified to write about the details of particle physics, we have to start somewhere.
	
	We will denote by $\vec{s}^{(k)}$ the intrinsic angular momentum of the $k$-th particle (in natural units) and the relation:
	
	will be the "\NewTerm{total spin}\index{total spin}" and finally (still in natural units or after simplification of $\hbar$):
	
	will be the "\NewTerm{total angular momentum}\index{total angular momentum}" of the system (do not confuse the notation $J$ with the orbital angular momentum or the current density....!) and we will prove during our study of the spin-orbit coupling that this angular momentum is a constant of movement in the presence of this coupling.
	
	We will assume (but this is relatively easy to prove once, among others, spinors are know\footnote{As always we can detail on reader request}) that each $\vec{s}^{(k)}$ and $\vec{l}^{(k)}$ also obeys the commutation rules seen previously:
	
	Therefore to sum up we have:
	
	where for recall $\epsilon_{ijk}$ is the Levi-Civita symbol (\SeeChapter{see section Tensor Calculus}).
	
	We notice that in the matrix representation of Heisenberg there are also other matrices than the $2\times 2$ Pauli matrices that satisfy these relations (we can give the detail of the proof on reader request). For example, the following zero trace Hermitian matrices (of which the conjugate transpose is equal to itself for recall ....):
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We will see in the section of Relativistic Quantum Physis that in fact the three matrices above are the spin operator of the massless spin-$1$ particles (photons).
	\end{tcolorbox}
	The prior-previous relations implies (also) in the same way as for the orbital angular momentum:
	
	with obviously the relation:
	
	named by the mathematicians "\NewTerm{Casimir element}\index{Casimir element}" or "\NewTerm{Casimir operator}\index{Casimir operator}" (a simple development perfectly similar to that obtained above is sufficient to prove it and we can write it on reader request).
	
	Let us now define in a purely formal way the two non-Hermitian operators named "\NewTerm{scale operators}\index{scale operators}" (Pauli's matrices and not only (!) still satisfy these relations!):
	
	where respectively $J_+$ is named "\NewTerm{elevator operator}" and $J_-$ "\NewTerm{step-down operator}".

	The $J_{\pm}$ commutes with $||\vec{J}||^2$, since it commutes with $J_x$ and $J_y$. This allows us to write the product:
	
	Furthermore:
	
	Therefore:
	
	Identically (we can detail on reader request):
	
	Finally, let us evaluate the products $J_+J_-$ and $J_-J_+$:
	
	Identically (we can also detail on reader request):
	
	Since the two Hermitian operators $||\vec{J}||^2$ and $J_z$ commute they therefore have common states and eigenvalues and, more precisely, they have a common complete common basis. When observables commuted and have their own common base, let us recall that we usually speak of a "CSCO" (Complete Set of Commuting Operators).
	
	To study their eigenvalues let us put:
	
	System which is sometimes denoted in the following form in the specialized literature (...):
	
	as it highlights the fact that the associated eigenstates will be defined at least in part by the parameters $K$, $m$ themselves.
	
	Many times in textbooks the last couple of relations is written:
	
	or even worst...:
	
	To begin, we know that the eigenvalues $K$ and $m$ are not independent since we have:
	
	The mean being denoted by the brackets $\langle \rangle$, we have by linearity of the mean (\SeeChapter{see section Statistics}):
	
	What can be written:
	
	We see that the left-hand side of the above relation is therefore equal by definition to:
	
	Since the square of the total orbital angular momentum is anyway Hermitian (it has no complex component in $\mathbb{C}$), we then construct by the postulates of quantum physics:
	
	It comes then that:
	
	The latter relation therefore implies that:
	
	This brings so far to the following informations:
	
	From $|\Psi\rangle$, we build the state $J_+|\Psi\rangle$, we will show that if this state is not identically zero, it is an eigenvalue of $||\vec{J}||^2$ and of $J_z$. From the relation:
	
	already introduced previously, we put:
	
	The $J_{\pm}$ commutate with $||\vec{J}||^2$, since it commutes with $J_x$ and $J_y$. This gives us that the previous relation is zero such that:
	
	From the relation $J_zJ_+-J_+J_z=J_+$ we put in and identical way (we multiply both side of equality left by $|\Psi\rangle$) and rearrange a bit:
	
	Still without forgetting that:
	
	We finally have the following package of relations so far:
	
	Therefore in the corresponding order, we have that:
	\begin{enumerate}
		\item $J_+|\Psi\rangle$ and $J_-|\Psi\rangle$ are identically zero
		\item $J_+|\Psi\rangle$ and $J_-|\Psi\rangle$ are eigenstates of the operator $||\vec{J}||^2$ for the eigenvalue $K$
		
		\item $J_\pm|\Psi\rangle$ are eigenstates for the operator $J_z$ and the eigenvalue $m\pm 1$
	\end{enumerate} 
	Since the angular momentum is quantified as we know from our study of Corpuscular Quantum Physice and also from experiments, its eigenvalues must therefore have a minimum and a maximum with for each of these values and associated eigenfunction.
	
	Let us put for what will follow that $m'$ and $|\Psi'\rangle$ are the maxmimum eigenvalue and eigenstate and $m''$ and $|\Psi''\rangle$ the minimum eigenvalue and eigenstate.
	
	Given the following three relations proved earlier above:
	
	We write using also the properties of eigenvalues (\SeeChapter{see section Linear Algebra}):
	
	What intuitively is not obvious to put but which mathematically is quite justifiable.

	From the last two relations above, we can write by subtracting the first one to the second one (use of the superposition of sate principle), that gives us immediately for the eigenvalue that:
	
	thus:
	
	As we have $m'$ being the maximum, $m''$ being the minimum of the same set, we then have:
	
	What gives us after simplification of the second parenthesis that is therefore not zero (because if it could be zero we would not be able to simplify this parenthesis):
	
	Let us denote by $J$ the value $m'$ (which corresponds to the maximum eigenvalue of the quantity $J_z$) since $m''=-m'$ we have:
	
	(where often in the literature we find a lowercase $j$ to avoid confusion with the associated operator) therefore:
	
	Since the difference on the left of the equality is necessarily an integer number (based on the known results that we get in the section of Corpuscular Quantum Physics), this imposes that $2J$ is a positive integer or zero, but it also implies directly that $J$ can only be an integer (!), half-integer or zero such that:
	
	Now as for recall: $m'=J$ and $m''=-J$ it follows that if we denote by $m$ the values that can take $m''$ \underline{and} $m'$ respectively, then for each value of $J$ we have obviously:
	
	Finally, as:
	
	Then we have:
	
	And finally this gives us the eigenvalue:
	
	And since we have put that $m'$ is equal to $J$ and that we have the relation:
	
	therefore:
	
	In a more explicit and less confused form (be careful not to confuse the eigenvalues with the operators!):
	
	So we have finally:
	
	As we have have introduced earlier above the spin-orbit interaction:
	
	As we introduced earlier above:
	
	(component by component of their respective vector/matrix) and if the particle has no spin (S=0) then we have the eigenvalue of the total orbital angular momentum which reduces to the eigenvalue of the angular momentum:
	
	where we no longer indicate the indices of the components or of the matrix indices (useless!).
	
	If we have only one particle then:
	
	Therefore, the orbital angular momentum is written by remembering (\SeeChapter{see section Wave Quantum Physics}) that $l$ is quantified:
	
	If we have $l\gg 1$, then in this case:
	
	We thus fall back on the result obtained at the beginning of our study of the angular momentum.
	
	Roughly, if we now put $l\cong n$, we thus that we fall back on the angular momentum relation postulated by Bohr as seen in the section of Corpuscular Quantum Physics. This is why it is customary to take only the integer values of $l$!
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Let us recall that in reality $0\le l \le n-1$ and therefore that unlike the Bohr corpuscular model the angular momentum can be zero wit the above approach...! Another way of accepting the values taken by $n$, putting apart the fact to refer to the Bohr model seen in the section of Corpuscular Quantum Physics, is to look at the values that can be taken by $l$ in the quantum model of the hydrogen atom of the section of Quantum Chemistry and that if the latter is equal to $0$ then the associated Legendre polynomials are no longer defined!
	\end{tcolorbox}
	This finding now justifies physically the use of the quantum number $l$ in the periodic table of elements as we saw and defined it  (without any real justification) in the previous section of Corpuscular Quantum Physics.

	Finally, let us say that exactly the same reasoning and development leads to the following possible values of the pseudo angular momentum of spin:
	
	where experiments show us (to list only the most known examples) that spin $S=0$ is a characteristic of the Higgs boson or of certain atoms, spin $S=1/2$ is a characteristic of the electron / positron, spin $S=1$ is a characteristic of the photon, spin $S=2$ would be a still theoretical characteristic of the graviton. At the time of writing, no particles of spin $S=3/2$ or $S=5/2$ are known (some supposed it could be dark energy particles spin).
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.57]{img/atomistic/particles_spin_classification.jpg}	
		\caption{Some particles classification by spin (Author: James Childs)}
	\end{figure}
	We will see another diagram of the same type including the Higgs particle in the section of Particle Physics.
	
	We see therefore that the integer or semi-integral value of the spin seems to determine a crucial property of the particle: if its spin is integer, it is a "\NewTerm{boson}\index{boson}", if its spin is half-integer, it is a "\NewTerm{fermion}\index{fermion}" (this can be proven with the "spin-statistics theorem" that as already mention we will perhaps in the future give the proof in this book).
	
	The total angular momentum is thus approximately given by:
	
	By analogy (it is really a dubious analogy ...), we write for $J$ sufficiently large...:
	
	But since the spin of the electron may have only two possible orientations\footnote{Electrons spin in all directions, but in the Stern Gerlach experiment they deflect only in the $z$ direction because there was only a field in the $z$ direction (\SeeChapter{see section Relativistic Quantum Physics}). This is why we speak of two "privileged" possible orientations.}, the values of $j$ will be in the case of a spin particle $1/2$:
	
	Hence a possible classification of the atomic electrons taking into account their spin:
	
	We see above that for a multiplicity of $2l+1$ states. Indeed, for $l=0$ we have $1$ states, for $l=1$ we have a total of $1+2=3$ states, for $l=2$ we have a total of $1+2+2=5$ states, and so on....
	
	Either in schematic form with the corresponding energy levels:
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/energy_levels_spin.jpg}	
		\caption{Schematic form of associated orbitals and spin energy levels}
	\end{figure}
	This table leads us to conclude that we finally:
	
	Also we therefore introduce a common nomenclature for naming energy terms when we deal with the details of spin-orbit coupling is\footnote{the spin term is sometimes omitted because it's the same for all the levels}:
	
	where for recall $n$ is the principal quantum number, $s$ is the spin therm, $l$ the orbital angular momentum and $j$ the total angular momentum.
	
	To return to more practical considerations ... we finally obtained for the norm of total angular momentum (in the case of a single particle and without spin):
	
	where $l$ is an integer (at least in the case where we don't do the approximation done earlier above). We also know from the section of Corpuscular Quantum Physics that the magnetic moment is given by:
	
	and that the secondary quantum number $l$ and the magnetic quantum number $m_l$ are in some way indissociable.

	In the same way we get:
	
	where can take for a particle like the electron only the values (don't forget that is doesn't mean it can take only two directions! It just means it can take two states and the famous "two directions" legend is due to the Stern-Gerlach experiment type configuration):
	
	Which simply correspond (in natural units) to the two eigenvalues of the eigenmatrices:
	
	which links the spin operator to the Pauli matrices for the electron  by the Dirac equation as we will prove it in the section of Relativistic Quantum Physics:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	It is more accurate to say that in reality that the eigenvalues $\pm 1/2\hbar$ are linked to eigenmatrices $\frac{\hbar}{2}\sigma_i$ but as the $\hbar$ simplifies...
	\end{tcolorbox}
	Now, what we know of our results obtained in the section of Corpuscular Quantum Physics is that when $l$ is equal to $1$ we have the magnetic moment which can take three different values according to whether a magnetic field is applied or not:
	
	At this time, although the norm of the total angular momentum remains constant (as it is conservative quantity), its components must necessarily change. Since we know only one of the components of the angular momentum by knowing its norm (commutating operators), we choose focus for pedagogical conventions on $J_z$.
	
	We choose a reference frame such that one of the spatial components is always equal to zero (this is always possible as we know it!). It is then sufficient, for example, in the chosen planar referential $X$, $Z$ (ie the component $Y$ will be zero) to have the norm of $J$ that is equal to $l=1$:
	
	and idem with $S$ by imposing that the norm for $s = 1/2$ is equal to:
	
	There is then three possibilities to arrive at the same result by simply applying the Euclidean norm if one of the components is always imposed as zero! It is because we have:
	
	What we write so (because in fact there is an infinity of possibility) because we want to found a way to introduce the quantum number of orbital projection (which therefore quantifies the projection of the orbital angular momentum along $Z$ and is in multiplicity $2l + 1$):
	
	What some physicists like to represent in a very simplified way by the following diagram:
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/total_angular_momentum_quantification.jpg}	
		\caption{Biased schematic representation of the quantification of total angular momentum in $2D$}
	\end{figure}
	But who in reality (by the square of the components of the norm) should draw in the following form in $2D$ (as in $3D$ this is represented by cones as we will see below):
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/total_angular_momentum_quantification_improved.jpg}	
		\caption{Improved schematic representation of the quantification of total angular momentum in $2D$}
	\end{figure}
	This allows us to observe on the way that as we have mention earlier above, we can always rotate the system of axes such that one of the components is zero and earlier we did the choice:
	
	Notice that with elementary trigonometry we get:
	
	and therefore the angles take the following values:
	
	Finally, let us indicate that we then have in the particular case of the figure above where $l=1$, $m_l=\pm 1$, that the (quite abstract) system introduced earlier above:
	
	that will be written as:
	
	Which is often written in condensed form as following:
	
	In the same way with the spin $1/2$ of electron we have by introducing the quantum number of spin (which therefore quantifies the projection of the spin moment of spin along $Z$ and can take as many values as there are between $-s$ and $+s$ but by increments of $1$ as required by the experimental results, for which reason there is no zero $Z$ component below):
	
	What some physicists like to represent in a very simplified way by the following diagram:
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/spin_momentum_quantification.jpg}	
		\caption{Biased schematic representation of the quantification of spin in $2D$}
	\end{figure}
	with for angle:
	
	As before, we have in the particular case where $s=\dfrac{1}{2}$ and $m_s=\pm\dfrac{1}{2}$:
	
	This can be written in a more condensed form as following:
	
	We therefore have the only experimentally measurable variables that are:
	
	Which are therefore discrete observables (bivalent for the spin).

	With an artist's view of the concept for the pleasure of the eyes:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.25]{img/atomistic/spin_sculpture.jpg}	
		\caption{Schematic representation of various quantifications of angular momentum by the physicist and sculptor Julian Voss-Andreae}
	\end{figure}
	Therefore, by applying a constant uniform magnetic field, the Pauli Hamiltonian (\SeeChapter{see section Relativistic Quantum Physics}) will make jumps equivalent to the relation:
	
	This result means that the energy levels for a given energy ($n$-layer) are separated into several levels distant of $\dfrac{q\hbar}{2m_0}B$ when the atom is placed in a magnetic field. This result is the Zeeman effect which we have discussed several times.
	
	All this makes it possible to better understand the mathematical origin of the $4$ quantum numbers (principal quantum number, secondary or azimuthal quantum number, magnetic quantum number, spin):
	
	also written (since in the particular case of the particles studied in this book the magnetic quantum number of spin projection has the same value as the spin since we are dealing mainly with the electron):
	
	With to sum up a bit all this ...:
	
	The reader must not think that what we have seen so far was developed in one day or even in one year by physicist. Furthermore this abstract approach was highly speculative and needed to be check with experiments. So far it works very good to explain and predict experimental observations but still in this beginning of the 21st century the spin matrices theory is confronted and tested by new experimentation!
	
	\pagebreak
	\subsubsection{Spin–orbit interaction (LS coupling)}
	In quantum physics, the "\NewTerm{spin–orbit interaction}\index{spin–orbit interaction}" (also named "\NewTerm{spin–orbit effect}\index{spin–orbit effect}"  or "\NewTerm{spin–orbit coupling}\index{spin–orbit coupling}") is an interaction of a particle's spin with its motion. The first and best known example of this is that spin–orbit interaction causes shifts in an electron's atomic energy levels due to electromagnetic interaction between the electron's spin and the magnetic field generated by the electron's orbit around the nucleus. This is detectable as a splitting of spectral lines, which can be thought of as a Zeeman Effect due to the internal field. A similar effect, due to the relationship between angular momentum and the strong nuclear force, occurs for protons and neutrons moving inside the nucleus, leading to a shift in their energy levels in the nucleus shell model. In the field of spintronics, spin–orbit effects for electrons in semiconductors and other materials are explored for technological applications. The spin–orbit interaction is one cause of magnetocrystalline anisotropy.
	
	Let us now recall that we have pointed out in the section of Corpuscular Quantum Physics that when we analyze the spectral lines of hydrogen in the absence of any external field at high resolution, we see that they are in fact made up of very close doublets, separated of $0.016$ [nm] in terms of wavelength. This phenomenon is due to a so-called spin-orbit coupling. It is now time to see where it comes from. So let us also recall that we previously obtained:
	
	Hence, the squared norm (what is measured) leads us to write:
	
	Which gives us after grouping:
	
	The term:
	
	is named "\NewTerm{spin-orbit coupling}\index{spin-orbit coupling}". It is the term that during the very precise measurements of spectral lines reveals a splitting of the lines due to the coupling between the electron spin and the orbital angular momentum (this is not an $||\vec{S}||^2$ because this term is always positive).
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Let us recall that when we have two bodies in interaction the total angular momentum is a constant of motion as we have proved it in the section of Classical Mechanics. There may thus be a transfer of angular momentum between these two bodies (this is the spin-orbit coupling!). One loses angular momentum and the other gain angular momentum.
	\end{tcolorbox}
	The measured deviation is thus attributed to the interaction of the electron spin with its orbital angular moment. The electron revolves around the nucleus, but if we place ourselves on the electron, we see the nucleus turning (on Earth the Sun revolves around the Earth!). Everything happens as if the nucleus created a magnetic field at the level of the electron, and this field interacts with the magnetic moment of the electron, the spin, and this differs according to whether the spin is in the direction of the field or opposite, It is this difference that adds or subtracts some energy from the level.
	
	Here is a first simplified diagram that summarizes the whole:
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/spin_orbit_coupling_simplified.jpg}	
		\caption{Pictorial representation of the spin-orbit interaction}
	\end{figure}
	or in $3$D perspective and that is a bit more accurate:
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/spin_orbit_coupling_3d_simplified.jpg}
	\end{figure}
	Let us prove that $\vec{J}$ as defined is a constant of motion. We have (needless to say that by squaring, these are the components of the vector that we put square and not the vector itself!):
	
	Hence:
	
	Let's make the development for one  component:
	
	But, by definition (of notation) $J_x=L_x+S_x$ therefore:
	
	Now we know that $[L_x,L_x^2]=0$ (because an operator always commutes with itself by construction) and concerning $[S_x,S_x^2$, we have mentioned it in the section of Spinor Calculus and we will prove it in the framework of the study the free classical Dirac equation (\SeeChapter{see section Relativistic Quantum Physics}), that the electron spin is fully described by the Pauli matrices which are linear operators. Let us then write to a constant factor:
	
	and we will see that this is in conformity with the Pauli equation that we introduce and study in the section of Relativistic Quantum Physics (and vice versa) !!!

	Therefore, disregarding the multiplicative constant:
	
	Which was in any case $100\%$ predictable since in any case, once again, a same operator always commutes with himself.

	So finally:	
	
	Therefore:
	
	Hence finally:
	
	$\vec{J}$ is the indeed total angular momentum which, even in the presence of spin-orbit interaction, is a constant of motion (an obligation for an isolated system!).
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Another way of reading that thing is to say that the measurement on one of the elements of the preceding commutator adapts the other one immediately so that their commutation is zero therefore by extension the total angular momentum is a constant of the motion.
	\end{tcolorbox}
	One notable atomic spectral line of sodium vapor is the so-called D-line, which may be observed directly as the sodium flame-test line and also the major light output of low-pressure sodium lamps (these produce pressure sodium lamps (these produce an unnatural yellow. The D-line is one of the classified Fraunhofer lines Sodium vapor in the upper layers of lines. Sodium vapor in the upper layers of the sun creates a dark line in the emitted spectrum of electromagnetic radiation by absorbing visible light in a band of wavelengths around $589.5$ [nm]. This wavelength corresponds to transitions in atomic sodium in which valence-electron transitions from $3s$ to $3p$ electronic state.
	
	The splitting of the sodium doublet in the presence of an external magnetic field was observed by Pieter Zeeman in 1896, and the effect was subsequently named the "Zeeman effect". It is remarkable that so much detailed spectroscopy was done long before the Bohr theory, and perhaps even more remarkable that Zeeman's first study of the sodium Zeeman splitting was done the year before J. J.Thomson's discovery of the electron in 1897.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/atomistic/sodium_sl_coupling.jpg}
		\caption[]{Focus on $3P_{3/2}$ Sodium splitting (source: Hyperphysics)}
	\end{figure}
	Or more detailed:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/sodium_splitting_general.jpg}
		\caption[]{General overview of Sodium splitting (source: Hyperphysics)}
	\end{figure}
	After Thomson's work, Zeeman and Lorentz did further study of the influence of magnetic fields on the spectral emissions from atoms. By analysis of the splitting of the sodium doublet, they were able to demonstrate that the charge to mass ratio of the charge responsible for the splitting was the same as Thomson's electron. This was the first direct demonstration that electrons were involved in the production of the spectral line emissions.
	
	\subsubsection{Spin operator for spin $1/2$ particles}
	Let us now return to the relation demonstrated earlier above for $1/2$ spin particles:
	
	We have for sure obtained the eigenvalues, but it would be wise to determine the expression of the corresponding Spin operators $\hat{S}^2$ and $\hat{S}_z$. We have seen in the section of Spinor Calculus that:
	
	and we proved that for the eigenvalue $+1$ the associated eigenvectors were:
	
	We then have:
	
	By multiplying to the left and right by a familiar term:
	
	And by analogy of the fact that the matrix of Pauli are special case of rotations matrices $\text{SU}(2)$ (\SeeChapter{see section Set Algebra}), we put:
	
	And of course we deduce:
	
	Therefore for spin-$1/2$ particles we have:
	
	and we know that it is not the most explicit form as we should write the one we have determine in the section of Spinor Calculus for the Pauli matrices.
	
	\pagebreak
	\subsection{Planck dimensions}
	It is time now to open a small parenthesis to finish about the Planck's constant that we promised in the section Principles of the chapter Mechanics to indeep during our study of Wave Quantum Physics (because many books mention what we will see without the precautions of rigor in our point of view). We have just seen that the measurement of objects depends on the Heisenberg incertitude principles. This precision affects both the time measurements and the particle trajectory or the energy density of the Universe. Let's see that by extension ... there are other possible implications.

	We have proved previously at the beginning of this section that one of the uncertainties relations is given, by taking the module, by (from the order of Planck's constant to one given factor):
	
	So, we can say roughly that at a fluctuation $\Delta x$ of space, we can associate the linear momentum:
	
	To that latter, according to our results of the section of Special Relativity, corresponds the relation of the energy $E=Mc^2=pc$, or the equivalent mass (by dividing by $c^2$) $p/c$. By denoting by $M$ this mass associated with the perturbation $\Delta x$, we thus have:
	
	The gravitation due to this mass is characterized by a length $R$ which we will determine in order of magnitude by writing that the potential energy associated with it (this assumes that Classical and Quantum gravitation are governed by the same laws...) $GM^2/R$ (\SeeChapter{see section Classical Mechanics}), is equal to the mass-energy $Mc^2$. This gives:
	
	or, by replacing $M$ by its previous expression:
	
	So that there is no self-amplification (and thus divergence) of the phenomenon of vacuum quantum fluctuation, we must preferably have $R=\Delta x$. By writing the equality between these two quantities, we arrive at a quantity which represents the minimum dimension (in order of magnitude) that physics can conceive according to the interpretation of some physicists. This is the famous "\NewTerm{Planck's length}\index{Planck's length}" where we replace $R$ by $l_p$:
	
	For which it corresponds the period or "\NewTerm{Planck time}\index{Planck time}" $t_P=l_p/c$ hence:
	
	We can now return to another more interesting expression of the fluctuating mass. Since:
	
	We have therefore after rearranging the "\NewTerm{Planck mass}\index{Planck's mass}":
	
	Another way to get the Planck's mass but that gives a slightly different result is first to suppose that the main three universal constant are:
	
	and that wee seek to write a mass by combining $h$, $c$ and $G$. So this is a dimensional analysis as:
	
	where we need to found the values of $\alpha$, $\beta$ and $\gamma$ to get a mass...
	
	We see quickly that to for the mass (kg) we must have:
	
	for the distance (m), we must have:
	
	and for the time (s):
	
	So we just need to solve this linear system:
	
	So first iteration:
	
	Second iteration:
	
	Third iteration:
	
	so finally:
	
	Therefore:
	
	to a given multiplicative dimensionless constant... This missing multiplicative constant is known to us as we have determined the Planck's mass earlier before. So with dimensional analysis we should consider $\hbar$ as being more fundamental than $h$.
	
	Dimensional analysis gives to a given constant factor and according to the Viriel theorem (\SeeChapter{see section Continuum Mechanics}):
	
	and therefore:
	
	Hence the "\NewTerm{Planck's temperature}\index{Planck's temperature}":
	
	and "\NewTerm{Planck's energy}\index{Planck's energy}":
	
	After all this, we easily get the "\NewTerm{Planck's density}\index{Planck's density}":
	
	That is:
	
	We can have fun getting other Planck values, but that have no physical experimental justification excepted for most of them invalid empirical and subjective interpretations (and we could go on for a long time with so many other magnitudes as we have give a quite exhaustive list in the section Principles of the Mechanics chapter). For example:
	\begin{itemize}
		\item The "\NewTerm{Planck's force}\index{Planck's force}":
		
		Therefore:
		
	
		\item The "\NewTerm{Planck's power}\index{Planck's power}":
		
	
		\item The "\NewTerm{Planck's pulsation}\index{Planck's pulsation}":
		
		
		\item By proceeding with the same initial reasoning that the one done with mass, but using the potential electrostatic energy instead of gravitational potential energy we can get the "\NewTerm{Planck's electric charge}\index{Planck's electric charge}":
		

		\item Then we can calculate a "\NewTerm{Planck's current}\index{Planck's current}":
		
		
		\item And also the "\NewTerm{Planck's voltage}\index{Planck's voltage}":
		
		
		\item ...and the "\NewTerm{Planck's impedance}\index{Planck's impedance}" (...):
		
	
		\item and so on\footnote{see section Principia of the chapter Mechanics to see a most exhaustive and structured list}...
	\end{itemize}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Some physicists have used (and still use) the above results for wacky and dangerous reasonings that are only hazardous interpretions! It is therefore advisable to take with care all the information concerning the Planck dimensions  that you could find in textbooks and Interent (even if these seem very sympathetic ). The best known example is given by the Compton wavelength $\lambda_C$ (\SeeChapter{see section Nuclear Physics}) which depends on the mass-energy of the photon. If this wavelength is equal to the classical Schwarzschild radius for the same mass-energy (\SeeChapter{see section General Relativity}), then in this case its value is that of the Planck's length and its mass is equal to the Planck's mass. It is then tempting to say that the particle then forms a Black Hole. But this is an analogy because in this case, nothing tells us that the expression of the Schwarzschild radius applies to quantum physics... !
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Wave Quantum Physics Interpretation}
	Ok now that we have some mathematical basics on Quantum Physics, before we continue with Relativistic Quantum Physics and Quantum Field theory we think that it is the good time to speak about the main existing interpretations of the results and mechanisms of the different models and results we get so far\footnote{For some physicists, interpret mathematics is a non-sense ans anyway human brain is limited. But as many people like to interpret stuff we still dedicate a subsection about this}. Indeed, we have seen that Quantum mechanics cannot, for most people, easily be reconciled with everyday language and observation. Its interpretation has often seemed counter-intuitive to physicists, including some of its inventors.
	
	\subsubsection{Copenhagen interpretation}
	In 1930, the probabilistic interpretation of the wave amplitude of a particle and the Heisenberg uncertainty principle constitute the elements of the "non-deterministic" standard interpretation of quantum physics as we have already mention it at the beginning of this section. This interpretation is often referred to as the "\NewTerm{Copenhagen interpretation}\index{Copenhagen interpretation}", as Niels Bohr who contributed largely to it, headed a renowned institute of physics at that time in this city. However many physicists, such as Albert Einstein and Erwin Schrödinger, who accepted the mathematical formulation of quantum physics, were not at ease with the Copenhagen interpretation of and criticized it. And up to the present day (still in year 2017), the question of the correct interpretation of mathematical formulation remains a problem.
	
	Indeed, we can ask ourselves the following question: Where is the reality? Is there a reality? Niels Bohr answers "\textit{No}": there is nothing at the quantum level, reality exists or appears only during a measurement. This view shared by most physicists (Copenhagen interpretation) implies that the measure "creates" the position of the electron. In other words: No elementary phenomenon is a real phenomenon before being an observed phenomenon!
	
	Albert Einstein thought that quantum physics, although very effective and very impressive, is not complete and gives only an imperfect picture of the quantum world. For him, there would be something else beyond that which would clarify and refine our present vision (in the same way as the theory of gases for which we had to wait for statistical models, Albert Einstein thought that hidden variables had yet to be discovered).
	
	Thus, in the Copenhagen interpretation of quantum mechanics, the principle of uncertainty means that, at an elementary level, the physical universe no longer exists in a deterministic way, but rather as a series of probabilities or potentials. For example, the pattern produced by millions of photons passing through a diffraction slot can be calculated using quantum mechanics, but the path of each photon can not be predicted by any known method following the Copenhagen interpretation. It is this interpretation that Albert Einstein doubted when he said: "\textit{I can not believe that God is playing dice with the Universe}". From a physical as well as a philosophical point of view, the principle of uncertainty implies the refutation of the universal determinism defended by Laplace at the beginning of the 19th century.
	
	An instantaneous reduction of all the possible states occurs from the observation of the system according to the Copenhagen  interpretation (wave function collapse). This random decision of the observed state respects the probabilities, corresponding to the square of the amplitudes of the states as we have seen it. In addition, the Copenhagen interpretation states that during a measurement, a reduction process, originating from the macroscopic object, eliminates superposition of quantum states.
	
	The Copenhagen interpretation therefore leads to the problem of measurement, since the Schrödinger cat's thought experiment states that when we measure a quantity, such as position or momentum, we intervene in the process of measurement by causing a radical change of the quantum state of the wave function. We change the quantities measured in an unpredictable manner and this (transition) state can not be described by the Schrödinger equation. Physicists and philosophers have reacted in several ways to this interpretation:
	\begin{itemize}
		\item Either we consider as Niels Bohr and Werner Heisenberg that this principle is law and that it is better not to seek the ultimate interpretation as our brain capacity is anyway limited in comparison to the powerful investigation tool that are mathematics. This attitude is accepted by most physicists.
		
		\item Either we consider quantum physics to be an incomplete theory and some, such as Albert Einstein, Eugene Wigner or David Bohm (De Broglie-Bohm pilot-wave interpretation that we will introduce very briefly further below), have not hesitated to seek other solutions, that seems sterile until now.
		
		\item Finally, Hugh Everett and many others take Schrödinger's equation very seriously, considering it a representation of reality. They consider that the  Copenhagen interpretation really represents the evolution of the wave function. The different terms of the equation would correspond to the different levels of energy in which the system is located. The reduction of the wave packet would be interpreted as a total division of the object and the measuring instrument into parallel universes (many-worlds interpretation).
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.75]{img/atomistic/many_world_interpretation.jpg}
			\caption[]{Schrödinger's cat paradox is resolved by assuming thatdifferent branches of the Universe\\ are created with each possible outcome (source: Wikipedia, author: Christian Schirm}
		\end{figure}
	\end{itemize}
	
	Today the debate remains open, but several experiments carried out since the 1930s allow us, step by step, to dispel the thick fog that covers the bottom of reality and answer a few questions. However, all these experiments confirm that the period of certainties is long gone. The most famous experiment remaining the "\NewTerm{EPR paradox}\index{v}" following the publication of an article by Einstein, Podolsky and Rosen whose sole purpose is to undermine the Copenhagen interpretation.
	
	\paragraph{EPR paradox}\mbox{}\\\\\
	The original article being a bit difficult (in a future version of that book we will perhaps write the underlying mathemtics in details), we will take the simplified version of use in small schools but that is used in laboratories, originally proposed by David Bohm. Whereas originally the paradox was presented with the pair (position, linear momentum), Bohm proposed to use the fermio spin which is a purely binary quantum property much more easy to deal with.
	
	As an electron can have only two states of spin "top" $|+\rangle$ or "bottom" $|-\rangle$ (in the famous $z$ orient magnetic field experiment), so the EPR experiment proposed by David Bohm consists in taking a null spin particle which disintegrates, thus producing two electrons $A$ and $B$ each being in the superposition of states $|-+\rangle$ or $|+-\rangle$. Since their combined spin must remain zero, one of the electrons must have its spin at the top and the other at the bottom when their are measured. The electrons are the driven in opposite directions until the distance separating them is large enough to eliminate any physical interaction between them, and the spin of each electron is measured exactly at the same instant using a spin detector.
	
	According to Niels Bohr, as long as no measurements have been made, neither electron $A$ nor electron $B$ possess a spin pre-existing in any direction. Instead, before being observed, electrons exist in a superposition of states, so that they are up and down at the same time. Since the two electrons are entangled, the information concerning the state of their spin is given by a wave function of the type:
	
	The electron $A$ has no determined spin component before a measurement is performed to determine it collapses the wave function of the system $A$ and $B$, after which it is either "top" or "bottom". At that moment, its partner $B$ acquires the opposite spin in the same direction, even though it is at the other side of the Universe violating therefore Special Relativity postulates (no information can go faster than the speed of light $c$). The Copenhagen interpretation is then named "\NewTerm{non-local interpretation}\index{non-local interpretation}" whereas Albert Einstein believed in local realism: that is, a particle can not be \underline{instantly} influenced by a distant event, and its properties exist independently of any measure.
	
	However, Bohm's approach has a flaw that Albert Einstein would probably have used as an argument: correlations could be explained by arguing that the two electrons each have spin values defined on each of the three axes $x$, $y$, $z$. Whether measured or not. So again, according to Albert Einstein, the fact that the pre-existing spin states of the electron couple can not be taken into account by wave quantum physics would still have been a proof of its incompleteness (hidden variables).
	
	A physicist, John Bell, however, had the idea of an experimental and theoretical means to break the impasse of the EPR paradox by changing the relative orientation of the two spin detectors.

	Thus, if the detectors measuring the electron spin $A$ and $B$ are aligned so as to be parallel, then there is a $100\%$ correlation between the two sets of measurements each time the spin is measured at "top" by the first detector, the "bottom" spin is measured by the other detector, and vice versa. If one of the detectors is turned slightly, they are no longer aligned. Now, if we measure the spin state of many pairs of entangled electrons, when we find "top" for electron $A$, the corresponding measure for $B$ will sometimes give "top" too. Increasing the angle between the axes of the two detectors therefore leads to a reduction in the degree of correlation. If the detectors are at right angles to one another and the experiment is repeated a large number of times, it is only in half of the cases that a spin is detected at the "bottom" for $B$ when we detect a "top" spin for $A$ following the $z$-axis. If the detectors are oriented $180$ degrees apart, the electron couple will be completely anticorrelated. If the measurement gives "top" for the spin state of $A$, then the spin state of $B$ will be "bottom".
	
	Although it is an imaginary experiment, it is possible to calculate the exact degree of spin correlation for a given orientation of the detectors, as predicted by quantum theory. However, it is not possible to perform a similar calculation by using an archetypal hidden variable theory and conserving the locality. The only thing that such a theory could predict would be an imperfect coupling between the spin states of $A$ and $B$. However, rigorously, it is insufficient to choose between quantum theory and a local theory with hidden variables. John Bell made an astonishing discovery. It was possible to decide between the predictions of quantum mechanics and those of any hidden-variable theory by measuring the correlations of electron pairs for a given configuration of the detectors and then repeating the experiment with a different orientation. This allowed John Bell to calculate the total correlation for the two orientation configurations in terms of individual results predicted by any local theory with hidden variables. Since, in any such theory, the result of a measurement performed by a detector can not be affected by what is measured with the other, it is possible to distinguish between hidden variables and quantum mechanics.
	
	John Bell succeeded in calculating the limits of the degree of spin correlation between pairs of entangled electrons in a Bohm-modified EPR experiment (if we have the time we will provide the mathematical details in the future). He found that in the ethereal kingdom of quanta there is a greater degree of correlation if quantum mechanics reigns as an absolute rule than in any universe that depends on hidden variables and locality. Bell's theorem stated that no local theory with hidden variables could reproduce the same set of correlations as quantum mechanics. Any local theory with hidden variables would lead to spin correlations generating numbers, name obviously "correlation coefficients", between $-2$ and $+2$. However, for some orientations of spin detectors, quantum mechanics predicts correlation coefficients that lie outside the range, named "\NewTerm{Bell inequalities}\index{Bell inequalities}", ranging from $-2$ to $+2$.

	The Bell theorem thus makes it possible to test face to face the local reality advocated by Albert Einstein with Niel Bohr Copenhagen interpretation, namely that the quantum universe exists independently of observation and that physical effects can not be transmitted to a speed higher than that of light. Bell had carried the Einstein-Bohr debate into a new arena: experimental philosophy. If Bell's inequality resisted, then Einstein's assertion that quantum mechanics was incomplete would be accurate. If this inequality were to be violated, Bohr would triumph. No more thought experiences! It would now be Einstein vs. Bohr in the lab.
	
	The first experiment that tested the Bell's inequalities used pairs of photons instead of pairs of electrons. This change was possible because the photons possess the polarization property, which for the needs of the test played the role of the quantum spin (moreover, the photons are easier to manipulate). This is certainly a simplification, but a photon can be considered to be polarized as "top" or "bottom". Like the spin of the electron, if the polarization of one of the photons on the $z$-axis is measured as "top", then the measurement of the other will give "bottom", since the combined polarizations Of the two photons must result in a zero spin (polarization state).
	
	The results violated Bell's inequalities, which was in favor of Bohr's non-local Copenhagen  interpretation and against the local reality supported by Albert Einstein.
	
	Bell derived this inequality from two assumptions (hypothesis):
	\begin{enumerate}
		\item[H1.] First, there is a reality independent of the observer. This is reflected in the fact that a particle has a well-defined property such as spin before it is measured. 

		\item[H2.] Second, the locality is conserved. There is no supra-luminous influence, so that what happens here can not instantly affect what happens elsewhere. 
	\end{enumerate}
	The experimental results mean that one of these two assumptions must be abandoned, but which one? Bell was willing to abandon the locality.
	
	\subsubsection{De Broglie-Bohm (pilot-wave) interpretation}
	The "\NewTerm{de Broglie–Bohm theory}\index{de Broglie–Bohm theory}", also known as the "\NewTerm{pilot-wave theory}\index{pilot-wave theory}" or "\NewTerm{Bohm's interpretation}\index{Bohm's interpretation}" is another interpretation of quantum theory. The theory is deterministic[1] and explicitly nonlocal: the velocity of any one particle depends on the value of the guiding equation, which depends on the configuration of the system given by its wavefunction; the latter depends on the boundary conditions of the system, which in principle may be the entire universe...

	The theory was historically developed in the 1920s by de Broglie, who in 1927 was persuaded to abandon it in favour of the then-mainstream Copenhagen interpretation. David Bohm, dissatisfied with the prevailing orthodoxy, rediscovered de Broglie's pilot-wave theory in 1952. Bohm's suggestions were not widely received then, partly due to reasons unrelated to their content, connected to Bohm's youthful communist affiliations. De Broglie–Bohm theory was widely deemed unacceptable by mainstream theorists, mostly because of its explicit non-locality. Bell's theorem (1964) was inspired by Bell's discovery of the work of David Bohm and his subsequent wondering whether the obvious nonlocality of the theory could be eliminated. Since the 1990s, there has been renewed interest in formulating extensions to de Broglie–Bohm theory, attempting to reconcile it with special relativity and quantum field theory, besides other features such as spin or curved spatial geometries.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.46]{img/atomistic/pilot_wave.jpg}
	\end{figure}
	We strongly recommend the reader to go see on Internet videos about the "walking droplets\footnote{hydrodynamic quantum analog involving bouncing fluid droplets over a vibrating fluid bath}" also named "walker" to understand better the idea behind the concept of pilot wave. 
	
	 In the below gallery (read from left to right and up to bottom) thanks to a little layer of air, droplets of silicone oil can continuously bounce on the surface of the pool beneath them without being absorbed. But even though the droplets don't directly connect with the pool beneath them, they still make waves, and then interact with those waves, resulting in a forward momentum.
	 \begin{figure}[H]
		\centering
		\includegraphics[scale=0.46]{img/atomistic/bouncing_droplet_forward.jpg}
		\caption[]{Thirty successive pictures of a walker}
	\end{figure}
	All observed phenomena are related to the particle-wave interaction. The droplet moves in a medium modified by previously generated waves.As the droplet moves, the points of the interface it visits keep emitting waves.The wavefield has thus a complex structure that contains a "memory"of the path.
	
	While this experiment isn't on the quantum scale, it does help to demonstrate the way quantum-scale particles may operate according to the pilot wave theory. And for any lay people who have struggled with grasping why things are so strange on the quantum scale according to the standard interpretation, this pilot wave theory provides a far more palatable framework for understanding quantum mechanics.
	
	And what about walkers facing a double split? Here you are (still read from left to right and up to bottom):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.46]{img/atomistic/bouncing_droplet_forward_double_split.jpg}
		\caption[]{Double split experiment with two walkers}
	\end{figure}
	The diffraction pattern obtained with the walker experiment is highly similar tot that of the diffraction pattern of photon or electron with the same experimental configuration.
	
	If we have the time in the future we will perhaps write a formal introduction (ie mathematical introduction) to this theory.
	
	
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{90} & \pbox{20cm}{\score{4}{5} \\ {\tiny 61 votes,  74.43\%}} 
	\end{tabular} 
	\end{flushright}

	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
   	\section{Relativistic Quantum Physics}
   	\lettrine[lines=4]{\color{BrickRed}T}he attentive reader will have noticed that quantum mechanics (wave quantum physics) is a non-relativistic theory: it does not incorporate the principles of Einstein's Special Relativity (\SeeChapter{see section Relativity}). We will strive to fill this gap by studying now the relativistic version of the wave quantum physics.
   	
   	 Relativistic quantum mechanics (RQM) is any Poincaré covariant formulation of quantum mechanics. This theory is applicable to massive particles propagating at all velocities up to those comparable to the speed of light $c$, and can accommodate massless particles. The theory has application in high energy physics,particle physics and accelerator physics, as well as atomic physics, chemistry and condensed matter physics. Non-relativistic quantum mechanics refers to the mathematical formulation of quantum mechanics applied in the context of Galilean relativity, more specifically quantizing the equations of classical mechanics by replacing dynamical variables by operators. Relativistic quantum mechanics is quantum mechanics applied with Special Relativity, but not General Relativity. An attempt to incorporate General Relativity into quantum theory is the subject of quantum gravity, yet an unsolved problem in physics, although some theories, such as the Kaluza-Klein, have been proposed but are unfounded and without proof. Although the earlier formulations, like the Schrödinger picture and Heisenberg picture were originally formulated in a non-relativistic background, these pictures of quantum mechanics also apply with Special Relativity.

	The relativistic formulation is more successful than the original quantum mechanics in some contexts, in particular: the prediction of antimatter, electron spin, spin magnetic moments of elementary spin-$1/2$ particles, fine structure, and quantum dynamics of charged particles in electromagnetic fields. The key result is the Dirac equation, from which these predictions emerge automatically. By contrast, in quantum mechanics, terms have to be introduced artificially into the Hamiltonian operator to achieve agreement with experimental observations.

	Nevertheless, RQM is only an approximation to a fully self-consistent relativistic theory of known particle interactions because it does not describe cases where the number of particles changes; for example in matter creation and annihilation. By yet another theoretical advance, a more accurate theory that allows for these occurrences and other predictions is relativistic quantum field theory in which particles are interpreted as field quanta (see article for details).
   	
   	Before we tackle the mathematical part, we emphasize and reiterate that we will limit ourselves only to theoretical developments made between 1910 and about 1935 (beyond the complexity of theories requires too many pages to a general book like this one).
   
   	
   	\pagebreak
	\subsection{Relativistic Schrödinger evolution equation}
	Particle physics can not be properly and completely described in the context of quantum mechanics. As the energies are generally superior to the masses of the particles, it is necessary, in addition, to work in the context of the theory of relativity. Let's see how to include it in a first basic approach.
	
	The energy-momentum of a free particle of mass $m$, satisfied as we have proved in the section of Special Relativity the relation:
	
	We seek to quantify this equation. For this, we will return to the relations we have proved in the study of functional linear operators and of the evolutionary Schrödinger equation.
	
	Recall that the momentum is described by the relation (using the divergence operator):
	
	and the total energy by:
	
	Both relations have been proved in details in the section of Wave Quantum Physics!
	
	The substitutions of the two previous relations applied to the equality:
	
	and multiplied by the wave function (\SeeChapter{see section Wave Quantum Physics}) on both sides of equality lead to development:
	
	Using the d'Alembertian (\SeeChapter{see section Electrodynamics}), we can write this latter relation in a more condensed form named "\NewTerm{relativistic Schrödinger evolution equation}\index{relativistic Schrödinger evolution equation}" or more frequently "\NewTerm{free Klein-Gordon equation}\index{free Klein-Gordon equation}" (in the absence of a magnetic field!):
	
	with the signature $+, -, -, -$ for the partial derivative:
	
	Some authors prefer the signature  $-, +, +, +$ therefore we have (this is also the choices we will make in the section of Quantum Field Theory as it is the tradition...):
	
	with:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	In particle physics, this equation is named "\NewTerm{bosons relativistic covariant equation}\index{bosons relativistic covariant equation}" whatever the chosen signature.
	\end{tcolorbox}
	The free Klein-Gordon equation is often given as follows (more aesthetic):
	
	It is important to note that the Klein-Gordon equation involves scalar and therefore characterizes zero spin particles.
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} We can verify that the plane waves of the form:
	
	are solutions of the free Klein-Gordon equation (we'll go more into details in the section Elementary Particles Physics).\\
	
	\textbf{R2.} We will come back during our study of the Dirac equation and fermions spins on the free Klein-Gordon equation (to generalize it).
	\end{tcolorbox}
	
	\subsubsection{Antimatter}
	When we have proved the free Klein-Gordon equation, we purposely left aside a very interesting case of development that we have made.
	
	Maybe you did not notice it, but the equation:
	
	can take two values for a given pulsation:
	
	one positive and the other negative. The value of energy could therefore take all values in $[-\infty,+\infty]$...!!!
	
	So far we have implicitly assumed in Classical Mechanics that negative solutions were not natural and should therefore simply be discarded. This cannot be done in quantified theory without lead to serious inconsistencies. Rather than ignore these negative energy solutions, we should find them a physical interpretation.
	
	We notice first that all the negative energies are authorized by the above equation (as well as for positive energy). We say then that negative energy states are occupied but not observable; the electrons are named "\NewTerm{virtual electrons}\index{virtual electrons}".
	   	\begin{theorem}
	Let us imagine a wave packet formed by a superposition of plane waves at a narrow interval in pulse. This packet moves in space. In the one-dimensional case, it propagates at the speed:
	
	\end{theorem}
	\begin{dem}
	Always based on the assumption that the potential field is zero, we have:
	
	and:
	
	So to resume:
	
	\begin{flushright}
		$\square$  Q.E.D.
	\end{flushright}
	\end{dem}
	Let us consider first a particle of positive energy $E^+$. Its position in function of time is given by:
	
	A particle of negative energy $E^-$ moves following:
	
	In other words, and this will be our first conclusion, we can say that a particle of negative energy $E^-<0$ is equivalent to a particle of positive energy $|E^-|>0$ moving backwards in time and this is what we name an "\NewTerm{antiparticle}\index{antiparticle}".
	
	Now we have to see what is the interpretation of a particle moving backwards in time:
	
	To simplify, we consider a non-relativistic particle of electric charge ($-q$) immersed in a static electric field $\vec{E}$ and a static magnetic field $\vec{B}$. It satisfies to the equation of motion:
	
	We have studied in the section of Electrodynamics that the fields $\vec{E}$ and $\vec{B}$ could be constructed from a quadripotential $\vec{A}$. So we can rewrite the above equation from the two relation proven in the section of Electrodynamics:
	
	However, it is always possible to impose the following gauge (we leave it to do reader to do the check by usingexactly the same methodology as that used in the section of Electrodynamics):
	
	The equation of motion becomes:
	
	or also:
	
	Comparing the last two equations, we come to our second conclusion: a particle of charge $q$ moving backwards in time obeys to the same equations of  motion that an oppositely charged particle $-q$ moving forward in the time. The physical interpretation of the second particle is obvious ("\NewTerm{Feynman-Stückelberg interpreation}\index{Feynman-Stückelberg interpreation}").

	Relativistic quantum physics implies the existence of anti particles, which are actually observed in laboratories (accleratoris and not only...!).

	All this to get where exactly?
	\begin{enumerate}
		\item First, the theoretical discovery of antimatter provides a possible explanation for the existence of the Universe which previously violated the principle of conservation of energy. The theory we have just seen, therefore predicted that our Universe should contain as much matter as antimatter. Scientists are still looking for the presence of this antimatter.

		\item Secondly, if we consider in vacuum a photon of energy $hv>2m_ec^2$, it is capable of carrying a virtual electron to a state of positive energy, where it becomes real. It appears then locally a gap, or "hole" in the region of negative energies. According to the principle of charge conservation, one sees a positive electron, or positron, symmetrical and antimatter equivalent particle of the electron.
	\end{enumerate}
	Thus, the photon materializes in the form of a pair $e^+,e^{-}$, with:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Some experimental results suggest that antiparticles are not the perfect mirrors of the particles we know. Indeed, the right/left and time symmetries does not appear to be respected (there is symmetry breaking). We do not have anything written about in this book at this date but we will do so as soon as we can.
	\end{tcolorbox}
	
	\subsection{Generalized Klein-Gordon Equation}
	The free Klein-Gordon equation that we initially presented and proved earlier does not take into account the influence of magnetic field on the observation of the splitting of spectral lines of atoms (experimental observation!). That's why Klein and Gordon have integrated it into their equation the magnetic field. However, they did not take into account the spin of the electron. Only after their work Pauli developed his equation (named "Pauli equation") which then led to the Dirac equation (see further below).

	To determine the expression of the Klein-Gordon generalized equation of a charged particle in a magnetic field and an electrostatic potential, we will use the power of the Lagrangian formalism!

	The classical equation of motion accepted is (\SeeChapter{see section Analytical Mechanics}), as valid also in relativity, is given we proved it by (Euler-Lagrange equation):
	
	In the section of Special Relativity, we have proved that the Lagrangian of a free particle is expressed as:
	
	with for recall:
	
	and in the section of Electrodynamics that the total Lagrangian was given by:
	
	For future needs, start by calculating:
	Let us calcualte the first term:
	
	As the potential does not dependent on the speed, the term:
	
	is equal zero.

	The vector potential does not depend on the velocity of the particle also, then:
	
	Therefore it comes:
	
	The classical Hamiltonian is written (\SeeChapter{see sectioin Analytical Mechanics}):
	
	So we have previously proved that:
	
	We can write the Hamiltonian with this relation in the form:
	
	The dot product $\vec{v}\circ\vec{p}$ has for expressed (since $\vec{p},\vec{v}$ are collinear)
	
	The Hamiltonian is therefore written:
	
	By working on the two first terms:
	
	But:
	
	Therefore:
	
	Finally, we get (for a conservative system):
	
	Always in the case of a particle moving in an electromagnetic field, the relation between the energy and impulsion (which is different from that of momentum by the presence of a term containing the vector potential) is calculated as follows:

	As (we have just prove it):
	
	and as we have prove it in the section of Wave Quantum Physics:
	
	where it is necessary to see the last term as an abusive notation for the square of the norm.
	
	Then by substituting into:
	
	we get (we change of notation for the Hamiltonian):
	
	If we rewrite this relation by making use of the corresponding operators (\SeeChapter{see section of Wave Quantum Physics}) of energy and momentum (canonical quantization):
	
	then finally we can write in analogy with the free Klein-Gordon equation (in the absence of field) the "\NewTerm{generalized Klein-Gordon equation}\index{generalized Klein-Gordon equation}":
	
	This equation is that of Klein-Gordon that applies to a particle of charge $q$ without  spin moving in an electromagnetic field.

	If $U=0,\vec{A}=\vec{=}$ the the previous relation simplif in multiple stages:
	
	We thus fall back on the Klein-Gordon equation for a free particle without spin!
	
	It would be interesting now to look at the expression of the continuity equation (let us recall that it expresses the the conservation of energy!!) with the inclusion of the magnetic field (because in fact the continuity equation still makes problem... and even a very big problem!!!). For this, consider the case of a free particle moving with a momentum $\vec{p}$ and having an energy $E$. We saw that we could associate to it a plane wave of the form:
	
	Given the free Klein-Gordon equation and its complex conjugate (we work with naturellesunits):
	\begin{subequations}
	\label{equations}
	\begin{align}
	  \label{eq:a}
	  &-\partial_t^2\Psi+\Delta \Psi-m_0^2\Psi=0 \\
	  \label{eq:b}
	  &-\partial_t^2\bar{\Psi}+\Delta \bar{\Psi}-m_0^2\bar{\Psi}=0
	\end{align}
	\end{subequations}
	We multiply (a) by $-\mathrm{i}\bar{\Psi}$ and (b) by $-\mathrm{i}\Psi$:
	\begin{subequations}
	\label{equations}
	\begin{align}
	  \label{eq:a}
	  &-\mathrm{i}\bar{\Psi}\left(-\partial_t^2\Psi\right)-\mathrm{i}\bar{\Psi}\Delta \Psi+\mathrm{i}\bar{\Psi}m_0^2\Psi=0 \\
	  \label{eq:b}
	  &-\mathrm{i}\Psi\left(-\partial_t^2\bar{\Psi}\right)-\mathrm{i}\Psi\Delta \bar{\Psi}+\mathrm{i}\Psi m_0^2\bar{\Psi}=0
	\end{align}
	\end{subequations}
	Therefore:
	\begin{subequations}
	\label{equations}
	\begin{align}
	  \label{eq:a}
	  &\mathrm{i}\bar{\Psi}\partial_t^2\Psi-\mathrm{i}\bar{\Psi}\Delta \Psi+\mathrm{i}\bar{\Psi}m_0^2\Psi=0 \\
	  \label{eq:b}
	  &\mathrm{i}\Psi\partial_t^2\bar{\Psi}-\mathrm{i}\Psi\Delta \bar{\Psi}+\mathrm{i}\Psi m_0^2\bar{\Psi}=0
	\end{align}
	\end{subequations}
	By difference (a)-(b):
	
	Computing the derivatives with respect to $t$ of the following functions:
	\begin{subequations}
	\label{equations}
	\begin{align}
	  \label{eq:a}
	  &\partial_t(\bar{\Psi}\partial_t\Psi)=\partial_t\bar{\Psi}\partial_t\Psi+\bar{\Psi}\partial_t^2\Psi \\
	  \label{eq:b}
	  &\partial_t(\Psi\partial_t\bar{\Psi})=\partial_t\Psi\partial_t\bar{\Psi}+\Psi\partial_t^2\bar{\Psi}
	\end{align}
	\end{subequations}
	By difference (a)-(b):
	
	Which gives us finally:
	
	Let $f$ be a scalar field and $\vec{g}$ and a vector field. The vector analysis gives:
	
	Let us put:
	
	Therefore:
	
	Let us put now:
	
	Therefore:
	
	So we have finally two relations:
	\begin{subequations}
	\label{equations}
	\begin{align}
	  \label{eq:a}
	  &\vec{\nabla}\circ(f\cdot \vec{g})=\vec{\nabla}\circ(f\cdot \vec{g})-(\vec{\nabla} f)\circ \vec{g}\Rightarrow \Psi(\vec{\nabla}\circ\vec{\nabla}\bar{\Psi})=\vec{\nabla}\circ(\Psi\vec{\nabla}\bar{\Psi})-(\vec{\nabla}\Psi)\circ\vec{\nabla}\bar{\Psi}\\
	  \label{eq:b}
	  &f(\vec{\nabla}\circ\vec{g})=\vec{\nabla}\circ(f\cdot \vec{g})-(\vec{\nabla})\circ\vec{g}\Rightarrow \bar{\Psi}(\vec{\nabla}\circ\vec{\nabla}\Psi)=\vec{\nabla}\circ(\bar{\Psi}\vec{\nabla}\Psi)-(\vec{\nabla}\bar{\Psi})\circ\vec{\nabla}\Psi
	\end{align}
	\end{subequations}
	We substract (a)-(b) to get:
	
	As $\vec{\nabla}\circ\vec{\nabla}=\vec{\nabla}^2=\Delta$:
	
	By changing the signs:
	
	This latter relation and:
	
	gives together:
	
	Again, let us compare this relation with the continuity equation:
	
	Let us recall that during our first study of the Klein-Gordon equation we have seen that in quantum physics equivalent is given by the same equation but with the following meanings: $\rho$ is the probability density, $\vec{j}$ is the flux density of particles.
	We then have:
	
	If the associated wave function $\Psi$ and its conjugate complex $\bar{\Psi}$ are given in a special cas by:
	
	The derivatives with respect to time of these functions are obviously:
	
	And their gradients are calculated as follows:
	
	By taking again the expression of the probability density and given previous differential, we get:
	
	The density of probability has therefore for expression:
	
	Taking again the expression of current density and considering the differential, we get:
	
	The current density has therefore for expression:
	
	Putting ourselves in the situation of that time of knowledge of quantum physics, the Klein-Gordon equation has several pathologies and disadvantages:
	\begin{itemize}
		\item The probability density $\rho=2EA^2$ may become negative (since as we have seen, the energy can also be), which is inexplicable. Such a situation does not exist with the Schrödinger equation.
		
		\item The Klein-Gordon equation has the disadvantage of being of the second order in $t$ (the Schrödinger equation is of the first order). The temporal evolution requires therefore the knowledge of not only of $\Psi(t_0)$ but also of its derivative $(\partial_t \Psi)_{t=t}$

		\item If we applied this equation to the hydrogen atom, we would not find the same fine structure\footnote{"Fine-structure" refers to splitting of levels with the same $l$ but different $j$} energy levels as observed.		
	\end{itemize}
	All this led at the time preceding the works of Dirac, to reject this equation that, in addition, did not include the spin.

	\pagebreak	
	\subsection{Classical free Dirac equation}
	So far, any particle was regarded as punctual and without any structure or internal degree freedom. In this context, all the information on the system state at time $t$ is then reputated as entirerly contained in the wave function $\Psi(x,y,z,t)$.

	However, such a description is not sufficient, as we shall see. This insufficience comes from the experimental evidence demonstrating that a particle such as an electron or also neutron has a magnetic intrinsec moment, independently of any rotation in space around a center. The existence of this magnetic moment in turn leads to the existence of an intrinsic angular momentum which was named "\NewTerm{spin}\index{spin}" because it was believed at first that this degree of freedom was linked to a rotation of the particle on itself. This degree of freedom is "internal" - although the electron continues to be regarded as a point particle; that is, along with the electric charge or mass, an intrinsic attribute, given once and for all. It seems so far impossible to physicist to give to the spin a classic analogy! Imagine the electron as a small non-zero radius ball that turns on itself leads to absurdities (for example we found that a point located on the surface of the electron has a much higher speed than $c$). It remains that the spin of a massic particle is its angular momentum in the repository where it is at rest. It is commonly accepted that hypothesis of the electron spin was formulated first by George Uhlenbeck and  Samuel  Goudsmit in 1925.

	The spin of a particle is always half full or unitary, it is a fact of experience until now. The half or unitary characteristic nature of the spin defines two broad families of particles:
	\begin{itemize}
		\item bosons (integer spin)
		
		\item fermions (half-integer spin)
	\end{itemize}  	
	obeying very different statistics such as those we have discussed and study in the section os Statistical Mechanics (hence the existence of a relation named "spin-statistics theorem").

	Let us return to the case of the electron. The two possible values revealed by measuring $S$ (the $\mu_l$ we had in the section of Corpuscular Quantum Physics) are the eigenvalues $\pm\hbar/2$ (\SeeChapter{see section Wave Quantum Physics}) associated with the two possible values of a quantum number $m_s=\pm 1/2$ itself associated at the free state ($\vec{L}=\vec{0}$) to the angular momentum:
	
	Therefore:
	
	A complete description of the state of the electron therefore necessarily contains a wave function as usual giving the probability density of presence, but also taking into account the degree of freedom of the spin, hence the notation $\Psi(x,y,z,m_s,t)$. If the coordinates of space are continuous real values, however the spin variable is essentially discrete!
	
	Maintaining the usual interpretation, the quantity $|\Psi(x,y,z,m_s,t)|^2\mathrm{d}r^2$ is the probability of presence around the point selected with the value $m_s\hbar$ for the spin. The condition of normalization of the probabilities introduced as always a sum, which covers not only the orbital degrees (continuous summation, that is to say: integration) but also the degrees of spin (discrete summation):
	
	expressing in particular the fact that we exhaust all the possibilities of the spin by summing on the two possible values. In any case, the electron no longer has one but two wave functions, one for each value of $m_s$.
	
	The previous notation is not necessarily the best for spin-free particles greater than $1/2$ as we have seen in our study of angular momentum. About the fact of having  a variable taking discrete values, it is just as legitimate to put $m_s$ in index of the $\Psi$ and to put: $\Phi_{m_S}(\vec{r},t)$. Finally, it is convenient to use a matrix notation, storing in columns the various functions corresponding to the possible values of the discrete variable $m_s$. Thus, for the electron, we will now assume that all information in the sense of Wave Quantum Physics is contained in a two-line column vector named "\NewTerm{spinor}\index{spinor}" (\SeeChapter{see section Spinor Calculus}) and denoted:
	
	Let us now come back to the free Klein-Gordon equation (more general than the Schrödinger equation, of course, but less than the one including the magnetic field):
	
	This equation is as we know unfortunately incomplete because it contains no information on the spin of the electron (if we focus still only on this particle).

	We can, however, in order to find a solution to this problem, make a parallel with the electromagnetic field. That latter includes also a spin, residing in the polarization of the field (\SeeChapter{see section Electrodynamics}). This polarization is closely related to the vector nature of the electromagnetic field and is reflected in the Maxwell equations, which are as we know first-order derivatives. However, by combining Maxwell's equations, we saw in the Electrodynamics section that we could obtain the wave equations:
	
	which are (very relevant coincidence!) a special case of the Klein-Gordon equation when $m_0=0$:
	
	However, the wave equations contain less information than the original Maxwell equations: they do not explicitly contain any relation between the various components of the fields $\vec{E}$ and $\vec{B}$, such as the fact that in an electromagnetic wave of wave vector $\vec{k}$ the fields $\vec{E}$ and $\vec{B}$ are mutually perpendicular and both are perpendicular to the wave vector $\vec{k}$ (ie they are bot perpendicular to the direction of propagation of the wave). To establish these constraints, we must return to Maxwell's equations and thus to equations with first-order derivatives.

	The same is true for fermions (electrons are part of it). Klein-Gordon's equation, even it is not false, is incomplete. We must try here to establish a first-order equation in derivatives which describes well the spin $1/2$ of the electrons of the fermions. This last condition means that this equation must therefore involve the two components of a spinor (in analogy with the one we have determined above):
	
	We will write this equation which we seek as:
	
	where $D$ is an $2\times 2$ matrix involving first order derivatives (a first-order differential operator) that we have to determine (and that we will!!!).

	To give an example before going further, let us look at how Klein-Gordon's equation can be expressed in such a form.

	We have first (free Klein-Gordon equation):
	
	or (generalized Klein-Gordon equation):
	
	What can also be written for the free Klein-Gordon equation:
	
	or for the generalized Klein-Gordon equation:
	
	Let us now restrict ourselves to the case of the free Klein-Gordon equation (the reasoning being similar but ... longer for the generalized version).

	The last expression of the free Klein-Gordon equation suggests introducing the two combinations found after (it seems...) numerous trial and errors by our predecessors:
	
	from which results:
	
	Therefore:
	
	Can be written in two ways after respective substitutions:
	
	Either in matrix form:
	
	or even:
	
	What we can write:
	
	Therefore, relatively to our initial idea of having a relation in the form:
	
	We can make the similarity (correspondence term by term) with the prior-previous equation:
	
	where $D$ is indeed a $2\times 2$ matrix.
	
	where $A$ is a vector (but represented by tradition as a scalar...), $\vec{B}$ is a vector (and denoted as such and has nothing to do with the notation of the magnetic field) and $\sigma$ a vector composed of $2\times 2$ symmetric matrices (by reading the rest you will see that posing this makes it possible to find what we are looking for...).

	Let us recall that the multiplication between $\vec{B}$ and $\vec{\sigma}$ constitutes a dot product such as that defined in our study of the Spinor Calculus section.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We reader must be very cautious in the developments that follow, because the traditional notations in the field make it very difficult to distinguish between products, dot products, and products of vector components forming a vector (element-wise multiplication).
	\end{tcolorbox}
	Let us put (by the way our predecessors also made numerous trials and errors before putting this ...):
	
	What we were initially seeking, that is, $\mathrm{D}\xi=0$, then becomes:
	
	Thus $A=\partial/\partial t$, $\vec{B}=\lambda\vec{\nabla}$ and for $\vec{\sigma}$ that latter remains (let us imagine...) unknown. We must also determine $\lambda$.
	
	Always by analogy with the example above, let us try to find the wave equation to determine the constant $\lambda$:
	
	So that we fall back on the wave equation, we must therefore have:
	\begin{enumerate}
		\item First that $\left(\vec{\sigma}\circ\vec{\nabla}\right)\left(\vec{\sigma}\circ\vec{\nabla}\right)=\vec{\nabla}^2$. Indeed (caution! the reader must have ideally at least studied the section of Spinor Calculus to know in detail how we get at this development!):
		
	
		\item And that $\lambda=c$:
		
	\end{enumerate}
	There are therefore, always in analogy with the previous developements, two possibilities which can be applied to different fields that we will be denoted $\chi^{(\pm)}$ and which are named "\NewTerm{Weyl spinors}\index{Weyl spinors}". We thus have a sort of double spinor (as in the parenthesis we have a two components vector formed by $2\times 2$ matrices) such that:
	
	These equations are named "\NewTerm{Weyl equations}" as the above relation represents in reality $4$ equations (don't forget that the parenthesis is a $2$ components vector made of $2\times 2$ matrices and therefore $4$ rows). Let us recall that a two dimensional spinor is a $2$ components vector that are both in $\mathbb{C}$.

	We now have to generalize the Weyl equations to the case of a half-integer spin fermion with mass. This new equation must respect the following constraints:
	\begin{enumerate}
		\item[C1.] It must be reduced to the Weyl equations when the mass tends to zero

		\item[C2.] It must lead to the free Klein-Gordon equation

		\item[C3.] It must describe particles with a spin
	\end{enumerate}
	The very astute solution (extremely difficult to guess) then consists in coupling the two Weyl equations by using different factor such as:
	
	To verify that these factors have been correctly chosen, we apply:
	
	to the first equation and substitute the second equation. We thus find:
	
	or developing the left extreme previous term and keeping the right extreme previous term and dividing by $\mathrm{i}$:
	
	to compare with:
	
	that is the free Klein-Gordon equation (we prove the same correspondence for the component $\chi^{-}$) and thus reinforces the validity of the assumptions and developments made so far.

	It is usual to combine the two spinors in a single spinor (this becomes a "\NewTerm{bispinor}\index{bispinor}") of $4$ components (a spinor with $4$ components, two of which are in fact associated with the particles and two with the antiparticles as we shall see):
	
	and to define the following two matrices (commonly named "\NewTerm{gamma matrices}\index{gamma matrices}") (in a so-named "\NewTerm{chiral form}"):
	
	where $\mathds{1}$ is the $2\times 2$ unit matrix traditionally defined by:
	
	and:
	
	where the $\sigma_i$ are the "\NewTerm{Pauli matrices}\index{Pauli matrices}" given by (\SeeChapter{see section Spinor Calculation}):
	
	which must satisfy for recall (proved earlier above):
	
	which is for recall $2\times 2$ matrix.. Pauli's matrices are therefore good candidates to solve our problem!
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} As we saw in the section of Spinor Calculatus (Algebra Chapter), $\sigma_0$ is not really a Pauli matrix in itself. However, in some books, it is indicated as being one (this is also our choice here).\\
	
	\textbf{R2.} As we have also seen in it in the section of Spinor Calculus, let us recall that the Pauli matrices implicitly represent infinitesimal spatial rotations of a spinor.\\
	
	\textbf{R3.} Caution! In the gamma matrices, the writing of the $0$ means in fact that it is $2\times 2$ matrices  which all the components are zero.
	\end{tcolorbox}	
	This allows us, finally, to combine the equations:
	
	in only one (do not forget the association of the operators $E\rightarrow \mathrm{i}\hbar\partial/\partial t$,$p\rightarrow -\mathrm{i}\hbar\vec{\nabla}$):
	
	Using the traditional notation in Tensor Calculus and choosing the natural units ($c=1,\hbar=1$) we have:
	
	which is the usual form of the "\NewTerm{Dirac equation}\index{Dirac equation}" (implicitly a system of four coupled differential equations) or "\NewTerm{relativistic equation of the electron}\index{relativistic equation of the electron}" with the "\NewTerm{covariant derivative}\index{covariant derivative}":
	
	and where the null vector is (for recall...) a vector with four components all equal to zero!
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	In elementary particle physics, the antecedent relation is named the "\NewTerm{covariant relativistic equation of fermions}\index{covariant relativistic equation of fermions}" because it describes particles with spin $1/2$.
	\end{tcolorbox}
	The matrices $\gamma^\mu$ are named commonly "\NewTerm{Dirac matrices}\index{Dirac matrices}". In an even more condensed form (using the "Feynman slash") the Dirac equation is sometimes written:
	
	or more explicitly in natural units (rare form but still correct):
	
	We thus have, as in analogy with Maxwell's equations, differential equations of the first order which have for properties:
	\begin{enumerate}
		\item[P1.] To allow to fall back on the Klein-Gordon equation, in extenso on Wave equation (as for the Maxwell's equations)

		\item[P2.] To take into account (explicitly) describe the spinor character of wave functions as we shall see by looking more closely at the Pauli matrices.
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Since the Dirac equation applies to the spin $1/2$ particles, it also applies to neutrinos whose mass at rest is zero (thus the resolution of the Dirac equation is largely simplified).
	\end{tcolorbox}
	In the purpose now of interpreting the physical content of the Dirac equation, we will use a different representation of the Pauli matrices. We have seen that the representation by the matrices $4\times 4$:
	
	was named "\NewTerm{Chiral representation}\index{Chiral representation}" whereas we will now use the "\NewTerm{Dirac representation}\index{Dirac representation}" defined by:
	
	We easily check (elementary linear algebra) that this representation is obtained by the transformation:
	
	where:
	
	Let us recall that $U^\dagger$ is the adjoint matrix (the conjugate of the transposed matrix) of $U$. Now, when all the elements are real as it is the case above and the matrix is a square one then (\SeeChapter{see section Linear Algebra}) we know that $U=U^\dagger$.
	\begin{dem}
	
	and:
	
	\begin{flushright}
		$\square$  Q.E.D.
	\end{flushright}
	\end{dem}
	Let us now look for solutions specific to the Dirac equation in the form:
	
	By substituting in the Dirac equation and after simplification by $e^{\mathrm{i}\vec{p}\circ\vec{r}-Et}$ we find easily:
	
	Indeed in natural units:
	
	With the representation of Dirac, we get after development (trivial calculation that we can detail on request as always):
	
	Or more explicitly:
	
	Indeed, with the Einstein summation convention (\SeeChapter{see section Tensor Calculus}):
	
	In order for this matrix equation to have nonzero solutions, as usual, the determinant of the matrix must be zero (\SeeChapter{see section Linear Algebra}). We easily check that:
	
	Which implies (do not forget that we are in natural units!):
	
	This is therefore a system of equations very funny (...) to solve....... (do not forget that each term of the above relation is implicitly a $2\times 2$ matrix).
	
	With Chirale, representation we would have obtained (always by adopting the traditional notation of this book for the scalar product):
	
	and we would not have come fall back on such an aesthetic and physical condition for there to be solutions!

Since the mass seems always positive, the Dirac equation thus has four linearly independent solutions, two of which have positive energy:
	
	and two with negative energy:
	
	It is therefore indeed the antiparticles that we had determined during our study of the free Klein-Gordon equation but with the spin in addition, hence the doubling of the additional solutions (two possible spin orientations per particle and by antiparticle). With the Chirale representation, we would not have fallen back on this result. Hence the necessity of the use of the representation of Dirac of the Pauli matrices.

We therefore know that there are solutions to the Dirac equation. Let us now determine these. Let us put:
	
	where $\phi_a$, $\phi_b$ are for recall the two double components of the spinor. We thus write the system of equations (we use, as Dirac did, the simple symbol of multiplication in place of the symbol of the scalar product):
	
	which gives us (do not forget that the term to the denominator is actually a $2\times 2$ matrix...):
	
	Thus, we have:
	
	We know that solutions exist and Quantum Physics imposes that these solutions are linearly independent. Thus, let us choose the solutions for $\phi_a$, $\phi_b$ as being proportional to:
	
	and as (\SeeChapter{see section Spinor Calculus}):
	
	We then have the following possibilities:
	
	The question is now ... should we use:
 	
	Well, for $(1)$ and $(2)$ above we must use $E=+\sqrt{\ldots}$ otherwise $1/(E+m_0)$ becomes a singularity for $\vec{p}=\vec{0}$. For $(3)$ and $(4)$ we must use $E=-\sqrt{\ldots}$ otherwise $1/(E-m_0)$ becomes a singularity for $\vec{p}=\vec{0}$.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The term $E=+\sqrt{\ldots}$ is often referred to as the "\NewTerm{particle solution}\index{particle solution}" in the literature and $E=-\sqrt{\ldots}$ as the "\NewTerm{antiparticle solution}\index{antiparticle solution}". The interpretation of this result as an anti-particle would sometimes be named as we already mentioned the "\NewTerm{Feynman-Stuckelberg inter-operation}\index{Feynman-Stuckelberg inter-operation}".
	\end{tcolorbox}
	Going back to:
	
	and noting the spinors by (we change the notation to come back to the original one):
	
	We finally have using $(1)$ and $(2)$ and by denoting $N()$ the part of the solution we should normalize, the following possible solutions and which are independent:
	
	
	with $E=+\sqrt{\ldots}$ and also:
	
	
	with $E=-\sqrt{\ldots}$. It is from usage to conventionally associate the two above functions to the "\NewTerm{positron}\index{positron}".

	All this can be abbreviated globally:
	
	Let us say indicate that the term $\vec{p}\circ\vec{\sigma}$ which is therefore the projection of the momentum (momentum) on a mathematical entity directly related to the spin is named "\NewTerm{helicity}".
	
	\subsection{Linearized Dirac Equation}
	We saw at the beginning of our study of Wave Quantum Physics that the classic Schrödinger equation of evolution was:
	
	ie a differential equation of the first order with respect to time and the second with respect to the spatial coordinates.

	We had then determined the relativistic Schrödinger equation of evolution (free Klein-Gordon equation) given by:
	
	We notice that by passing to a relativistic form, we now have a differential equation of the second order in time AND space.

	Then through the generalized Klein-Gordon equation which also contained a second-order differential equation in time and space:
	
	and with the free Dirac equation we have just proved that we get in the same way a matrix differential equation of first order in time and second order in space:
	
	These changes of order of the differentials of a relativistic model or not impose of course in the case of a first order to know the initial conditions in time and space of the wave equation, which is feasible. However, when a second order appears, it is necessary to know more about the initial conditions of the derivatives of the wave functions (\SeeChapter{see section Differential and Integral Calculus}). Moreover, even if mathematically the (relative) rigor naturally led us to the different orders obtained, it is strange that changing to a relativist model that we change order of the differentials. Why?: For the quite simple reason that by approximating the relativistic equations we are not able because of the presence of the Planck constant factor to make approximations (development in series of $v/c$) that would bring us back to the first order. The relativistic and nonrelativistic equations seems then a priori incompatible within the non relativistic limits!
	
	The Dirac method to solve this problem will have been the following:

	The orders of the Klein-Gordon differential equation coming from the relation  of the total energy (see the beginnings of our developments of the free Klein-Gordon equation) in the absence of any field:
	
	It seem that Dirac would have had the brilliant idea of linearizing this Hamiltonian by putting (sadly we don't know how he get this idea):
	
	which we will have to determine the parameters $\beta$, $\alpha(\alpha_1,\alpha_2,\alpha_3)$ which can be scalars, vectors or matrices (wait a little ... the answer will come). It follows that the Hamiltonian is also a scalar, a vector, or a matrix.

	Thus, the simplest relativistic evolution wave equation that we can construct will be:
	
	In a much more common form in the literature:
	
	As here:
	
	we find then the prior-previous relation also in the form:
	
	If the linear momentum were to be zero, we would thus fall back on the energy at rest for the Hamiltonian:
	
	where as we shall see later $\beta=\pm 1$.
	
	The validity of this linearization must be verified by falling back on the results obtained during our previous study of the Dirac equation.

	Let us now raise the operator to the square:
	
	and let us put:
	
	At this point, it is important to notice that we may be working with operators (typically matrices) that might not commute because the $\alpha$, $\beta$ are unknown. Therefore, the squaring will be done as follows:
	
	thus simply developed but without simplifying the sum $AB + BA$ into a $2AB$ or a $2BA$ since we are not sure for now if there will be commutativity or not.

	We therefore develop the Dirac Hamiltonian:
	
	By making the products of the terms in brackets and respecting the order of the operators, it comes:
	
	By grouping certain terms:
	
	To be consistent with our linearization assumptions, we must have:
	
	and we will see immediately that to satisfy these conditions the $\alpha$, $\beta$ will have to be $4\times 4$ matrices.

	Written in the form of commutators, we therefore have the following three conditions to satisfy:
	
	We observe the following:
	\begin{itemize}
		\item The square of each operator $\beta$ and $\alpha_j$ is equal to $1$ (or to the unit matrix if it is a matrix...).

		\item $\left[\alpha_i,\alpha_j\right]_{+}=0,\forall i,j=\{1,2,3\},i\neq j$ is an anticommutator

		\item $\left[\alpha_i,\beta\right]_{+}=0$ is an anticommutator
	\end{itemize}
	These three relations can be summarized as follows:
	
	At this point, we need to look for mathematical objects that meet the three conditions above. We could show that a square matrix of dimension $2$ or $3$ does not satisfy the three conditions and a scalar even less!

	Dirac then adopted by analogy with the previous developments, square matrices of dimension $4$ including Pauli matrices (so lucky guy...) and admitted for $\beta$ a unit matrix (this choice made by Dirac is particular, there are possible choices hence the necessity to check the theory with experimental observations!).

	So what we denoted "$1$" before is actually a square unit matrix of dimension $4$! And so the wave functions must have $4$ components and mathematical beings that have specificity are the spinors!

	The matrices considered by Dirac (which are a particular choice!) are therefore for $\alpha_i$, $\beta$:
	
	and for example in certain domains of quantum physics we use Weyl's choice:
	
	In the choice considered by Dirac or Weyl, we have the following Pauli matrices and unit matrix:
	
	This leads to the matrices $\alpha_i$, $\beta$ (we recognize that the $\alpha_i$ are in fact the gamma matrices $\gamma_i$ introduce earlier above at the difference of a $-$ sign on one component but that anyways leads to exactly the same results!!!!!):
	
	We can be check that the linearization conditions are satisfied by the preceding matrices:
	\begin{itemize}
		\item First condition:
		
		Similarly for the $\alpha_j$:
		
		The first condition is therefore fulfilled!
	
		\item Second condition (caution with notations that sadly slip a bit by tradition between matrices and scalars!):
		
		and:
		
		Therefore (caution! this is in fact a $4\times 4$ matrix!!):
		
		The second condition is well fulfilled!
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		The last relation should be written:
		
		and if we take the gamma matrices with the Dirac representation introduce earlier above, the same is satisfied bu generalized if we include $\gamma_0$ to:
		
		where $\nu_{ij}$ the Minkowski metric with signature $(+ - - -)$. This relation is important because it is sometimes used as the definition of the gamma matrices (!!!) and it makes appears the metric!!!
		\end{tcolorbox}
		
		\item Third condition:
		
		The third and last condition is thus fulfilled.
	\end{itemize}
	Referring to the beginning equation written with the Dirac formalism:
	
	With:
	
	Which gives finally:
	
	We find ourselves in front of a state function having $4$ components in which:
	
	are spinners and the set:
	
	is therefore "\NewTerm{Dirac bispinor}\index{Dirac bispinor}" and we denote by:
	
	the "\NewTerm{Dirac state function}". The reader will have notice that we fall back one the same concepts as in our study of the free nonlinearized Dirac equation. This is quite a good sign!
	
	By developing, it comes:
	
	For a free electron, we know that the solution is:
	
	With Dirac bispinor we have:
	
	with:
	
	where $b_1$ to $b_2$ are the components of the Dirac bispinor.
	
	We will write:
	
	By calculating their derivatives with respect to $t$:
	
	With \eqref{diraclin02} and \eqref{diraclin03} in \eqref{diraclin01}, it comes:
	
	Hence a system of equations whose unknowns are $b_1,b_2,b_3,b_4$:
	
	We have solutions that are not all zero if and only if the determinant of the coefficients is zero (to know the reasons, refer to the section of Linear Algebra) and therefore an infinite number of solutions (for the components of the Dirac spinor). Therefore:
	
	By simplifying by $c$:
	
	The division in the preceding determinant allows the calculation of the partial determinants (\SeeChapter{see section of Linear Algebra}):
	
	In solving the preceding determinant, it comes:
	
	Hence the following relation:
	
	The values of the energy given by the Dirac equation are thus (we recognize here a famous relation determined in the section of Special Relativity!):
	
	Therefore:
	
	If we adopt for $E_{+}$ two constant values for $b_1$ and $b_2$ we have two relations to calculate $b_3$ and $b_4$, thus:
	\begin{itemize}
		\item With $($\ref{diracdeterminant} $c)$:
		
		Thus:
		
		
		\item With $($\ref{diracdeterminant} $d)$:
		
		Thus:
		
		By adopting $b_1=1$, $b_2=0$, it comes:
		
		Taking the natural units:
		
	\end{itemize}
	If we adopt for $E_{-}$ two constant values for $b_3$, $b_4$ we have two relations to calculate $b_1$, $b_2$, thus:
	\begin{itemize}
		\item With $($\ref{diracdeterminant} $a)$:
		
		Thus:
		
	
		\item With $($\ref{diracdeterminant} $b)$:
		
		Thus:
		
		Notice that by adopting $b_3=1$, $b_4=0$ it comes:
		
		With the natural units:
		
		By adopting $b_3=0$, $b_4=1$ it comes:
		
		Either with the natural units:
		
	\end{itemize}
	Even if the method is different, we fall back on the coefficients of the spinors obtained in our study of the classical free Dirac equation. This confort us in the assumptions made at the beginning of this linearization and validates these results. Moreover, the preceding relations also indicate a degeneracy of order two of the energy for each value of the pulsation. In the absence of an external field, the free electron is therefore not influenced by the orientation of its spin. We thus find the same results for either the classical or linearized free Dirac equation.

	However, Dirac's explanation for the positive and negative energies is that his equation applies not only to the state of a positive energy particle (in this case the electron) but also to the state of a particle with negative energy (its antiparticle is the "\NewTerm{positron}\index{positron}"). The absolute value of these two energies being strictly equal.

	The presence of the negative sign affecting the energy posed a problem at the time for its interpretation (in the context where we omit the time variable since we had seen when studying the free Klein-Gordon equation, a particle with negative energy can be seen as a particle that goes back in time).

	If we reason in the case where the term $p^2c^2$ is small compared to $m_0c^2$, we ask ourselves: how and what are the consequences of a transition between a state of energy $m_0c^2$ and that of the state of energy $-m_0c^2$ with a gap of $-m_0c^2$ (we will find again this value in our study of materialization in the section of Nuclear Physics).

	Dirac uses the image of a sea of negative energy named "\NewTerm{Dirac sea}\index{Dirac sea}" (since, remember, the number of solutions to our matrix system is infinite, hence the analogy with a sea than a discrete context) in which all negative energy states are occupied by the electrons and the positive energy states would be empty. If an electron is subjected to a transition (via, for example, a photon of energy greater than $2m_0c^2$), it leaves this sea leaving behind a gap (the famous "hole" of positive charge to which the electronics sometimes refers to...). This gap becomes a positive charge, of energy $E_{+}$. The appearance of this gap is assimilated to the appearance of a particle having a positive charge. Obviously, we can imagine the opposite case, it is only a matter of conventions.
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/dirac_sea.jpg}	
		\caption{Dirac Sea (source: ?)}
	\end{figure}
	
	\pagebreak
	\subsection{Generalized Dirac Equation}
	In the case of the free electron, we have thus often seen and proved that the Hamiltonian has as expression:
	
	In the case of an electron moving in an electromagnetic field, we have also proved in our study the Klein-Gordon equation at the beginning of this section that:
	
	Therefore:
	
	OK finish for the recall!
	
	If we now take back the Dirac Hamiltonian for the free electron proved earlier above (which for recall is a $4\times 4$ matrix):
	
	and from the fact that we must add to the Hamiltonian the term of potential electrostatic energy via the potential vector, we get:
	
	where the potential $U$ will also have to be expressed in the form of a diagonal $4\times 4$ matrix.

	We then get the generalized Hamiltonian of Dirac in the following traditional form:
	
	Therefore we have in another well known form:
	
	
	\pagebreak
	\subsection{Pauli Equation}
	In quantum mechanics, the Pauli equation or Schrödinger–Pauli equation is the formulation of the Schrödinger equation for spin-$1/2$ particles, which takes into account the interaction of the particle's spin with an external electromagnetic field. It is the non-relativistic limit of the Dirac equation and can be used where particles are moving at speeds much less than the speed of light, so that relativistic effects can be neglected. It was formulated by Wolfgang Pauli in 1927.
	
	Let us now consider a two-component representation of the spinor:
	
	and let us recall that:
	
	Therefore it comes:
	
	Hence:
	
	What after simplification gives:
	
	Before continuing, let us open an important parenthesis, otherwise we will not be able to find a solution to these two equations.

	Let us recall that one of the spinors solution of the free Dirac equation was given by (as we have proved it earlier above):
	
	Either in International Standard units:
	
	In order to simplify the calculation of the prior-previous equations we will reduce the situation to a non-relativistic case, that is to say when the mass energy is much greater than the kinetic energy. Therefore the previous solution becomes (we forget the second which would cause problems ...):
	
	The idea is simple but you had to think about it! It should be notice that depending on the authors and professor it is the following term which is named "\NewTerm{helicity}\index{helicity}":

	as it is the projection (as we will see later) of the spin operator on the linear momentum.
	
	The idea is then to find a solution to:
	
	such that we make a non-relativistic approximation and we cancel the magnetic field (in extenso the vector potential), we fall back on:
	
	After many trial and errors (yes Quantum Physics was not made in one day... not even if one year...) we find that a particular solution satisfying our previous idea is:
	
	Indeed:
	
	We finally have $2$ equations (yes yes... we know that in fact as spinor have two components this if $4$ equations...):
	
	Now let us consider only the second equation:
	
	Assuming (for free, after which it will be necessary to compare that assumption to experimental results) that the term $|\partial_t \phi_b$ is much smaller than $|2m_0c^2\phi_{b0}|$ we can write:
	
	By making the same assumption with $|qU\phi_{b0}|$ we have:
	
	We then have (do not forget that the denominator is a diagonal matrix in reality ...):
	
	Now, we see that if the magnetic field (in extenso the vector potential) vanishes, we fall back on well our idea of departure! So the bet is good!

	Because of all these downward approximations, the $\phi_b$ component is often taken as the "small" component of the wave function $\Psi$, relatively to the large component $\phi_a$.

	The first equation:
	
	can now be simplified easily by taking the previous solution such that:
	
	Therefore:
	
	Using the remarkable identity proved in the section of Spinor Calculus:
	
	we get:
	
	Let us detail the cross product by remembering that it will act as an operator on $\phi_{a0}$:
	
	But, we have (\SeeChapter{see section Spinor Calculus}):
	
	Let us just look at the component in the upper left corner (if not the calculations are too long and boring to do) of this sum of matrices. We must not forget that this component of the matrix will act on the first component as an operator on $\phi_{a0}$ (written with the same notation as the spinor itself to avoid to many indices):
	
	But:
	
	Therefore:
	 
	But, we recognize here the third component of a cross product that does not act as an operator. Finally, it comes:
	
	Therefore:
	
	Thus, the relation of the main component:
	
	becomes:
	
	The reader will notice that the notations are not the most enjoyable (between vectors, matrices and constants it is necessary to follow well to know what is in there ...).

	After rearrangement:
	
	which constitutes the "\NewTerm{Pauli equation}\index{Pauli equation}" and thus describes in a relativistic way the two components of $\phi_{a0}$ of the spin of the electron (this is really a two-equation system).

	The expression:
	
	is named the "\NewTerm{Stern-Gerlach term}\index{Stern-Gerlach term}" and represents the interaction energy of the magnetic field with the intrinsic moment of the electron (this is an $2\times 2$ matrix for recall).

	The Pauli equation, and therefore that of Dirac (since the latter is more general), give the correct gyromagnetic factor for a free electron of $g=2$. To verify this, let us take as it has been done experimentally, a constant magnetic field:

	We can easily verify that the choice of a vector potential corresponding to a constant magnetic field is then:
	
	This choice will have the effect of make disappear the vector potential in favor of the magnetic field in the Pauli equation, which will reveal the interaction between the orbital angular momentum and the magnetic field as we shall see:

	Indeed, we have:
	
	As requested by a reader, here is the proof of that latter relation:
	
	We then have in Pauli equation:
	
	But, let us recall that we have seen in the section of Vector Calculus that:
	
	This gives us therefore:
	
	where:
	
	denoted also $\vec{b}$ (\SeeChapter{see sections of Classical Mechanics and Corpuscular Quantum Physics}), is therefore an operator representing the angular momentum.

	Therefore we have:
	
	By proceeding as Dirac did, that is, by defining the spin operator as the matrix (oh! we fall back on something known and seen in the section of Wave Quantum Physics!! it's beautiful no!?):
	
	This relationship will be very useful to us in the section Quantum Computing. Let us indicate the following notation (quite logic) in the literature for the intrinsic magnetic moment:
	
	The Pauli equation is then written:
	
	or after factorization:
	
	or even more condensed by being careful to differentiate what is an operator, what is a vector and what is a simple multiplication from an inner product and what is a function of a spinor ... (only happiness...) and by putting:
	
	as being the "\NewTerm{Landé factor}\index{Landé factor}" or "\NewTerm{gyromagnetic factor}\index{gyromagnetic factor}", we have:
	
	with $\vec{\mu}_L$ being the "\NewTerm{orbital magnetic moment}\index{orbital magnetic moment}" (by comparing with the expression of the magnetic moment proved in the Magnetostatic section you will see that it actually has the same form and also the same units), $\vec{\mu}_S$ the "\NewTerm{magnetic moment of spin}\index{magnetic moment of spin}".

	Sometimes we find the expression in parentheses in the above-mentioned relationship as follows:
	
	With all this the Stern-Gerlach term (magnetic moment) becomes explicitly:
	
	Recalling that:
	
	is the Bohr magneton which we had introduced rigorously in the section of Corpuscular Quantum Physics.
	
	Interestingly ... the neutron has (experimental observation!) a spin magnetic moment that is non-zero yet its charge is zero ... so it must be composed of charged particles (if we remain within the framework of a model of the explanation of nature by particles ...). It is interesting to know that in practice we have for the electron, the proton and the neutron:
	
	and the following values were measured for spin $1/2$ particles such as the electron, proton and neutron (caution! the sign may change depending on how the Dirac equation is written):
	
	Therefore the Dirac theory in the non-relativistic framework predicts in a good approximation that the elementary particles of spin $1/2$ have a gyromagnetic factor of $2$, and this prediction consistent with the experiment for the electron is the greatest triumph of the Dirac equation. The deviations of the theoretical value (consequent deviations!) for the proton and the neutron are perfectly explained in the framework of quantum electrodynamics. These deviations show that the structure of the proton and that of the neutron are more complex (subparticle composition) than a spin $1/2$ point particle whereas in the case of the electron, it would seem that has no underlying substructure.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The gyromagnetic factor is sometimes taken as negative but it is only a matter of convention.
	\end{tcolorbox}
	It is by the way the interaction term between the magnetic field and the addition of the orbital angular momentum and intrinsic spin:
	
	of the Pauli Hamiltonian which gives the values measured by the Zeeman effect! Reason why this expression is sometimes named "\NewTerm{Zeeman energy}\index{Zeeman energy}" and sometimes written in the form of a Hamiltonian operator:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The above treatment of the Zeeman effect describes the phenomenon when the magnetic fields are small enough that the orbital and spin angular momenta can be considered to be coupled (\SeeChapter{see section Wave Quantum Physics}). For extremely strong magnetic fields this coupling is broken and another approach must be taken. The strong field effect is called the "\NewTerm{Paschen-Back effect}\index{Paschen-Back effect}".
	\end{tcolorbox}
	We know that this last relation can also be written for any single particle (\SeeChapter{see section Wave Quantum Physics}) immersed in a magnetic field collinear to the $z$-axis and zero in the other directions:
	
	where $\gamma$ is the "\NewTerm{gyromagnetic ratio}\index{gyromagnetic ratio}" and the $\mu_B$ the "\NewTerm{Bohr magnetron}\index{Bohr magnetron}" (and we saw in the proof of this relation that the Bohr magnetron is only a special case with the Landé $g$ factor equal to that of the electron).
	
	Let us now focus only on the smallest variation in energy between two states of the orbital angular momentum (since the smallest variation in the variation is $1$ by the quantification of $l$). It will then always be by equal spin to the form:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	As $\gamma\hbar\cong 5.8\cdot 10^{-5}\;[\text{eV}\cdot\text{T}^{-1}]$ and that largest field which can regularly be produced in a lab is around $10$ [T]. Thus, $\Delta E_{p,l}\cong 10^{-3}$ [eV]. This is a very small energy. Thus magnetic effects are wiped out by thermal fluctuation at about $10$ [K]! Thus experiments which measure the magnetism of non-interacting systems must be carried out at low temperatures. These experiments typically measure the susceptibility
of the system with a Faraday balance or a magnetometer (SQUID).
	\end{tcolorbox}
	This variation of energy will be restored in the form of electromagnetic waves corresponding to:
	
	hence:
	
	which is the "\NewTerm{Larmor relation}" (not to be confused with the "\NewTerm{Larmor radius}" seen in the Magnetostatic section). But in practice we mainly use the relation:
	
	which gives what we name the "\NewTerm{resonance frequency}" of the orbital angular momentum.
	
	Similarly, for the intrinsic kinetic moment, we get for a constant orbital kinetic moment (during the spin transition from $+1/2$ to $-1/2$ or vice versa):
	
	and putting again it comes:
	
	hence:
	
	and obviously:
	
	This last study of energy variation due to the application of a magnetic field and the resulting energy emission frequencies is the basis of "\NewTerm{nuclear magnetic resonance (NMR)}\index{nuclear magnetic resonance}", which therefore works only for the particles having the magnetic moment of spin $\mu_S$ (by construction of the Pauli Hamiltonian).
	
	NMR consists in modifying the nuclear magnetic moment, in other words, to pass the nucleus from one level of energy to another by applying magnetic fields to the sample to be studied. When the energy of the photons constituting these magnetic fields corresponds to the energy of transition from one level of energy to the other, these photons can be absorbed by the nucleus: we then say that there is "\NewTerm{nuclear resonance}\index{nuclear resonance}".
	
	We can characterize the transition energy of the nuclear magnetic spin moment by giving the frequency of the electromagnetic wave that allows resonance. For the usual fields (of the order of the Tesla), the proton resonance takes place in the range of radio waves ($100$ [MHz] approximately): $42$ [MHz] in a field of $1.0$ [T] and $63$ [MHz] In a field of $1.5$ [T].
	
	Let us point out that on the way that in some textbooks, since we have:
	
	Then for a particle with two possible states of spin the energy of each of the level is then obviously half the above difference, which is often denoted by:
	
	
	\subsubsection{Landé g factor}
	We have seen previously that the Landé g-factor is a multiplicative term appearing in the expression for the energy levels of an atom in a weak magnetic field. We seek now for a more detailed expression and u will see that all the abstract stuff we have introduced during our study of orbital-spin coupling in the section of Wave Quantum Physics will be useful to us.
	
	In the weak field limit, we assume that the magnetic dipole moment of the atom is proportional to the total angular momentum $\vec{J}$:
	
	where $m_B$ is still the Bohr magneton and $g_J$ is the Landé g factor for $\vec{J}$. To find a value of $g_J$, we relate $\vec{J}$ to the know values of $g_L$ and $g_S$ and we use (\SeeChapter{see section Wave Quantum Physics}):
	
	and therefore:
	
	with the assumption that:
	
	Hence:
	
	Therefore after simplification:
	
	We then take the dot product with $\vec{J}$ and get:
	
	Therefore:
	
	That is often written for simplification purposes:
	
	Now let us recall that in the section of Wave Quantum Physics during the study of spin-orbit coupling we have proved that (still simplifying the notation of the norm of a vector):
	
	Therefore:
	
	Now we introduce the eigenvalues of $J^2$, $L^2$ and $J^2$ as we have determined the during our study of Angular Momentum and Spin and we simplify by $\hbar^2$ directly and that gives:
	
	Therefore:
	
	If we put $g_L=1$ and $g_S\cong 2$ (implying that the spin angular momentum is twice as effective in producing a magnetic moment) then the above expression simplified to:
	
	Now following the same reasoning, let us determine in a most robust was the relation:
	
	determined earlier above.
	
	The problem with evaluating the scalar product above is that $\vec{L}$ and $\vec{S}$ continually change in direction as shown in the vector model. The strategy for dealing with this problem is to use the direction of the total angular momentum $\vec{J}$ as a coordinate axis and obtain the projection of each of the vectors in that direction. This is done by taking the scalar product of each vector with a unit vector in the $\vec{J}$ direction:
	
	Using the fact that:
	
	we get:
	
	These vector relationships must be evaluated and expressed in terms of quantum numbers in order to evaluate the energy shifts. Carrying out the scalar products above leads to:
	
	And we have also proved in the section of Wave Quantum Physics that:
	
	Rearranging gives the scalar product term needed:
	
	Substitution into the energy expression gives:
	
	which can be reduced to:
	
	where using eigenvalues we have:
	
	so we fall back on the approximation of $g_J$!
	
	So finally:
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us give a word about the fine structure of sodium atoms.\\
	
	As we have seen in the section of of Corpuscular Quantum Physics and following the aufbau principle (also introduced in the same section) for the atom of Sodium $\mathrm{Na}$ we have seen that its electronic configuration was:
	
	Explicitly we then say (following orbital-spin coupling notation), that is ground state is then:
	
	The first excited state is obtained (still following the aufbau principle) by promoting the $3s$ electron to the $3p$ orbital, to obtain the:	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	configuration. In a sodium-vapor lamp for example, sodium atoms are excited to the $3p$ level by an electrical discharge, and return to the ground state by emitting yellow light of wavelength $589$ [nm].\\
	
	Now let us recall the following table (\SeeChapter{see section Wave Quantum Physics}):
	
	Therefore for the ground state $3^2s_{1/2}$ of the Sodium we get $l=0$ and $j=1/2$. And in the absence of magnetic field $B=0$ we have:
	
	so there is no fine structure of the Sodium spectrum.\\
	
	But when the electron move to $3p$ under the influence of a weak magnetic field, we have (still look to the table above) that $l=1$ with $j=1/2$ and $j=3/2$. \\
	
	Therefore a fine structure appears for $3p$ and we get that the transition $3s\rightarrow 3p$ is split into a doublet that we name for Sodium the "D-lines" and respectively each line is named $\text{D}_1$ and $\text{D}_2$ lines. 
	\begin{figure}[H]
		\centering
		\includegraphics{img/atomistic/sodium_zeeman_effect.jpg}	
		\caption{Source: Hyperphysics}
	\end{figure}
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	The fact that there is a doublet shows the smaller dependence of the atomic energy levels on the total angular momentum. This effect is called the "\NewTerm{spin-orbit effect}\index{spin-orbit effect}".\\
	
	In fact this doublet are also visible without the presence of an additional externally applied magnetic field, but with a magnetic fields these levels are further split by the magnetic interaction (and therefore more easily measurable), showing dependence of the energies on the $z$-component of the total angular momentum. This splitting gives the Zeeman effect for sodium.\\
	
	Now let us roughly approximate the magnetic field necessary to create this doublet by taking:
	
	Assuming\footnote{In fact even if we we caculate precisely $m_j$ for each orbit we get almost the same result with less than $10\%$ error} same $g_L$ and same $m_j$ we get $B=18$ [T]. This is a very large magnetic field by laboratory standards. Large magnets with dimensions over a meter, used for NMR and ESR experiments, have magnetic fields on the order of a Tesla.\\

	Experimentally (therefore in reality) it was the opposite, the field was applied and the $g_L$ were determined as we get:
	
	where $g_L$ was calculated using:
	
	\end{tcolorbox}
	
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{90} & \pbox{20cm}{\score{4}{5} \\ {\tiny 4 votes,  74.39\%}} 
	\end{tabular} 
	\end{flushright}

	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Nuclear Physics}
	\lettrine[lines=4]{\color{BrickRed}N}uclear physics is the field of physics that studies the constituents and interactions of atomic nuclei. The most commonly known applications of nuclear physics are nuclear power generation but the research has provided application in many fields, including those in nuclear medicine and magnetic resonance imaging, nuclear weapons, nuclear batteries as betavoltaic devices (we will come back on this subject later), ion implantation in materials engineering, and radiocarbon dating in geology and archaeology.
	
	\subsection{Nuclear Weapon}
	Without wishing to make any confusion, we consider as essential, at the time when nuclear weapon serves as a dissuasion vector and thus as element of global stability (but also threat of global destruction ...), for the general culture of the engineer to know some basic properties of the atomic fission bomb. We'll exceptionally in this small subsection without mathematics (mathematical developments of nuclear weapons and nuclear power plants will be presented during our study of neutronic further below) speak a little bit of this mass destruction weapon that often fascinates the young students.
	
	For sure, we will study later how theoretically cause a divergent chain reaction in a given closed constrained volume. However, the reader must not expect to gain the knowledge necessary to manufacture such a weapon since it did not appeal only to knowledge of physics, but also electronics, mechanics, chemistry, mathematics, etc.
	
	About an explosion, usage has immediately propagate to compare the energy of a nuclear weapon than a well know explosive material: the Trinitrotoluene (TNT). This TN provides $4,200,000$ joules per kilo, but the energies of nuclear weapons are such high that it is more meaningful to evaluate them in thousands of tonnes - or kilotons of TNT (subsequently thermonuclear weapons represented a new leap in explosive energy: the practical unity of comparison is one million tonnes - megaton of TNT).
	
	The fission of $56$ grams of Uranium 235 or Plutonium 239 gives the equivalent of $1$ kiloton with the fission of $145^{21}$ atoms (this is not even an integer value of Avogadro's number !!).
	
	The first know nuclear test explosion at Alamogordo in July 16, 1945 - was estimated at $20$ kt, with good accuracy because it had been possible to set up multiple measuring devices.
	
	Those of August 6, 1945 on Hiroshima (Uranium 235) and August 9 on Nagasaki (Plutonium 239) were first estimated as $20$ kt. Subsequently, and after a detailed study on the effects of the blast, their energies were respectively reduced to about $17$ to $15$ kt respectively. This, however, did not represented less of the equivalent of a load of TNT of a convoy of around $6,000$ trucks from the US Army.
	
	Here is a very good diagram (sad I found it only in French) both interesting and persuasive on the effects of an atomic bomb (for information from a speed of $220$ [km/h] an average human being is lifted from the ground):
	\begin{figure}[H]
		\begin{center}
		\includegraphics{img/atomistic/nuclear_bomb_range.jpg}
		\end{center}	
		\caption{Effects of fission weapon of $1$ Mt as a function of the distance (source: Pour la science)}
	\end{figure}
	So in other words here is a summary and approximately the effects of a 1 Mt fission weapon exploding at 2,450 meters of altitude (knowing that today the Americans and the Russians have nuclear fusion weapons with firepower exceeding the 50 megatonnes...):
	
	Up to $1.3$ [km], everything is down, even reinforced buildings. Up to $4.8$ [km], most factories and commercial buildings are destroyed; houses made of brick and wood are blow up, and debris scattered. Up to $7$ [km], commercial sets of light structure and private residences are demolished. Heavier buildings are seriously damaged. Up to $9.5$ [km], the walls of small vessels are reversed, private residences severely deteriorated. The breath (or pressure) is still powerful enough to kill the people who are outside (explosion of the lungs). Up to $18.6$ [km], different buildings are damaged, the streets are littered with debris and glass tiles. Between $10$ to $20$ minutes after the explosion, the debris drawn into the depression of the stem of the mushroom, fall to the ground. Follow after $1$ to $2$ hours later, the debris being in the head of the mushroom.
	
	Most of the effects shown in the diagram above are not proportional to the energy. There is therefore no need to make a simple multiplication for a weapon with multiple power that used for the above figure!
	
	For people interested in mushroom size (...) here is a figure whose source in unknown:
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=0.75]{img/atomistic/nuclear_bomb_mushroom.jpg}
		\end{center}	
		\caption{Mushroom size for Little Boy Hiroshima (1) and Castle Bravo(2)}
	\end{figure}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	For a nice little calculation on nuclear bombs using dimensional analysis the reader can refer to section Principles of Mechanics where we give the expression of the energy of a bomb according to the radius of the fireball.
	\end{tcolorbox}
	
	Before we start the mathematical part, we emphasize and reiterate that we will limit ourselves only to theoretical developments made between 1890 and about the years 1935 (beyond the complexity of theories requires too many pages to a general book like this one).
	
	\pagebreak
	\subsection{Radioactivity}
	When we experimentally analyze radioactivity, we first perceive that the nucleus does not emits its constituents. Then we discover new forces, that struggles and dominate in turn. Finally, new particulate matter, and even antimatter particles appear. Deciphering these puzzles provided a coherent picture of the infinitely small world that radioactivity has revealed, a world where physical laws escape to the common intuition of daily practice of our macroscopic world.
	
	From the start, the radioactivity has surprised. From the years 1900 it was known that radiation from uranium and his descendants had three components, named "alpha" $(\alpha)$, "beta" $(\beta)$, and "gamma" $(\gamma)$ separable by the action of a magnetic field as shown symbolically in the image below:
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=0.85]{img/atomistic/radition_separation.jpg}
		\end{center}	
		\caption{Separation of radiation using a magnetic field (source: Pour la Science)}
	\end{figure}
	Later, it was shown that the alpha radioactivity was the emission of a helium nuclei (2 protons and two neutrons), beta radiation the emission of electrons. From these observations, it was logical to assume that the nucleus was formed of these three types of particles (protons, neutrons and electrons), which is not the case: the  components of nucleus seems to have been discovered by J. Chadwick in 1932.
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=0.7]{img/atomistic/radiation_penetration.jpg}
		\end{center}	
		\caption{Typical radiation penetration}
	\end{figure}
	
	So why do radioactive nuclei not emit protons or neutrons? How do they eject nuclei other than their constituents? These issues must be preceded by another, perhaps more fundamental, why some nuclei are radioactive? The response is the same for all spontaneous physical phenomena. The apple falling from the tree, for example: it is because the system can reach a more stable state by losing potential energy, the excess energy escaping in the form of kinetic energy, i.e. in the form of movement.
	
	This reason also explains why the isotopes emit no protons or neutrons alone because often at the quantum structure of the nucleus, it is more favorable at the energy level to emit a small nucleus or to change a proton into a neutron (the quantum study of the nucleus exceeds the mathematical framework of the topics covered in this book actually).
	
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] Any chemical element (\SeeChapter{see chapter Chemistry}) is characterized in the chemical point of view by its number of protons $Z$ named "\NewTerm{atomic number}\index{atomic number}".
		
		\item[D2.] The "\NewTerm{mass number}\index{mass number}" $A$ is by definition the number of protons $Z$ summoned by the number of neutrons $N$ for the given chemical element. Thus, the latter is completely characterized if we know its name or atomic number $Z$ and it  number $N$ of neutrons or its mass number $A$. We usually denote any element in the form:
		
		The chemical elements of the same species (same $Z$) may have different numbers of neutrons $N$, that is to say therefore different mass numbers $A$, then we speak of "\NewTerm{isotopes}\index{isotopes}" or "\NewTerm{nuclides}\index{nuclides}" (today this seems obvious as a definition but it took many years of research to get to this concept and we own this particularly to the works of Niels Bohr). The chemical elements which have the same $A$ are named "\NewTerm{isobars}\index{isobars}".
		\begin{figure}[H]
		\centering
		\includegraphics[width=\textwidth]{img/atomistic/chart_of_the_nuclides.jpg}	
		\caption{Table of isotopes (source: ?)}
	\end{figure}
		Obviously, the nuclear energy (of the nucleus) associated with the same chemical element differs depending on the mass number $A$ and we will prove that there is number $A$ for which energy this minimal. Isotopes for which the energy is not a minimum may, for some of them and spontaneously, release the excess energy by doing a form of desintegration.
		
		\item[D3.] The property of certain atoms to spontaneously change the structure of their nuclei to reach a lower energy level, more fundamental, is named "\NewTerm{radioactivity}\index{radioactivity}". We then talk about "\NewTerm{radio-isotopes}\index{radio-isotopes}" for the atoms involved.
		
		The chemical properties of an atom depend (\SeeChapter{see section Chemistry}) on the number and arrangement of electrons in the cloud (corresponding to $Z$). Thus all isotopes of the same element have the same chemical properties (it is this chemical characteristic which is the basis for nuclear medicine). These are sort of "brothers" atoms. However, the small difference in mass of the nucleus makes that their physical properties differ somewhat.
		
		\item[D4.] The "\NewTerm{isotones}\index{isotones}" are the isotopes of various chemical elements (different $Z$) with the same number of neutrons $N$.
		
		 Rather than speak of "chemicals elements" when we actually are implicitly referring to the nucleus, the term "\NewTerm{nuclide}\index{nuclide}" is preferred.
		 
		 The smallness of atoms is an obvious problem of mass measurement. That is why it was preferred by physicists and chemists to develop an atomic mass system which is a system of numbers proportional to the real mass of the atoms.
		 
		 Since there are infinitely many possible systems, one was chosen judiciously as a reference and is the number $12$ for the carbon-$12$ isotope (for more details see the section of Analytical Chemistry):
		 
		where "\NewTerm{amu}" is the abbreviation of "\NewTerm{atomic mass units}\index{atomic mass units}".
		
		This choice has for interesting consequence to give to the proton and neutron an atomic weight near to the unit.
		
		We can therefore connect the I.S. system (\SeeChapter{see chapter Principles}) with the system of atomic mass units (amu).
		
		\item[D5.] The "\NewTerm{atomic mass unit}" is defined as the mass of the $1/12$ of the atom of carbon ${}^{12}C$, we have (the mass of the electrons is neglected as very small compared to that of nucleons):
		
		Therefore the uma mass of the proton is equal to:
		
		 for the value allowed the reader is referred to the literature of the ad hoc international norms (because it varies depending on the new versions of international norms).
		 
		However the reader must be careful, though the molar mass of a different isotope than ${}^{12}\mathrm{C}$ by adding the masses of the nucleons (protons and neutrons) that make up its nucleus because we must take into account the mass defect (a concept that we will see further below).
	
		The masses can also be expressed in energy units because there is a mass-energy equivalence as we have prove in the section of Special Relativity from the relation $E=mc^2$. The energy units used in nuclear physics is often the "\NewTerm{electron volt}\index{electron volt}".
	
	  \item[D6.] An "\NewTerm{electron volt}\index{electron volt}" denoted [eV] is the energy acquired by an elementary charge subject to a potential difference of $1$ [V].
	  
	  Thus, according to the relationbetween energy and the electrostatic potential $E=qU$ (\SeeChapter{see section Electrostatics}), we have:
	
	We get from this since the speed of light in vacuum is almost equal to $c=299,792,458\;[\text{ms}^{-1}]$:
	
	  
	 By mass–energy equivalence, the electronvolt is also a unit of mass. It is common in particle physics, where units of mass and energy are often interchanged, to express mass in units of $\text{eV}/c^2$. It is common to simply express mass in terms of "eV" as a unit of mass, effectively using a system of natural units with $c$ set to $1$ (\SeeChapter{see section Principia}). The mass equivalent of $1$ $\text{eV}/c^2$ is
	
	Again, for the accepted value in practice the reader has to refer to the literature of the ad hoc international norms (because it varies depending on the new versions of these norms).
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Practicing astrophysicists and plasma physicists routinely refer to temperatures in units of [eV] or [keV], even thought this is wrong, because temperature is not dimensionally equivalent to energy. Nevertheless, they still do it, with the Boltzmann constant being implicitly included in the conversion. Here are formulas for temperature in Kelvin to [keV] conversion:
	
	because $1$ [eV] is by definition the energy required to move a charge $e$ through a potential difference of $1$ [V] and is equal to $1.6\cdot 10^{-19}$ [J]. Therefore:
	
	\end{tcolorbox}
	\end{enumerate}
	
	\pagebreak
	\subsubsection{Disintegration}
	Some nucleus therefore have the property of spontaneously change their internal structure to achieve a basic level of energy. This transformation is accompanied by the emission of particles and / or electromagnetic radiation. The residual core can be radioactive too and process to other transformations afterwards or be stable.

	The radioactive decay of an isotope is a random phenomenon and we can never tell when a nucleus will disintegrate (probability no memory effect).
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	To prove this assertion, the reader may refer to the section of Quantitative Management in the subsection of Queuing Theory, in particular modeling arrivals. Indeed, the development is identical in all point but only the object of study changes (that are no longer phone calls but disintegration). Thus, we proved that under certain assumptions the phenomenon follows a Poisson law and we we prove a little bit afterwards that it has no memory!
	\end{tcolorbox}
	We can only give the probability of disintegration (decay) per unit time. This probability is given by the "\NewTerm{disintegration constant}\index{isintegration constant}" and has for unit the inverse of time: $\lambda\; [\text{s}^{-1}]$. This constant can be calculated as we have already prove it during our the study of the tunnel effect in the section of Wave Quantum Physics.

	The radioactive constant varies for all known isotopes:
	
	Given now $N (t)$ the initial stock of atoms of a radioactive isotope at time $t$. The number of atoms disintegrating during the infinitesimal time $\mathrm{d}t$ is equal to:
	
	leading to a decrease in the stock obviously equal to:
	
	The decay differential equation (\SeeChapter{see section of Differential and Integral Calculus}) is thus written:
	
	or:
	
	which has the simple solution (\SeeChapter{see section of Differential and Integral Calculus}):
	
	with $N_0=N(t_0)$ being the initial stock of nucleus at time $t_0$. 
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	$N(t)$ does not represent the number of atoms remaining at time $t$ but the most likely number of radioactive atoms remaining at time $t$ !!
	\end{tcolorbox}
	
	In the practice, the measurement of the decay constant is done thanks the decrease of the activity (see below) of the individual isotope.
	
	\paragraph{Half-life isotope}\mbox{}\\\\\
	\textbf{Definition (\#\mydef):} The "\NewTerm{half-life period}\index{half-life period}" or just "\NewTerm{half-life}\index{half-life}" $T_{1/2}$ of an isotope is the average time that it takes for $50\%$ of the radioactive nuclei stock of a given isotope to disintegrate:
	
	Thus we have a very important relation between the half-life period and the decay constant!

	If the radioisotope has the choice to disintegrate in two separate path of disintegrations characterized by two distinct radioactive periods $T_{1/2}^1$ and $T_{1/2}^2$, the half-life of this nuclide is defined as the mean:
	
	and we calculate the number of remaining nuclides by the relation:
	
	
	\pagebreak
	\subsubsection{Activity}
	\textbf{Definition (\#\mydef):} The "\NewTerm{activity}\index{activity (nuclear)}" of a radioactive source is the number of disintegrations per time unit.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Its unit of measurement is the "\NewTerm{Becquerel}\index{Becquerel}" denoted [A]$=$[Bq]. One Becquerel thus corresponding to one disintegration per second.
	\end{tcolorbox}
	The old unit of measurement of radioactivity was the "\NewTerm{Curie}\index{Curie}" denoted [Ci]. The Curie was defined initially as the activity of about one gram of radium, natural element that we find in the soil with uranium. This unit is much larger than the previous one because, by definition $1$ [Ci] is equal to 37 billion disintegrations per second:
	
	The activity is obtained by the time derivative of the stock atoms of a given sample:
	
	The relation, named "\NewTerm{equation of activity}\index{equation of activity}":
	
	thus shows that the activity of a given number of $N$ atoms of a radioactive isotope is proportional to that number and inversely proportional to the half-life of the isotope (by the relation seen above between the radioactive constant and the half-life period).
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	One gram of ${^{225}\mathrm{Ra}}$ contains (the reader must regularly refer to the common international standards for the values of the constants used!):
	
	Therefore the activity of this gram is, knowing experimentally that $T_{1/2}=1,600\;[\text{years}]$, equal to:
	
	\end{tcolorbox}
	By the same reasoning (that we can detailed on reader request as always in this book), we show that the activity over time follows the same exponential law of the number of nuclides:
	
	with:
	
	Experimentally to determine the half-life period of an isotope we proceed as following:
	\begin{enumerate}
		\item We choose a pure as possible sample of an isotope which we wish to determine the half-life period.

		\item 	At the time $t_0$ we measure thanks to a detector for a fixed time interval time $\Delta t$ the number of disintegrations. We then have the number of decays during a time interval at the start of experiment (the unit of measurement is then decays and not the number of disintegrations per second).

		\item Then, for each consecutive $\Delta t$ (time interval is fixed) we measure the number of disintegrations during this time interval. This gives us a series of measures the number of decays observed for $\Delta t_1, \Delta t_2, \Delta t_3,\ldots$.
		
		\item To the set of all decay measurements, we remove the laboratory background noise (as the whole environment is radioactive).
	\end{enumerate}
	As:
	
	By taking the neperian logarithm we get:
	
	Thus:
	
	So this is the equation of a staright line of slope $-\lambda$ and intercept $\ln(A(t_0)\Delta t)$. Thus, the decay constant is immediately measured and we quickly deduced the period of half-life using the relation proved earlier above:
	
	
	\pagebreak
	\paragraph{Carbon-14 dating (radiocarbon dating)}\mbox{}\\\\\
	Radiocarbon dating (also referred to as carbon dating or carbon-14 dating) is a method for determining the age of an object containing organic material by using the properties of radiocarbon $^{14}\mathrm{C}$, a radioactive isotope of carbon.
	
	The method was developed by Willard Libby in the late 1940s and soon became a standard tool for archaeologists. Libby received the Nobel Prize for his work in 1960. The radiocarbon dating method is based on the fact that radiocarbon is constantly being created in the atmosphere by the interaction of cosmic rays with atmospheric nitrogen. The resulting radiocarbon combines with atmospheric oxygen to form radioactive carbon dioxide, which is incorporated into plants by photosynthesis; animals then acquire $^{14}\mathrm{C}$ by eating the plants. When the animal or plant dies, it stops exchanging carbon with its environment, and from that point onwards the amount of $^{14}\mathrm{C}$ it contains begins to decrease as the $^{14}\mathrm{C}$ undergoes radioactive decay. Measuring the amount of $^{14}\mathrm{C}$ in a sample from a dead plant or animal such as piece of wood or a fragment of bone provides information that can be used to calculate when the animal or plant died. The older a sample is, the less $^{14}\mathrm{C}$ there is to be detected, and because the half-life of $^{14}\mathrm{C}$ (the period of time after which half of a given sample will have decayed) is about $5,730$ years, the oldest dates that can be reliably measured by radiocarbon dating are around $50,000$ years ago, although special preparation methods occasionally permit dating of older samples.

	The idea behind radiocarbon dating is straightforward, but years of work were required to develop the technique to the point where accurate dates could be obtained. Research has been ongoing since the 1960s to determine what the proportion of $^{14}\mathrm{C}$ in the atmosphere has been over the past fifty thousand years. The resulting data, in the form of a calibration curve, is now used to convert a given measurement of radiocarbon in a sample into an estimate of the sample's calendar age. Other corrections must be made to account for the proportion of $^{14}\mathrm{C}$ in different types of organisms (fractionation), and the varying levels of $^{14}\mathrm{C}$ throughout the biosphere (reservoir effects). Additional complications come from the burning of fossil fuels such as coal and oil, and from the above-ground nuclear tests done in the 1950s and 1960s. Because the time it takes to convert biological materials to fossil fuels is substantially longer than the time it takes for its $^{14}\mathrm{C}$ to decay below detectable levels, they contain almost no $^{14}\mathrm{C}$, and as a result there was a noticeable drop in the proportion of $^{14}\mathrm{C}$ in the atmosphere beginning in the late 19th century. Conversely, nuclear testing increased the amount of $^{14}\mathrm{C}$ in the atmosphere, which attained a maximum in 1963 of almost twice what it had been before the testing began.
	
	Natural carbon has two stable isotopes: the $^{12}\mathrm{C}$ ($98.892\%$) and $^{13}\mathrm{C}$ ($1.108\%$). So there is no $^{14}C$ in the natural carbon. The latter is produced in the upper atmosphere through the action of cosmic neutrons on the $^{13}\mathrm{C}$ . We speak then of "\NewTerm{neutron capture}\index{neutron capture}" (see further below) or "\NewTerm{activation $^{13}\mathrm{C}(n,\gamma) ^{14}\mathrm{C}$}" following:
	
	where $n$ represents a neutron and $p$ represents a proton. 
	
	Thus, continuously $^{14} \mathrm{C}$ is produced in the upper atmosphere and decays naturally with a period of $5,730$ years. We easily imagine that the concentration in $^{14}\mathrm{C}$ is balanced when the production rate is equal to the disappearing rate due to radioactive decay process (otherwise they will no longer be any $^{14}\mathrm{C}$ everywhere today) following:
	
	By emitting a beta particle (an electron, $e^{-}$) and an electron antineutrino ($\nu_e$), one of the neutrons in the $^{14}\mathrm{C}$ nucleus changes to a proton and the $^{14}\mathrm{C}$  nucleus reverts to the stable (non-radioactive) isotope $^{14}\mathrm{N}$.
	
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=0.6]{img/atomistic/carbon_dating.jpg}
		\end{center}	
		\caption{Idea behind Carbon dating (source: ?)}
	\end{figure}
	
	About $2.5$ atoms of carbon $^{14}\mathrm{C}$ are forme every second by square centimeter on the Earth's surface (this value is however dependent on many factors but with negligible variations over the very long term - you can find entire books on this subject), the positive contribution to the number of atoms of  $^{14}\mathrm{C}$ is about:
	
	$R$ being the Earth's radius.
	
	Or in mass flow it represents:
	
	The desintegration rate is supposed equal to the rate of radioactive production, that is to say:
	
	As the global quantity:
	
	As the disintegration rate is equal to:
	
	We conclude that there are $N\cong 3.32\cdot 10^{30}$ atoms of $^{14}\mathrm{C}$. The reader can calculate by multiplying by the mass value of the  $^{14}\mathrm{C}$ atom ($2.34\cdot 10^{-26}$ [kg]) that it makes quite a significant mass in the atmosphere.
	
	As we already said it, this radioisotope is found in the chemical form $\mathrm{CO}_2$ and penetrates through photosynthesis and metabolism in plants and animals. On the death of the plant or animal, the quantity of $^{14}\mathrm{C}$ freezes and starts to decrease through radioactive decay over the ages:
	
	Radiocarbon dating is therefore a simple comparison between the concentration in  $^{14}\mathrm{C}$ of living matter and dead matter of two same objects/living entity. In fact, we determine the specific activities:
	
	where for objects "living entity" can be translate in "nowadays comparable element".
	
	That is to say the "\NewTerm{radiocarbon age}\index{radiocarbon age}" is given by:
	
	with for $^{14}\mathrm{C}$, $\dfrac{T_{1/2}}{\ln(2)}\cong 8267$.
	
	Therefore the latter relation is often written:
	
	and even more shortly:
	
	where $F_m$ is the mass fraction notation.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Measurement of radiocarbon was originally done by beta-counting devices, which counted the amount of beta radiation emitted by decaying $^14C$ atoms in a sample. More recently, accelerator mass spectrometry has become the method of choice; it counts all the $^{14}\mathrm{C}$ atoms in the sample and not just the few that happen to decay during the measurements; it can therefore be used with much smaller samples (as small as individual plant seeds), and gives results much more quickly.
	\end{tcolorbox}
	The reader must keep in mind that two hypothesis (assumptions) have been implicitly done for the calculations above:
	\begin{enumerate}
		\item[H1.] The amounts of parent and daughter isotopes at the beginning, when the rock formed, must know (the initial conditions) or their incertituded impacted on the resulting calculations results.

		\item[H2.] All daughter atoms measured today must only have been derived by in situ radioactive decay of parent atoms (a closed system) in the opposite case the external sources must be taken into account in the incertitude value of the resulting calculations results.
		
		\item[H3.] The radioisotope decay rates are constant and if not the variations must be taken into account in the incertitude value of the resulting calculations results.
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The oldest dated rocks on Earth, as an aggregate of minerals that have not been subsequently broken down by erosion or melted, are more than $4$ billion years old, formed during the Hadean Eon of Earth's geological history. Such rocks are exposed on the Earth's surface in very few places. (Some meteorites, such as the ALH84001 Mars meteorite found in the Allan Hills of Antarctica, are older but they were not formed on Earth). Some of the oldest surface rock can be found in the Canadian Shield, Australia, Africa and in a few other old regions around the world. The ages of these felsic rocks are generally between $2.5$ and $3.8$ billion years. The approximate ages have a margin of error of millions of years. In 1999, the oldest known rock on Earth was dated to $4.031 \pm 0.003$ billion years, and is part of the Acasta Gneiss of the Slave craton in northwestern Canada.
	\end{tcolorbox}	
	
	\pagebreak
	\paragraph{Radioactive Decay chain}\mbox{}\\\\\
	 \textbf{Definition (\#\mydef):} A "\NewTerm{decay chain}\index{decay chain}" refers to the radioactive decay of different discrete radioactive decay products as a chained series of transformations. They are also known as "\NewTerm{radioactive cascades}\index{radioactive cascades}". Most radioisotopes do not decay directly to a stable state, but rather undergo a series of decays until eventually a stable isotope is reached.

	Decay stages are referred to by their relationship to previous or subsequent stages. A "\NewTerm{parent isotope} \index{parent isotope}" is one that undergoes decay to form a "\NewTerm{daughter isotope}\index{daughter isotope}". One example of this is uranium (atomic number 92) decaying into thorium (atomic number 90). The daughter isotope may be stable or it may decay to form a daughter isotope of its own. The daughter of a daughter isotope is sometimes named a "granddaughter isotope".
	
	\begin{figure}[H]
		\begin{center}
		\includegraphics{img/atomistic/uranium_decay_series.jpg}
		\end{center}	
		\caption{Uranium decay series (source: Wikipedia)}
	\end{figure}
	
	In general we denote a radioactive cascade by:
	
	where the small symbol $^{*}$ denotes a given radioactive isotope, $X_n$ being the final stable isotope of the radioactive of the mother element $X_1^{*}$ (the elements between all being unstable nuclides).
	
	\subsubsection{Two level radioactive cascade}
	Let us consider the two nuclide situation (we will not focus on higher order level excepted on request of many readers). 

	Let us suppose that at the beginning of time, the first descendant exists only negligible quantity such that we have for initial conditions (IC) at $t=0$:
	
	The variation of the parent nuclide (1) is given by a negative contribution: the disintegration of (1).

	We have:
	
	having for solution taking into account the initial conditions:
	
	
	The variation of the descendant member (2), that is to say the daughter of (1), is given by a positive contribution (the atoms of (1) disintegrated) and one negative, the disintegration of (2). Then:
	
	So we have to solve this differential equation (\SeeChapter{see section of Differential and Integral Calculus}).
	
	We have for homogeneous solution (characteristic equation):
	
	We derive from the homogeneous equation the special (homogenous) solution:
	
	with the letter $h$ to signify that this is the "homogeneous" solution.

	Let us now determine the particular solution of:
	
	The idea to solve this is to put:
	
	with the letter $p$ for "particular" (implicitly: "special solution"). Substituting we find:
	
	Because if we had $\lambda_p=\lambda_2$ we would have a zero equality which is absurd and we have therefore:
	
	from which we derive that:
	
	Finally the general solution is the sum of the homogeneous solution and the particular, therefore:
	
	Let us apply the initial conditions:
	
	Finally we have:
	
	We will leave to the reader to plot the functions:
	
	to see how it looks like if he feels the need.
	
	$N_2(t)$ being zero for $t=0$ and for $t=+\infty$, it must pass, as the for activity $A_2(t)$, through a maximum. Given $t^{*}$ the time when the maximum is observed, we have:
	
	from where:
	
	The knowledge of $t^{*}$ is important especially in nuclear medicine where we want to administer the product $1$ for radio-diagnostic purposes  and minimize adverse effects of the daughter(s) element(s) of $1$. We then choose elements such that the time $t^{*}$ is greater than the biological elimination time (body's natural elimination pathways) of its daughter(s9 elements(s). We will return on this subject in a few paragraphs after studying three typical scenarios of radioactive filiation:
	
	\paragraph{Secular equilibrium}\mbox{}\\\\\
	This type of relation between mother and daughter activities occurs when the mother nucleus half-life is much greater than that of the daughter nucleus half-life. In other words, the radioactive decay of the mother nucleus is negligible and the activity of the descendent tends toward that of the parent.

	We then have in this particular case:
	
	So we have for the activity using the previously proved relation:
	
	Therefore:
	
	We also see that in the case where $\lambda_1\ll\lambda_2$ and $t+\rightarrow$, we have:
	
	in other words, the mother and daughter activities, become equivalent after a large enough time. For example, after a period of a half-life of the daughter isotope, we already have the daughter activity that is $50\%$ that of the parent activity:
	
	If we have the case where the following approximation is acceptable:
	
	we will have:
	
	
	\pagebreak
	\paragraph{Transient equilibrium}\mbox{}\\\\\
	This type of relation between mother and daughter activity occurs when the half-life of the mother nucleus is greater than that of the daughter nucleus (but not much much greater in contrast to the secular equilibrium). In other words, the decay radioactivity of the mother nucleus and the activity of the descendants are equal to a constant factor (in other words, their radioactive decay curves are parallel after a sufficiently long time).

	We then in this particular case:
	
	So we have for the activity using the previously prove relation:
	
	After a long enough time, we get:
	
	where we see that the factor:
	
	is greater than unity. So after a sufficiently long time, not only the activity of the daughter isotope is parallel to that of the mother but in addition it is superior to that latter.
	
	\paragraph{Nonequilibrium}\mbox{}\\\\\
	Here the half life time of the daughter element is greater than that of the parent element. In other words we have the hypothesis:
	
	This implies for recall that the probability of decay of the parent element $1$ is greater than that of the daughter element $2$ which is logical here.

	The activity of the daughter isotope in the sample then increases according to the relation proved just above:
	
	Finally, after a sufficiently long time, only the activity of the daughter element will remain, since the activity of the parent element disappears at a higher rate following:
	
	After a time $t_{\max}$, the activity of the daugther element reach a maximum value for:
	
	Therefore:
	
	This simplifies to:
	
	It then comes immediately the result already proved in the example earlier above:
	
	Finally, let us consider the extreme case of the situation of non-equilibrium consisting to consider the case where:
	
	In other words the daughter element is not radioactive. We then fall back on the classic case:
	
	
	\subsubsection{Radioactive phenomena}
	When we "weigh" a nucleus, we experimentally observe a very important fact !: its mass is less than the sum of the masses of its constituents. This difference is named the "\NewTerm{mass defect}\index{mass defect}" and is relatively well determined with simplistic theoretical models.

	The mass defect is given by construction by:
	
	with $m_\text{nucleus}$ being the mass of the nucleus in its fundamental state, $m_p$ the proton mass and $m_n$  the neutron mass.

	The mass of a set of linked nucleons is less than the sum of the masses of individual nucleons (far enough of each other to not interact). We get from Special Relativity (see section of the same name):
	
	where $E_\text{bound}^\text{nucleons}$ is the binding energy of the nucleons that compose the nucleus ($> 0$).
	
	$\Delta m$ is therefore positive for all the elements (emission of energy and therefore of mass to the outer system). If this were not the case, the nucleons have no reason to come together naturally to form stable nuclei (or more stable ...).
	
	Given $\bar{B}=f(A,Z)$ the average energy by nucleon of a given atom. We have then:
	
	which is by convention a positive value!

	Notice that the mass of the nucleus is linked to the mass of the atom by:
	
	Similarly, the nucleus mass added to the weight of its isolated electrons is higher than that of the nucleus surrounded by its linked electrons. Let us notice that the electronic binding energy can often neglected relatively to that of the nucleus and it is a rule we will adopt throughout this section.

	The energy released during the nuclear fusion, that is to say during the constitution of the atom from its constituents, also named "\NewTerm{binding energy}\index{binding energy}" (name that often makes  interpretations problems to young students) because it is that energy must be given if we want at the opposite, separate the elements. We must never forget that behind the term "binding energy", so there is the energy variation between individual elements and combined elements of an atomic element.
	
	The general practical expression of the average energy per nucleon of a given atom expressed in atomic mass units is:
	
	A numerical application for Helium ($\alpha$ particle) gives ($Z=N=2$, $A=4$) we get $\bar{B}\cong 7.07$ [MeV/nucleon].
	
	The principles of nuclear energy production from fission or fusion result of the shape of the average energy per nucleon according to $A$.

	We have in reality the following curve linking the average energy per nucleon (that is to say, the average energy variation between the nucleon alone and accompanied...) and the number of nucleons named "\NewTerm{Aston curve}\index{Aston curve}":
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/aston_curve.jpg}
		\caption{Aston Curve}
	\end{figure}
	where we see that from the Iron (element which is therefore the most "glued" and the most stable in energy terms because having the highest average bonding energy) the average energy decreases again. This decrease being due to the fact that from about $70$ nucleons it seems that appear an electrostatic force inside the nucleus that begins to take over another force that rules in the very small nuclei (this force will be named later the "\NewTerm{strong force}\index{stron force}" or "\NewTerm{strong interaction}\index{strong interaction}").

	By the way, what is really important to notice in the chart above is that there is a flexion point and it is this one that makes it possible to obtain energy both with the fusion, only with nuclear fission! We also see that the variation is much greater on the left than on the right, hence the fact that fusion releases much more energies.

	Of all radioactivity phenomena, we can distinguish $8$ of them, some of them are named "secondary" because they are only the possible side effects of the first $6$. Some of these phenomena are caused by man, others are natural and others are purely probabilistic:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.19]{img/atomistic/radio_decay_types_2.jpg}
		\caption{This table summarizes the type, nuclear equation, representation, and any changes in the mass or atomic numbers for various types of decay (source: chemwiki.ucdavis.edu)}
	\end{figure}
	Here is a diagram representing at the top the "\NewTerm{valley of stability}\index{valley of stability}" of atoms and isotopes and down the same valley but highlighting the type of disintegration:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/atomistic/stability_valley.jpg}
		\caption{Nuclear Stability Valley (source: ?)}
	\end{figure}
	Let us see then the types of disintegration or modifications of the structure of the atom / nucleus which are possible in this details:
	
	\paragraph{Nuclear Fusion (1)}\mbox{}\\\\\
	If we assemble two light nuclei $_{Z_X}^{A_x}\mathrm{X}$ and $_{Z_Y}^{A_Y}\mathrm{Y}$ (with $A_x \le A_y$) to form a "heavy" atom $_{Z_Z}^{A_Z}\mathrm{Z}$, then in accordance with the left part of the Aston curve seen above, we increase the mass defect since the average energy per nucleon increases. Indeed:
	\begin{itemize}
		\item The energy of $\mathrm{X}$ is equal to:
		
	
		\item The energy of $\mathrm{Y}$ is equal to:
		
		
		\item Then energy of $\mathrm{Z}$ is equal to:
		
	\end{itemize}
	As:
	
	then:
	
	is strictly positive.
	
	Nuclear fusion is almost exclusively caused by man (on Earth at least... because the stars do it alone with the help of gravity). The probability of observing a natural nuclear fusion under normal conditions of temperature and pressure is so low that it is unnecessary to speak of it at least to this day...
	
	The three most common method for achieving fusion are so far:
	\begin{itemize}
		\item Thermonuclear fusion (heated gas under the form of a plasma in analogy with what happens in Stars)
		\item Inertial confinement fusion (compression of a fuel target with LASER)
		\item Inertial electrostatic confinement (use of electrif field to heat ions to fusion conditions)
	\end{itemize}
	For more information about plasma the reader can reefer to the corresponding section in this books.
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/atomistic/jet.jpg}
		\caption{The Joint European Torus (JET) tokamak fusion detector (source: EUROfusion)}
	\end{figure}
	
	\paragraph{Nuclear Fission (2)}\mbox{}\\\\\
	Similarly, if we break with adequate tools (often with neutrons because to approach the nucleus and overcome its electrostatic repulsion it is the appropriate way... and this is the method used by nuclear power plant and nuclear bombs) an heavy atom $_{Z_X}^{A_X}\mathrm{X}$ in two light atoms $_{Z_Y}^{A_Y}\mathrm{Y}$ and $_{Z_Z}^{A_Z}\mathrm{Z}$ (with $A_Y\le A_Z$) we also increase the mass defect and the energy gained is therefore equal to:
	
	That it is in the case of fission or fusion, the energy released is then divided into the kinetic energy of the fission products, the neutrons emitted and finally the various radiations.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	An atom is said to be "\NewTerm{fissible}\index{fissible}" (or sometimes "fissionable") when fast neutrons are needed to produce fission and "\NewTerm{fissile}\index{fissile}" when it is enough to have slow neutrons for fission (which is more rare).
	\end{tcolorbox}
	Nuclear energy is by far one of the most concentrated form of energy, since $1$ kilogram of natural Uranium supplies a heat quantity of $100,000$ [kWh] in a current power plant, while $1$ kilogram of coal supplies by burning only $8$ [kWh]. For this reason, only a relatively small amount of nuclear fuel is used for electricity production: a nuclear power plant with an electrical capacity of $1,000$ [MW] consumes $27$ tons of enriched Uranium annually, one quarter of its load, whereas a thermal power plant of the same power consumes $1,500,000$ tons of oil by year... For comparison in the Sun, $1$ kilogram of hydrogen produced, by nuclear reactions transforming it into helium, $180$ million kWh!!!! But beware, industrially we know how to extract only a small part of the nuclear energy stored in the material. Of the $27$ tons of enriched Uranium consumed in one year by a power plant, only a small amount of nucleus was really consumed (hence the economic need to reprocess the Uranium after use).

	We quickly realize that the calorific value of fission is gigantic compared to that of fossil energies. An estimate gives a ratio of energy released per atom of $50,000$ million !!! On the other hand, the risk ratio in terms of exploitation and management of waste and impact on the environment is of the same order but in the opposite direction. That mankind finds either another way of producing electricity, or changes its habits of consumption.

	For information in Switzerland, there are only $5$ at this nuclear power plants (at the beginning of the 21st century) for a population of almost $7$ million individuals in year 2000:
	 \begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/atomistic/swiss_nuclear_plants.jpg}
		\caption{Nuclear power plants in Switzerland at the end of the 20th century (source: Swiss confederation)}
	\end{figure}
	This little introduction being made, let us return to the purely mathematical part!

	In the case of spontaneous (or natural) fission we have the emission of two fission products and of $w$ neutrons. What we we denote by:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The isotope of Carbon 15 by spontaneous fission gives an isotope of Nitrogen with the emission of an electron and an antineutrino:
	
	\end{tcolorbox}
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/atomistic/sandia_core_reactor.jpg}
		\caption[]{Sandia National Laboratories' Annular Core Research Reactor (we can the the bubbles of the evaporated water on the picture)}
	\end{figure}
	
	\paragraph{Alpha Disintegration (3)}\mbox{}\\\\\
	\textbf{Definition (\#\mydef):} When a heavy nucleus contains too many protons and neutrons (like Uranium 238 for example), it will empty its nucleon overflow by emitting an alpha particle (Helium nucleus composed of $2$ protons and $2$ neutrons) and the final system which will be a new nucleus will have a smaller and possibly stable mass. This mode of disintegration is named "\NewTerm{alpha radioactivity}\index{alpha radioactivity}".
	
	The probability of disintegration is governed by the mechanism of penetration barrier ("\NewTerm{Tunnel effect}\index{Tunnel effect}") as we will prove it a little further after a small introduction.

	The radioactive decay according to the alpha radioactivity can be schematized as (with some small variations depending on the case):
	
	where:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Radon is found in significant concentrations in underground mines and sometimes in some caves (in some mining or granitic areas):
	
	where Polonium is one of the main factors of induction of so-named "radon-induced lung cancers".
	\end{tcolorbox}
	The energy released during the transmutation is calculated thanks to the mass default:
	
	with $M_X$ being the mass of the initial nucleus, $M_Y$ the mass of the final nucleus and $M_\text{He}$ the mass of the Helium nucleus.

	By neglecting the binding energy of electrons, we have:
	
	Finally:
	
	This expression shows that the energy of $\alpha$ particle is well defined for given initial and final nuclei. In fact, we actually observe a discrete energy spectrum. We conclude that these emissions lead the nucleus to intermediate energy levels corresponding to excited states of the final nucleus. We can explain these observations by a layered nuclear structure!!! The de-excitation of the final is done by emission of $\gamma$ photons.

	The conservation of energy requires that the energy of the $\alpha$ disintegration is distributed between the kinetic energy of the two residual products:
	
	The conservation of the linear momentum gives us:
	
	and therefore:
	
	which we replace in the energy conservation equation:
	
	and we derive that the energy of the $\alpha$ particle is equal to:
	
	since the masses of the nucleus and the $\alpha$ particle are approximately proportional to their mass number, thus $A$ and $4$ respectively.
	
	Let us see now the details of the mechanism of $\alpha$ disintegration with an academic approach, simplified to the extreme and therefore approximative (but sufficient nevertheless). For this approach, we will use the developments on the tunnel effect that we have carried out in the section of Wave Quantum Physics.

	For nuclei having a nucleon number becoming too large, the Coulomb repulsion between protons takes significant values in comparison to the strong interaction which ensures the cohesion of the nucleus. We then observe a phenomenon of saturation, which gives rise to the $\alpha$ disintegration which is a special case of a spontaneous fission.

	George Gamow proposed a theoretical explanation for this phenomenon in 1928. He assumes that the particle $\alpha$ preexists in the nucleus and bumps on the potential walls. It then has a non-zero probability of crossing the potential barrier of the nucleus by tunneling.

	If by thought we disconnect Coulomb interactions, such an $\alpha$ particle is linked to the rest of the nucleus by a nuclear potential of short range equation and depth corresponding to a potential energy that we will determine.

	Schematically in the case of Uranium 238 the situation is considered as follows:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/tunelling_effect.jpg}
		\caption{Gamow's idea of Tunelling effect (source: Pour la Science)}
	\end{figure}
	In Classical Physics we would represent the $\alpha$ emission as the leak of the nucleus from the nucleus. This representation is not valid because it implies that the $\alpha$ particle is undergoing the electrostatic repulsion of the residual core of Thorium 234 would only be removed with an energy of about $25$ [MeV]. However, the low experimentally observed value (of only $4.2$ [MeV]) is found only with the tool of Quantum Physics.
	
	Let's move on to the mathematical part:
	
	Let us connect the Coulomb repulsion between the $\alpha$ particle of electric charge $+2e$ (two protons and two neutrons) and the rest of the nucleus, then of charge $+(Z-2)e$ outside the nuclear potential well.

	We then get the expression of the potential energy (\SeeChapter{see section Electrostatics}):
	
	Where $r$ is the distance between the center of the nucleus and the $\alpha$ particle. The potential energy therefore decreases with distance since the force is repulsive.

	Now, let's have a qualitative approach to the phenomenon. We put that the probability $T$ of tunelling as being proportional, according to our results in the section of Wave Quantum Quantum Physics, to:

	
	knowing that it is, following the approximations we made during the proof, of a lower limit.

	If we model the potential barrier of the core by a non-rectangular profile as presented below:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/irregular_tunelling_barrier.jpg}
		\caption{Non-rectangular barrier profile of the nucleus}
	\end{figure}
	where we replace the real profile of the curve by a series of barriers of thickness $\Delta x$ and where the potential equals $E_\text{pot}$ at the point $\xi_i$

	The probability of passing a barrier will therefore be proportional to:
	
	and we know (\SeeChapter{see section Probabilities}) that the probability of passing one of the barriers is an independent event (mutually exclusive). We can therefore multiply the probabilities such that:
	
	And passing to the limit it comes:
	
	and if $x$ is assimilated to a radius of a spherical symmetry configuration:
	
	In the case of a $\alpha$ nucleus, the potential barrier ranges from $r_1$ where it begins to $r_2$ value where the potential barrier is considered as negligible.

	But, the potential energy of the $\alpha$ at any remote point $r$ from the outside of the border of the nucleus of the radioactive atom will be equal, as we have seen a earlier before, to:
	
	We thus have for $r_1\leq r \leq r_2$:
	
	To determine $E_\text{tot}$ of the emitted $\alpha$ nucleus, it is necessary to know that its total energy is supposed preserved in this simplified model. It is therefore the same as before it passes through the nuclear potential barrier when $r_1\leq r$, during, and after $r_2$.

	Moreover, in this model, kinetic energy is also assumed constant when $r\leq r_2$. In other words, since the $\alpha$ nucleus pre-exists in the nucleus of the radioactive atom, it already has the final velocity that it will have at the point of crossing the barrier of the nuclear potential ...

	So under all these very simplifying assumptions... if we know how to determine the total energy of the $\alpha$ nucleus in $r_2$ (for example), at the output of the barrier, we have its total energy during the whole crossing phenomenon of the barrier.

	Conversely, its total energy required to exit in $r_2$ of the potential barrier by tunnel effect starting from the nucleus of the radioactive atom (and then going away at infinity and gaining kinetic energy and losing all its potential Coulomb energy) is the same assuming that the total energy obtained by calculating the work of the force which from an infinite distance of the nucleus of the radioactive atom would make the $\alpha$ nucleus return inside the nucleus of the radioactive atom at the aforementioned velocity to the minimum exit point $r_2$ (minimum exit radius taken as constant because very distant in orders of magnitude with respect to the nucleus of the radioactive atom).

	This then corresponds to the potential energy difference between a point at infinity and an $r_2$. And since the potential energy is zero at infinity for a repulsive system, there remains only the term:
	
	and finally:
	
	still valid only for $r_1\le r \le r_2$ (it is as if during the crossing of the barrier, the $\alpha$nucleus restored kinetic energy to the vacuum as it approach of the point $r_2$, that said, in Quantum Physics one we cannot use the Classical Mechanics interpretation...).

	Now, very often in laboratories, $r_2$ is expressed as a constant sufficiently far from the nucleus of the radioactive atom. It is then relatively natural (even if it is do-it-yourself physicist way of doing maths...) to take $r$ as an integration variable such as:
	
	and it is traditional to put:
	
	which brings us to:
	
	Let us now do the following change of variables (the derivation of the $\cos^2(u)$ is detailed in the section of of Differential and Integral Calculus):
	
	hence:
	
	and by putting:
	
	The integral:
	
	becomes:
	
	Concerning the terminals we have for recall:
	
	So if $r$ is is equal to $r_1$ we write the bound as $u_0$ and if $r$ equals $r_2$ then:
	
	Therefore:
	
	We have seen in the section of Differential and Integral Calculus:
	
	Therefore:
	
	Then:
	
	What makes:
	
	But, we also have (\SeeChapter{see section Trigonometry}):
	
	Therefore:
	
	Let us recall again that:
	
	But, $r_2 \gg r_1$, therefore, $r_1/r_2\cong 0$.
	
	If we develop in Maclaurin series (\SeeChapter{see section Sequences and Series}) to the first order:
	
	Therefore:
	
	We then have:
	
	If we take the Maclaurin development to the first order:
	
	Therefore:
	
	So all this to write finally:
	
	Relation in which we can put again the coefficient of the exponential that we had determined in the section of Wave Quantum Quantum Physics. It is the exponential factor in the above relation which explains the great variation of the radioactive periods of the different nuclides, whereas the energy of the particles varies relatively little.
	
	Typically for the Uranium $^{238}\mathrm{U}$ nucleus, we take the values in the tables of the physical and universal constants that are in the previous relation to get a certain value of $T$ (I will omit this numerical application for now since the constant and physic tables are not all of agree between them still in this 21st century...).
	
	Then, in the semi-classical approximation, the $\alpha$ nucleus has, in the potential well, a velocity of the order of:
	
	And it goes back and forth in a nucleus whose radius is of the order of $r_1$. These round trips therefore correspond to a certain number of oscillations per second. Indeed, if we denote by $\tau$ the average duration between of two successive shocks, we then have:
	
	Therefore the frequency is equal to:
	
	Each time it has a probability $T$ to cross the potential barrier. This probability per unit time is thus determined by:
	
	and gives the disintegration constant $\lambda$ of the isotope by $\alpha$ emission with a relatively large error if we make the calculation with the numerical values. Otherwise, the order of magnitude is by contrast exact what is not bad at all! Therefore the simplified academic approach above gives satisfactory results.
	
	\pagebreak
	\paragraph{Beta- Disintegration (4)}\mbox{}\\\\\
	\textbf{Definition (\#\mydef):} When a nucleus is unstable due to an overflow of neutrons (like Carbon $14$ for example) it will not emit neutrons. On the other hand, it will have the ability to change one of its neutrons into a proton. During this transformation, to keep the total electrical charge of the system, an electron will be created. This transformation is named "\NewTerm{$\beta^{-}$ radioactivity}\index{beta- radioactivity}\index{$\beta^{-}$ radioactivity}" (because the electron has a negative charge in this disintegration).
	
	The so-named $\beta^{-}$ disintegration is therefore a characteristic of nuclei with an excess of neutrons. The isotopes concerned become more stable by transforming a neutron into a proton with emission of an electron $\beta^{-}$ and a particle named "\NewTerm{antineutrino}\index{antineutrino}" which we will justify the presence further below.

	We then have for the concerned neutron:
	
	We have put in right exponent the spin of the concerned  particle and in right index the sign of the electric charge of the particle. Thus, we observe that the spin is a conserved quantity, as well as the electric charge.

	We have for the concerned isotope:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The famous case of Radiocarbon dating:
	
	\end{tcolorbox}
	The energy released during the transmutation is calculated by means of the mass default:
	
	By neglecting the binding energy of the electrons, we have:
	
	Caution!!! The $Z$ in the equality of $M_X$ is the same as that found in the expression of $M_Y$ hence the $Z + 1$.

	We have then:
	
	Each pure $\beta^{-}$ disintegration is characterized by a fixed decay energy $Q$. Since the kinetic energy of the nucleus is negligible by its mass compared to that of the combined electron and antineutrino, the released energy $Q$ is shared between the kinetic energies of the $\beta^{-}$ and $\bar{}$. Since the mass of the antineutrino is very far below that of the electron, the kinetic energy of the antineutrino can also be neglected. However, the energy of the $\beta^{-}$ is not fixed and can take any value between $0$ and $Q$. We thus observe an energy spectrum unlike other types of radioactivity (because the electron may have variable kinetic energy).

	The shape of the observed distributions makes it possible to give an average energy value to the $\beta^{-}$ which is around:
	
	The existence of antineutrino was postulated in 1933 ($3$ years after the neutrino that we will see below) by Wolfgang Pauli in order to satisfy the spin conservation. The introduction of such a strange particle was very controversial and not widely accepted (zero charge, non-zero spin, negligible mass) and it continues to pose some problems in the contemporary physics of the 21st century.

	Independently of the "\NewTerm{electron neutrino}\index{electron neutrino}" (denoted usually $\nu_e$) accompanying the particles $\beta^{-}$ and $\beta^{+}$ (the latter having several names "\NewTerm{positron}\index{positron}" or "\NewTerm{positive electron}\index{positive electron}") there exists a neutrino of the meson $\mu$ (muon) and a neutrino of the tau (tauon) that are respectively denoted: $\nu_\mu$ and $\mu_\tau$ not to confuse them. Subsequently, as we will not be confronted with the neutrino of meson or tau neutrinos, we will simply denote the electron neutrino $\nu$ instead of $\nu_e$ or of $\upsilon$.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	At the beginning of its discovery, ther disintegration $\beta^{-}$ was seen as a transmutation of the nucleus ... in small classes, even today, it is seen as the transformation of a neutron into a proton. In contemporary theories, it is seen as a the transformation of a quark $d$ into quark $u$ and it has led physicists to develop the theory of weak interaction to explain its origin.
	\end{tcolorbox}
	Betavoltaic devices, also known as betavoltaic cells, are generators of electric current, in effect a form of battery, which use energy from a radioactive source emitting beta particles (electrons). A common source used is the hydrogen isotope, tritium. Unlike most nuclear power sources, which use nuclear radiation to generate heat, which then is used to generate electricity (thermoelectric and thermionic sources), betavoltaics use a non-thermal conversion process; converting the electron-hole pairs produced by the ionization trail of beta particles traversing a semiconductor.
	
	Betavoltaic power sources (and the related technology of alphavoltaic power sources) are particularly well-suited to low-power electrical applications where long life of the energy source is needed, such as implantable medical devices or military and space applications.
	
	The actual technologie provide a source of continuous nanowatt-to-microwatt battery power that is resistant to a broad range of temperatures and other environmental conditions for extended periods of $20$ or more years.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/atomistic/citylabs_betavoltaic_battery.jpg}
		\caption{City Labs Nanotritium™ Battery}
	\end{figure}
	For example, the amount of electricity is tied with half-life. For example, if $^{63}\mathrm{Ni}$ has half-life of $100$ years this means, that mole of Nickel (63 grams) will produce Avogadro/$2$ electrons during that $100$ years. This means $10^{21}$ electrons per year and $10^{14}$ electrons per second.

	This means up to $0.1$ [mA] or electric current.
	
	The total decay energy of $^{63}\mathrm{Ni}$ is $67$ [keV] and is approximately the maximum energy of electron. But, since the decay also produces neutrino the mean energy of electron is much smaller: $17$ [keV] ($2.732\cdot 10^{-15}$ [J]).  This means that each electron has $17$ kilovolts of electric tension.
	
	So, the power of electricity from one mole of $^{63}\mathrm{Ni}$:
	
	This is not sufficient to provide iPhone 4 peak power consumption, which is about $1.5$ [W]. So atomic batteries can actually not supersede conventional batteries, and serve for years.
	
	Also, it must be noticed that the usage of radioactive batteries definitely require regulations for handling and probably would not be allowed inside a single unit for unrestricted civilian use. We know that when properly used betavoltaics are safe. But what about im-proper use / improper disposal / potential for abuse? At any rate, current perception of nuclear power by general public is not that good, so marketing nuclear batteries will present certain challenge. Also the price of Plutonium or Americanium batteries (that are superior to batteries in almost every way) have cost that limits them to micro power devices (one gram is worth almost $1,000$ dollars...).
	
	\paragraph{Beta+ Disintegration (5)}\mbox{}\\\\\
	\textbf{Definition (\#\mydef):} When a nucleus is unstable due to an overflow of protons it will not emit protons. On the other hand, it will have the ability to change one of its protons into a neutron. During this transformation, to keep the total electrical charge of the system there are two ways for this disintegration to occur: 
	\begin{itemize}
		\item By capture of a negative electron an then we speak of "electronic capture radioactivity" (see further below)

		\item Or by emission of positive and then this transformation is named "\NewTerm{$\beta^{+}$ radioactivity}\index{beta+ radioactivity}\index{$\beta^{+}$ radioactivity}" (because the electron detected has a positive charge in this disintegration).
	\end{itemize}
	Therefore during a $\beta^{+}$ disintegration a proton is dissociated into a neutron, a positive electron ("\NewTerm{positron}\index{positron}") denoted $\beta^{+}$  and of a neutrino (which we will justify the presence a little lower).
	
	This transformation has a ridiculously low probability since the inverse of the emission of an electron and an antineutrino would be the simultaneous capture of these two particles... and such a reunion of circonstances would be a miracle. To overcome this difficulty, the nucleus uses a quantum subterfuge: the emission of a particle is equivalent to the capture of its antiparticle. This joker then offers the aforementioned possibilities to the surplus nucleus in protons. Indeed, to carry out the inverse of the $\beta^{-}$ disintegration, the solution consists for the nucleus to use the conservation of the energy and of the spin by emitting a positron and capturing in the quantum energy of vacuum (\SeeChapter{see section Quantum Field Theory}) an antineutrino and to emit in exchange a neutrino.

	We write this:
	
	or:
	
	The energy released during the transmutation is calculated by means of the mass default:
	
	By neglecting the binding energy of the electrons, we have:
	
	Caution!!! The $Z$ in the equality of $M_X$ is the same as that found in the expression of $M_X$ hence the $Z + 1$.

	We have then:
	
	The $\beta^+$ disintegration can therefore take place only if $Q_{\beta^{+}}\ge 0$, that is to say if:
	
	The mass energy of the electron $E=m_ec^2$ is important because it is the energy of one of the two photons resulting from an annihilation of a positron $\beta^{+}$ with an electron $\beta^{-}$.

	As for the $\beta^{-}$ disintegration, the energy of the $\beta^{+}$ is not fixed and can take any value between $0$ and $Q$. We thus observe an energy spectrum.

	\paragraph{Electronic capture (6)}\mbox{}\\\\\
	\textbf{Definition (\#\mydef):} When a nucleus is unstable because of an overflow of protons with respect to neutrons, we know now that a favorable solution from the point of view of its energy is to transform one of its protons into a neutron, ie to realize the inverse of the $\beta^{-}$ radioactivity. We saw just earlier that a possibility was for the nucleus via the disintegration $\beta^{+}$ to catch an antineutrino of the vacuum and to emit a positron (loss of its electric charge) and a neutrino. But it can also capture an electron from the electronic cloud (neutralizing its electrical charge) instead of emitting a positron, this is why we name this an "\NewTerm{electronic capture}".

	This will most often be an electron of layer $K$. What is written:
	
	The energy released during the transmutation is calculated by means of the mass default:
	
	assuming that the binding energy of the electron of the layer K and that of recoil of the nucleus are negligible.
	
	It is therefore the electron neutrino that carries all the energy, hence the necessity that Wolfgang Pauli had to introduce this new particle (what had horrified him ...!). Since the captured electron occupies a precise energy level in the atom, the neutrinos resulting from the disintegration of an isotope by electronic capture have a determined energy and therefore have a discrete spectrum.
	
	By neglecting the binding energy of electrons, we have:
	
	Therefore:
	
	The disintegration by electronic capture is in competition with the $\beta^{+}$ disintegration only if:
	
	in the case where:
	
	only the disintegration by electronic capture is possible.

	However, the hole left by the absorbed electron requires a rearrangement of the atomic cloud and the emission of radiation.
	
	
	\paragraph{Gamma emission (7)}\mbox{}\\\\\
	\textbf{Definition (\#\mydef):} For the nucleus, the emission of electromagnetic radiation $\gamma$ is a possibility of gaining stability. This emission generally follows a phenomenon of disintegration $\beta^{-}$, $\beta^{+}$, $\alpha$ or electronic capture. We can imagine that, in such types of disintegration, the topology of the nucleons in the nucleus is not ideal and that the rearrangement of the latter will be accompanied by a reduction of energy; The latter emitted in the form of one or more photons.

	So we have a schema ($\beta^{-}$) disintegration:
	
	then:
	
	where the $m$ means "metastable" or "\NewTerm{isomer}\index{isomer}" (this last term is used when the emission of radiation takes place long time after disintegration).
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	"Isomer" means that the nucleus is excited. It will de-energize with a period $T_{1/2}$. Generally the $T_{1/2}$ is extremely small and the photon(s) is (are) emitted immediately after the electron in the case of our example of a $\beta^{-}$ disintegration. We then talk about "metastable state" or "isomer". It should be noted that these radioisotopes are particularly interesting in medical imaging.
	\end{tcolorbox}
	The energy of the $\gamma$ photon is equal to:
	
	Now let us suppose that a proton is confined in a box of width $L = 1.00\cdot 10^{-14}$ [m] (a typical nuclear radius). If we assume that the proton confined in the nucleus can be modeled as a quantum particle in a box, all we need to do is to use te relation proven in the section of Wave Quantum Physics for an infinite potential wall box to find its energies $E_1$ and $E_2$:
	
	Therefore the emitted photo has for energy:
	
	Hence:
	
	This is the typical frequency of a gamma ray emitted by a nucleus. The energy of this photon is about $10$ million times greater than that of a visible light photon!
	
	\paragraph{Internal conversion (8)}\mbox{}\\\\\
	\textbf{Definition (\#\mydef):} The  "\NewTerm{internal conversion (I.C.)}\index{internal conversion}" is a process related to the emission of a $\gamma$ photon. Indeed, it is possible that the energy is transmitted directly to an electron of the electronic cloud, generally of the layer K, which is then ejected from the atom. This electron is named the "\NewTerm{conversion electron}". The place left in the electronic cloud is subsequently filled by an electron of the upper layers and so on. Thus, as in the case of an electronic capture decay process, there is a rearrangement of the electronic cloud characterized by the emission of X-rays characteristic by the element $Y$.

	The energy transmitted is:
	
	with $E_{e^{-}}$ being the kinetic energy of the emitted electron, $E_\gamma$ the photon energy percutting the electron, $E_b$, the binding energy of the electron considered (K, L, M, ...).
	
	The energy of the $\gamma$ photon is transmitted directly to an electron that is ejected. The process is followed by the rearrangement of the electrons (an emission of X-rays will follow). The ejected electron is named "\NewTerm{Auger electron}\index{Auger electron}" and sometimes the whole effect is named the "\NewTerm{Auger effect}\index{Auger effect}".
	
	The emission of a Auger electron is thus a process similar to the internal conversion process (IC), but the electromagnetic radiation does not come from a de-excitation of the nucleus (it is not a $\gamma$ photon) but of an X-ray produced during the rearrangement of the electronic cloud. In a radioactive process, this electronic rearrangement can come either from an electronic capture (EC) or from an internal conversion (IC).

	The ejected Auger electron comes mainly from an internal orbit and its energy is the characteristic energy of the X-ray minus its binding energy. The energy of the Auger electrons is therefore quite small (some [keV]) with respect to a  $\beta^{-}$ particle and these electrons are therefore often reabsorbed inside the source. The process of emission of an Auger electron is favored for elements with low atomic number because of their low energies of electronic connection.

	During a rearrangement of the electronic cloud such as the passage of an electron from the layer L to the layer K, the energy of the X-ray emitted will be equal to $E_\text{K}-E_\text{L}$. Since this energy difference is greater than the binding energy of another electron on layer L, the latter will then be emitted with the kinetic energy:
	
	In turn, the $2$ vacations left on layer L are filled with electrons from the upper layers. Fluorescence and Auger electron are then in competition. It is even possible that several Auger electrons are emitted during the de-excitation of the atom. This is named "\NewTerm{Auger cascade}\index{Auger cascade}" leaving the atom considered highly ionized, which can lead to the Coulomb explosion of the molecule of which it is a part.
	
	To conclude on all these radioactive phenomena, let us indicate the order of magnitude of the radioactive periods of some natural and artificial elements:
	
	
	\subsection{Radiation protection}
	In nuclear physics (and also in "high energy astrophysics"), it is very important to know how the various $\alpha$, $\gamma$, X-ray or neutron radiation interact with matter (roughly uncharged or charged radiation). This allows us to know how their kinetic energy is distributed or dissipated in the material they encounter along their path and to protect themselves in a suitable way.
	
	"\NewTerm{Radiation protection}\index{Radiation protection}", sometimes known as "\NewTerm{radiological protection}\index{radiological protection}", is defined by the International Atomic Energy Agency (IAEA) as "\textit{The protection of people from harmful effects of exposure to ionizing radiation, and the means for achieving this}". The IAEA also states "\textit{The accepted understanding of the term radiation protection is restricted to protection of people. Suggestions to extend the definition to include the protection of non-human species or the protection of the environment are controversial}".

	Ionizing radiation is widely used in industry and medicine, and can present a significant health hazard. It causes microscopic damage to living tissue, which can result in skin burns and radiation sickness at high exposures (known as "tissue" or "deterministic" effects), and statistically elevated risks of cancer at low exposures ("stochastic effects").

	Fundamental to radiation protection is the reduction of expected dose and the measurement of human dose uptake. For radiation protection and dosimetry assessment the International Committee on Radiation Protection (ICRP) and International Commission on Radiation Units and Measurements (ICRU) have published recommendations and data which is used to calculate the biological effects on the human body, and set regulatory and guidance limits.

	Before investing this subject we must prove some relations and study some phenomena related to high energy radiations and behaviors of particles. So let us tackle this first!

	\subsubsection{Bether formula}
	The "\NewTerm{Bethe formula}\index{Bethe formula}" describes the mean energy loss per distance traveled of swift charged particles (protons, alpha particles, atomic ions) traversing matter (or alternatively the stopping power of the material). For electrons the energy loss is slightly different due to their small mass (requiring relativistic corrections) and their indistinguishability, and since they suffer much larger losses by Bremsstrahlung, terms must be added to account for this. Fast charged particles moving through matter interact with the electrons of atoms in the material. The interaction excites or ionizes the atoms, leading to an energy loss of the traveling particle.
	
	The non-relativistic version was found by Hans Bethe in 1930. The Bethe formula is sometimes named "\NewTerm{Bethe-Bloch formula}", but this is quite misleading.
	
	A heavy charged particle having an energy of one or more MeV loses its energy mainly by collisions with the electrons cloud of atoms, electrons which appear to it as quasi-free. The process by which electrons are thus ejected during the passage of an ionizing particle is named "\NewTerm{primary ionization electrons}". An electron can escape if it receives an energy higher than its binding energy.

	The maximum energy transfer $E_{\max}$ that can occur in a non-relativistic and elastic collision (where the system energy is conserved because there is by definition no heat - radiation - dissipation) is calculated simply by using the conservation principle of the linear momentum and energy:

	Given $M$,$v_M$ and $m_e$, $v_e$ be the masses and velocities of the incident particle and the electron respectively. We assume that the electron is immobile in its orbit and that its initial velocity is zero ($v_e=0$). After the shock, we will assume that the incident particle will have transferred all its kinetic energy to the electron and will in turn find itself at rest such as $v_M^{'}=0$.
	
	Let us first consider the conservation of linear momentum:
	
	The conservation of energy also allows us to write:
	
	Hence, after regrouping and simplification:
	
	Or otherwise written:
	
	Then, after dividing the second equation by the first we have:
	
	We then have the system:
	
	we then deduce the expression of the speed after the impact:
	
	relatively to our initial hypotheses ($v_e=0$, $v'_M=0$), we then have:
	
	Let us manipulate this relation a bit:
	
	For a heavy particle, with $M\gg m_e$ we can write:
	
	An ionization can only occur if the $_\text{max}E^e_c$ is at least equal to the ionization threshold of the electron that we will denote by $I_0$ and whose calculation was seen during the study of the Bohr model (\SeeChapter{see section Corpuscular Quantum Physics}).

	The energy of the incident particle must therefore at least be equal to:
	
	Thus, when passing through matter, the electrically charged body of charge $Z\cdot e$ and speed $v_M$ yields its energy in numerous collisions with the electrons of the atoms encountered. The interaction is of the Coulomb type and each time a diffusion occurs. The recoil energy of the electron, assumed to be free, can be calculated accurately. To estimate the loss of energy, we will here approximate that the quantity of linear momentum transferred $\Delta p$ is equal to the product of the interaction force at the distance $r$ multiplied by the time necessary for the projectile to traverse the path $2r$. We have the Coulomb for $F$ given for recall by (\SeeChapter{see section Electrostatics}):
	
	and the linear momentum:
	
	The kinetic energy transferred to an electron of mass $m_e$ will be:
	
	The total energy loss will be obtained by integrating on all the electrons encountered. At the distance range that is between $r$ and $r + \mathrm{d}r$ of the trajectory and on the path of length $\mathrm{d}x$, there are:
	
	electrons, where $N$ is the number of atoms of atomic number $Z'$p er unit volume. The loss of energy per unit of distance is therefore:
	
	The value of $r_{\min}$ is evaluated by noting that this "\NewTerm{impact parameter}\index{impact parameters}" should corresponds in practice to the maximum energy transfer. Using the equations we have proved previously:
	
	With $E_c^M=\dfrac{1}{2}Mv_m^2$, we can get the parameter $r_{\min}$ by:
	
	and we get:
	
	When $r$ becomes very big, the energy transfer is smaller than the mean ionization energy denoted $\bar{I}$ of the electrons and the process is no longer efficient. We must therefore have the following relation:
	
	We derive a value for $r_\text{max}$:
	
	By replacing the values of $r_{\min}$ and $r_{\max}$ of the above relations into the equation:
	
	we get:
	
	A more rigorous quantum treatment would show that the root of the argument of the logarithm should be suppressed by taking into account the relativistic effects as well as the intrinsic properties of the electron (fine structure constant). We would then obtain the following Bethe formula:
	
	where $\beta=v/c$ and $c_k$ is a correction term that depends on energy and of $Z$ when we take into account the complete structure of the nucleus (layered model) of the material.

	Finally, we see that the loss of linear energy is proportional to the atomic number of the incident radiation and the penetrated matter. Thus, to come back on radioprotection..., protections composed of materials with high atomic numbers (high density) will have a high decelerating power and will be advantageous in radiation protection.
	
	\subsubsection{Compton scattering}
	The Compton effect is observed when a photon is scattered inelastically by a charged particle. In fact, the photon is absorbed and then re-emitted by the particle, thus giving up part of its energy. It is this transfer of energy that justifies the inelastic character of the diffusion.

	Thus if the charged particle is an electron, this effect can take place indifferently on an electron of any electronic layer or even on a free electron. The energy of the photon and that of the electron depend on the direction of emission of these particles. Since this effect depends on the number of electrons available per target atom, the Compton diffusion probability increases linearly with the atomic number $Z$ of the absorbent. But since this effect is in competition with the production of an electron-positron pair that we shall see later, the Compton effect is especially important at average energies and at average atomic numbers.

	We have recall that in the section of Special Relativity we have proved that:
	
	And let us recall that we have thus for the momentum of a photon:
	
	and we have also proved in the Special Relativity section that, starting from the total energy, the linear momentum is given by:
	
	Hence the relation, which we shall use further below:
	
	Before the interaction, photon-electron, we have (we roughly consider the electron as being at rest) for the total energy of the system:
	
	and after collision:
	
	Conservation of energy leads us to write:
	
	By considering only the kinetic energies, we neglect that of the electron before the shock:
	
	Either the figure below:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/compton_scattering.jpg}
		\caption[]{Simplified illustration of the Compton effect}
	\end{figure}
	The conservation of the linear momentum give us according to the $x$-axis:
	
	and according to the $y$-axis:
	
	The sum of these two relations squared gives us the total linear momentum:
	
	Then by substituting $p_e^2c^2$:
	
	and as $E_{c,e}=E_1-E_2$:
	
	When the energy of the photon is high enough, that is $E_1\gg m_0c^2$, that of the scattered photon tends to a limit given by (see Hospital rule in the section of Differential and Integral Calculus):
	
	The energy acquired by the Compton electron is finally equal to:
	
	It is interesting to notice that we can not have $E_e=E_1$. Indeed this would suppose that:
	
	and we see well that in fact, whatever the value of $\phi$, we always have $E_2\neq 0$.
	
	The frequency of the scattered photon is lower than that of the incident photon because its energy $E_2$ is always lower and hence its wavelength $\lambda_2$ larger. Therefore:
	
	and since:
	
	We have:
	
	That is:
	
	which is also written using the definition of the Planck constant and the usual trigonometric relations:
	
	We name the factor $\dfrac{h}{m_0c^2}$ the "\NewTerm{Compton wavelength}\index{Compton wavelength}" and its value is equal to:
	
	Notice that if the angle $\phi$ is equal to zero, then the wavelength variation is zero and if the angle $\phi$ is equal to $180^\circ$ ($\pi$ [rad]) then the variation of the incident photon wavelength is twice the length of the Compton's wavelength!
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/atomistic/compton_scattering_final.jpg}
		\caption{Compton scattering (source: ? )}
	\end{figure}
	
	\paragraph{Thomson scattering}\mbox{}\\\\\\
	Thomson scattering is the elastic scattering of electromagnetic radiation by a free charged particle, as described by classical electromagnetism. It is just the low-energy limit of Compton scattering: the particle kinetic energy and photon frequency do not change as a result of the scattering.[1] This limit is valid as long as the photon energy is much less than the mass energy of the particle: $h\nu \ll m_ec^{2}$, or equivalently, if the wavelength of the light is much greater than the Compton wavelength of the particle.
	
	First consider scattering of (classical) electromagnetic waves by free electrons, named "Thomson scattering". The electron experiences a force due to the incident electric field:
	
	The equation of motion for the charged particle is:
	
	We recall that in the dipole approximation (valid since the wavelength of light is large compared to the scatterer) the power emitted per unit solid angle is (\SeeChapter{see section Electrodynamics}):
	
	where $\langle a^2 \rangle$ represents the time-averaged squared acceleration. The equation of motion gives the acceleration directly and we assume that it is equal to the time averaged squared acceleration such that:
	
	This yields, upon rearranging:
	
	The term in the brackets is known as the "\NewTerm{classical electron radius}\index{classical electron radius}":
	
	Now recall that the time-averaged Poynting vector which is the incident energy per unit area per unit time is (\SeeChapter{see section Electrodynamics}):
	
	We can astutely fall back on the "\NewTerm{differential scattering cross section}\index{differential scattering cross section}" define earlier above by using dimensional analysis:
	
	and is the area of the wavefront which delivers the same power as is scattered into a given solid angle $\mathrm{d}\Omega$. So we get after simplification:
	
	The total cross section is obtained by integrating over $\mathrm{d}\Omega$:
	
	But from the section Trigonometry we know that:
	
	Therefore (identically as the integral we did in the Trigonometry section):
	
	Therefore:
		
	This is known as the "\NewTerm{classical Thomson cross section}\index{classical Thomson cross section}".
	
	\subsubsection{Photoelectric effect}
	The photoelectric effect is the ejection of electrons (the named "\NewTerm{photoelectrons}\index{photoelectrons}") from the surface of various metals exposed to radiation energy. This radiation can come from the rearrangement of the nucleus of the atom as well as from an external radiation.

	Moreover, it was by quantitative measurements of the photoelectric effect that Albert Einstein proposed to test the quantum theory of light (Wave Quantum theory) and therefore the theoretical explanation that bring him to receive Nobel prize of Physics.

	Let us first describe the experiment: the emission of electrons by a metal does not contradict the electromagnetic theory of light. If we consider a uniform beam, its energy is uniformly distributed over the entire wave front. The more intense the light is, the greater the amplitudes of the electric and magnetic fields at each point of the wavefront and the greater the energy per second transmitted by the wave is. These fields exert forces on the electrons in the metal and can even tear them from its surface.

	Here is the experience set up:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/atomistic/photoelectric_effect.jpg}
		\caption{Experiment for measuring the photoelectric effect (source: ?)}
	\end{figure}
	If the collecting anode is at a positive potential relative to the emitting cathode, the photoelectrons traverse the tube and constitute the current measured by the ammeter. We then observe experimentally a proportionality between the intensity of the incident beam and the current.
	However, at least three problems persist between the theoretical model and the experimental observation:
	\begin{enumerate}
		\item The wave aspect of light is not suitable for explaining the time required to absorb the extraction energy.

		Indeed, let us consider a $100$ [W] lamp with a $15\%$ luminous efficiency placed at $0.5$ [m] of a potassium-coated plate (chemical symbol of potassium is $\mathrm{K}$) have a minimum extraction energy of $2.25$ [eV] assuming a diameter of $1\cdot 10^{-10}$ [m] for the Potassium atom.
		
		We then have:
		
		The luminous power absorbed by the atom by its half-surface which faces the radiation is then:
		
		The time required for absorption is then:
		
		Which is in contradiction with the experience where we observe that the phenomenon is almost instantaneous (time for light to travel to the metal).
		
		\item If we reverse the poles, the electrons emitted by the metal are pushed back by the negative electrode, but if the reverse voltage is not to big the faster ones can still reach the negative electrode and a current will occur. At a negative potential, specific for each metal, named the equation "\NewTerm{stop potential $U_a$}", all the emitted electrons are pushed back and the current is zero. The maximum kinetic energy of these photoelectrons is then:
		
		However, we see experimentally that this stopping potential is independent of the intensity of the radiation. In the Wave theory, the increase in intensity should increase the number of electrons extracted (regardless of their energy level) and their maximum kinetic energy. A greater intensity implies a greater amplitude of the electric field: $I \propto E^2$. Thus, a bigger electric field should eject the electrons at higher velocities from all layers of the electronic cloud as the intensity increases.
		
		\item When we vary the frequency $v$ of incident light and we measure $U_a$, we observe that the photoelectric effect does not occur if $\nu<\nu_0$ ($\nu_0$ is named the "\NewTerm{frequency threshold}") and this whatever the intensity of the light . What is rather annoying... because in Wave theory, we must always be able to eject electrons whatever the frequency, we just have to increase the intensity.
	\end{enumerate}
	Each problem can be solved by adopting the following point of view:
	\begin{enumerate}
		\item In the Wave theory approach, the source is seen as propagating like a spherical wavefront whose surface density of energy decreases as a $1/r^2$ (\SeeChapter{see section Electrodynamics}). In order to explain the experimental observation, we must see the experiment from a corpuscular point of view where the wavefront is a front of corpuscles whose surface density of photons decreases in $1/r^2$ but where the energy of each photon remains $h\nu$ (according to the Planck-Einstein relation).
		
		\item If we think in terms of photons, when we increase the intensity, we increase the number of photons, but the energy per photon $h\nu$ remains unchanged. Thus, the energy $E_{c,\max}$ that each photon can have does not change. Hence the stopping potential is independent of the field intensity.
		
		\item If we think in terms of photons again, the electrons in the target are retained by the attraction forces, extracting an electron from the surface requires a minimal energy $E_l$ that depends on each material ($E_l$ is also named "\NewTerm{extraction work}" denoted $W_i$ which is of the order of some electronvolts). If the energy of the incident photon $E_i=h\nu$ is greater than $E_l$, an electron can be extracted, but if it is inferior, no electron can be extracted. The energy input $h\nu$ is equal to the kinetic output energy of the electron plus the energy required to extract it from the metal, therefore:
		
		Thus, if the frequency of light is increased, the maximum kinetic energy of the electrons increases linearly. R.A. Millikan made rigorous experiments between 1913 and 1914, the results were matching perfectly with Albert Einstein's theory.
	\end{enumerate}
	The light spreads from place to place as if it were a wave. But light interacts with matter in absorption and emission processes as if it were a particle stream. We already know that this is the  "wave-corpuscle duality" (\SeeChapter{see section Wave Quantum Physics}). Thus, as that latter can found in the mass particles as suggested by the De Broglie hypothesis that we have seen in the section of Wave Quantum Physics, is finally verified also for the light.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/photoelectric_effect_bohr_model.jpg}
		\caption{Principle of the photoelectric effect on the Bohr model of the atom}
	\end{figure}
	A photon of incident energy $E_i$ which interacts with an electron of a target atom can eject this electron from its orbit by communicating to it a kinetic energy equation:
	
	where $E_l$ is the binding energy of the electron ejected from its orbit (this relation is given in the form $T=E-W_i$ in the figure above).
	
	If the energy of the incident photon is lower than the binding energy of the electron of layer K (\SeeChapter{see section Corpuscular Quantum Physics}), the photoelectric effect is done with an electron of the layer L, etc.

In the case where the radiation is absorbed, the atom is said to be "excited", because its energy state is not the minimum state. There follows a "relaxation" (or "de-excitation"): an electron of a higher layer fills the quantum space left vacant by the ejected electron.

	If the transition energy is moderate (ie, if the incident radiation has a moderate energy), relaxation causes the emission of a low-energy (visible or ultra-violet) photon, this is the phenomenon of fluorescence. 
	
	\pagebreak
	\subsubsection{Rutherford scattering}
	Let us now consider the diffusion that a charged particle undergoes when it is subjected to an electrostatic repulsive force inversely proportional to the square of the distance between the moving particle and a fixed point or center of force. This problem is particularly interesting because of its application to atomic and nuclear physics. For example, when a proton, accelerated by a machine such as a cyclotron, passes near a core of the target's material, it is deflected by a force of this type from electrostatic repulsion of the nucleus. This study is referred to as the "\NewTerm{Rutherford scattering}\index{Rutherford scattering}" or "\NewTerm{Coulomb scattering}\index{Coulomb scattering}" and is manly based on the following figure:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/rutherford_scattering.jpg}
		\caption{Rutherford scattering}
	\end{figure}
	Let O be a center of force and a particle thrown against O from a great distance with the velocity $v_0$ (see figure above) moving from right to left and entering the figure above at point $A$. We will choose The axis of $X$ passing through O and parallel to $v_0$. The distance $b$, named the "\NewTerm{impact parameter}", is the distance between the $x$-axis of the abscissa and the point $A$. Assuming that the force between $A$ and O is repulsive and central, the particle will follow the curved trajectory $AMB$. The shape of the curve depends on how the force varies with the distance. If the force is inversely proportional to the square of the distance, that is to say if:
	
	The trajectory is a hyperbola (indeed, the equation and solution are the same as Kepler problem, we just need to flip the sign of the constant $K_e$). Of course (\SeeChapter{see section Electrostatics}):
	
	When the particle is in $A$ its angular moment is $mv_0b$. In any position such that $M$, its angular momentum (\SeeChapter{see section of Classical Mechanics}), is also given by $mr^2\dfrac{\mathrm{d}\theta}{\mathrm{d}t}$. Since the angular momentum must remain constant since the force is central:
	
	The equation of motion in the O$Y$ direction is obtained by injecting this relation into:
	
	so we can get rid of $r^2$ and write therefore:
	
	To find the deviation of the particle, we must integrate this equation from one end of the trajectory to the other. In $A$ the value of $v_y$ is zero because the initial motion is parallel to the $X$ axis and we also have $\theta=0$. In $B$ we have $v_y=v_0\sin(\phi)$ and $\theta=\phi$ or $\pi-\theta=\pi-\phi$. We notice that in $B$ the speed is again $v_0$ because, by symmetry, the speed lost when the particle approaches $O$ must be regained when it moves away from it. Therefore:
	
	Which give:
	
	thus by using the common trigonometric relations:
	
	and after rearrangement:
	
	Let us recall that (\SeeChapter{see section Trigonometry}) that:
	
	What then gives us:
	
	Either more explicitly:
	
	This relation gives the angle of deviation $\phi$ as a function of the impact parameter $b$:
	
	Of course, in undergraduate cases, $Q = q$ is often put, which simplifies the previous relation slightly, but we then loses in generalization.
	
	This equation is applied to the analysis of the deviation of an electrically charged particle by a nucleus. Let us notice that this result is only valid for a force inversely proportional to the square of the distance. If the force depends on the distance according to another law, the deflection angle satisfies another equation. The deviation experiments are therefore also very useful when we want to determine the law of force involved in the interactions between particles!
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/rutherford_scattering_closest_approach.jpg}
		\caption{Representation of the Rutherford diffusion of alpha nucleus}
	\end{figure}
	In nuclear physics laboratories, diffusion experiments are carried out by accelerating electrons, protons or other particles by means of a cyclotron, a Van de Graaf accelerator or some other similar device, and by observing the angular distribution of the deflected particles.

	It is clear that an incident particle in a surface defined by a radius between $b$ and $b + \mathrm{d}b$ will be respectively included in the solid diffusion angle:
	
	with (\SeeChapter{see section Trigonometry}):
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/solid_diffusion_angle.jpg}
		\caption{Representation of the solid diffusion angle}
	\end{figure}
	The "\NewTerm{cross-section}\index{cross-section}" or "\NewTerm{differential cross-section}\index{differential cross-section}" being defined in Nuclear Physics and Particle physics by:
	
	Starting from (relation proved just earlier above):
	
	and using the following usual derivative proved in the section of Differential and Integral Calculus:
	
	we have:
	
	Hence:
	
	Therefore:
	
	Thus:
	
	We have then, remembering that the member on the left is nothing but the effective section:
	
	and it is customary to take the absolute value to define the "\NewTerm{effective (differential) section of Rutherford (or Coulomb)}\index{effective (differential) section of Rutherford (or Coulomb)}":
	
	We notice several interesting things:
	\begin{enumerate}
		\item For an angle of incidence equal to zero, the effective cross-section diverges (because of the sinus in the denominator)

		\item The effective section decreases according to the square of the kinetic energy of the incident particle

		\item The expression is valid whatever the charges involved (positive or negative)
	\end{enumerate}
	Thanks to the Rutherford / Coulomb scattering, Rutherford was able to determine an approximation of the size of the nucleus of the atom (bombardment of a gold leaf using alpha nuclei) as we did notice it at the beginning of the section of Corpuscular Quantum Physics. The reasoning applied is as follows to determine a lower bound of the radius of the nucleus:
	
	The total energy of a rotating system is the kinetic energy of translation summed with the kinetic energy of rotation, summed to the potential energy. Which gives us:
	
	By denoting by $L$ the angular momentum given by $L=mr^2\dot{\theta}$ (\SeeChapter{see section Classical Mechanics}) we have:
	
	hence:
	
	It therefore follows:
	
	From this it follows that the angle associated with two radial distances $r_1$, $r_2$ is given by:
	
	The figure below shows a process of collision by a central potential $U(r)$. The incident particle has an initial speed:
	
	in $t=-\infty$ with $z(t=-\infty)$ and $y(t=+\infty)$ by symmetry again.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/rutherford_radius_target.jpg}
		\caption[]{Schematic approach for determining the radius of the target}
	\end{figure}
	The angle $\Theta$ is the angle of deflection when the incident particle approaches the diffuser at the minimum distance $r_{\min}.$

	Let us return to our equations where the angular momentum is linked to the impact parameter by the relation $L=mvb$ or written differently:
	
	We can therefore write after simplifications:
	
	Where we put $E\cong E_c$ (the rotation energy and the potential energy are considered negligible with respect to the kinetic energy) and:
	
	The minimum approach distance is thus determined by the largest zero of the denominator:
	
	That is to say (trivial):
	
	So we have:
	
	As we see in this last relation, the incident particle will undergo a frontal collision when $b=0$. Therefore, the value of the maximum approach is:
	
	Rutherford's experiment made it possible to estimate the size of the atomic nucleus. Indeed, the particles that have bounced back on the nucleus with a diffusion angle of $180^\circ$ (we are then talking of "\NewTerm{backscattering}\index{backscattering}") are those that have approached nearest to the latter. Since we have:
	
	with an initial kinetic energy of $7.7$ [MeV], Rutherford found for the radius of the gold atom ($Z = 79$) with alpha particles ($Z = 2$) a value of:
	
	Thus, the nucleus is not punctual but of the order of ten tens of femtometers
	
	\subsubsection{X-rays and Gamma rays}
	The fundamental difference of this type of radiation, with respect to the $\alpha$, $\beta^-$, $\beta^+$ is that it does not carry an electric charge and therefore does not have a coulombic interaction with the electronic cloud of the medium traversed. Consequently, the photon follows a straight path without loss of energy until it encounters a particle on its path (electron, nucleus) where it will undergo an interaction that profoundly alters its state.

	"\NewTerm{Gamma radiation}\index{Gamma radiation}" is a high-energy electromagnetic radiation produced by a nuclear phenomenon, while "\NewTerm{X-rays radiation}\index{X-rays radiation}" are high-energy electromagnetic radiation produced during atomic or molecular phenomena. The photon is the elementary particle that is associated with these electromagnetic waves. The X and gamma photon are therefore of the same nature but of different origins, they therefore have identical properties that depend on their energy.

	Let us recall that:
	
	By crossing mater a photon can interact with:
	\begin{itemize}
		\item One of the electrons of the atom cloud

		\item The nucleus of the atom

		\item  The electric field of charged atomic particles

		\item The meson field of the nucleons (strong interaction)
	\end{itemize}
	The result of the interaction can be schematized as follows:
	\begin{itemize}
		\item The photon is deflected by conserving its energy, then there is "\NewTerm{total diffusion}\index{total diffusion}" of the energy and the process is say to be "\NewTerm{coherent}" (elastic).

		\item The photon is deflected and its energy decreased, then there is "\NewTerm{partial diffusion}\index{total diffusion}" of energy, the other part is absorbed by matter, the process is then say to be "\NewTerm{incoherent}" (inelastic).

		\item The photon disappears, there is "\NewTerm{total absorption}\index{total absorption}" of its energy by matter.
	\end{itemize}
	We can prove that the macroscopic characteristics of these interactions in the framework of a thin and collimated beam lead to an exponential law of attenuation of photonic radiation in matter. This means that for photons there is no finite path (!) as for charged particles; It can never be assured that at a given distance all the photons of a beam have interacted.

	The number of particles interacting with the material obviously depends on the intensity $I$ and the type of material traversed (characterized by the "\NewTerm{linear attenuation coefficient $\mu$}\index{linear attenuation coefficient}") and its thickness $x$.

	We have:
	
	the "$-$" sign being there to show a decrease. We easily solve this differential equation (it is simply the Beer-Lambert law that we have already studied in the section of Geometric Optics):
	
	with $I_0$ being the initial intensity or "\NewTerm{flow rate of fluence}" and $\mu$ being the coefficient of linear attenuation in [cm$^{-1}$] which takes into account all the possible attenuation effects.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Often in the tables, we find the coefficient of attenuation mass $\mu/rho$ expressed in [cm$^2\cdot$g$^{-1}$]. We have then:
	
	\end{tcolorbox}
	In the case of an absorbent containing several homogeneously distributed chemical elements, the attenuation coefficient is equal to:
	
	where $\mu_T$ is the absorption coefficient of the absorbent, $\mu_i$ the absorption coefficient of the element $i$, $\rho_T$ the density of the absorbent, $\rho_i$ the density of the element $i$, $w_i$ being the mass fraction of the element $i$ in the absorbent.

	Let us now take a microscopic approach:
	
	Let a beam of $I$ [particles$\cdot$ cm$^{-2}\cdot$s$^{-1}$] striking perpendicularly the surface of a material of thickness $\mathrm{d}x$ and of atomic density $N$ [atoms$\cdot$cm$^{-3}$]. If we consider the particles striking tje surface $S$, the latter can theoretically encounter $N\cdot S\cdot \mathrm{d}x$ target atoms in this layer. The number of particles interacting will be proportional to the intensity times this number and we will have:
	
	where $\sigma$ is the constant of proportionality, named "\NewTerm{microscopic cross section}\index{microscopic cross section}". Its units are often expressed in [barn]\index{barn} ($1$ [barn]=$10^{-24}$ [cm$^2$]). 
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	\textbf{R1.} The atomic density $N$ is equal to $\rho N_\text{Av.}\cdot M_m^{-1}$ where $\rho $is the density in [g$\cdot$ cm$^{-3}$] of the target, $N_\text{Av.}$ the Avogadro constant ($6.022\cdot 10^{23}$ [atoms$\cdot$mole$^{-1}$]) and $M_m$ is the molar mass of the target expressed in [g$\cdot$ mole$^{-1}$].\\

	\textbf{R2.} If we admit that the diffusion centers are the electrons and not the target atoms, then we have to replace $N$ by $N_e=N\cdot Z$.
	\end{tcolorbox}
	From where we get:
	
	By identifying the macro and microscopic aspect, we see that $\mu$ plays the same role as $ZN_\text{Av.}\rho\sigma/M_m$ and that we find that the effective section can be written as:
	
	and in the hypothesis that the electron constitutes a "\NewTerm{sphere of action}" having a frontal surface $\pi r_e^2$, where $r_e$is the radius of the sphere of action then:
	
	and we have:
	
	By definition, we name the "\NewTerm{Half-Attenuation Elbow (HAE)}"  the thickness of the material that divides the fluence rate $I_0$ by a factor of two. Therefore:
	
	In radioprotection, we sometimes use the notion of attenuation layer to the tenth TVL (Tenth Value Layer) given by:
	
	We sometimes also use the "\NewTerm{relaxation length}", which represents the thickness from which the intensity of a monoenergetic beam is decreased by a factor $e$, and which is therefore given by:
	
	This value is much more useful than the others, because it is also the average distance at which the first photon collision takes place.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The Gamma irradiation is anecdotally used in the conservation of the heritage of organic objects. Indeed, during the discovery of archaeologists of ancient works or vestiges, the latter are attacked by microorganisms that will destroy these objects over time. The gamma radiation will allow, without destroying the objects, to kill by gamma irradiation all these microorganisms. The best known example is the irradiation of the Tutankhamun's mummy during $10$ hours in the CEA laboratories.
	\end{tcolorbox}
	The known microscopic causes of the attenuation of a photon beam (neutral from the Coulombian point of view) which deserve our attention in the energy domain of gamma or X-ray photons are:
	\begin{itemize}
		\item Coherent Thomson scattering

		\item Coherent Rayleigh scattering

		\item Coherent Delbrück scattering

		\item Raman scattering (inelastic equivalent of Rayleigh scattering)

		\item Coherent Compton scattering (already partially seen earlier above)

		\item Photoelectric absorption (already partially seen earlier above)

		\item Photonuclear reaction

		\item Creation of electron-positron pairs (already partially seen earlier above)
	\end{itemize}
	Although we can now discuss these effects, it is impossible for us in the present state of this book to introduce the mathematical formalism used to determine the effective cross-section of each of these scattering.
	
	\paragraph{Electron-Positron pair production}\mbox{}\\\\\
	The concept of the wave-particle duality which means all matter has both "particle-like" and "wave-like" nature has been the inception of the quantum physics. Especially, for the "particle-like" nature, there are some examples that can be an empirical proof: first two processes as proves are the photoelectric effect and the Compton effect. Those two effects are well-known processes of photon absorption or scattering that photon loses its energy by interacting with other matter as a "particle-like" things.
	
	The final one for proving "particle-like" nature of all matter is "\NewTerm{pair production}\index{pair production}" or especially the special case of "\NewTerm{Electron-Positron pair production}\index{Electron-Positron pair production}". It was first observed by the physicist, Patrick Blackett, who was the winner of the 1948 Novel Prize in physics. While both the two processes mentioned above - the photoelectric effect and the Compton effect - usually occur in low energy and middle energy condition, pair production interaction is known to occur when photon has high enough energy before its collision.
	
	The pair production is a crucial example that photon energy (gamma ray) can convert into kinetic energy as well as rest mass energy:
	
	Schematic diagram about the process of pair production is shown in the figure below. The high-energy photon that has energy $h\nu$ loses its entire energy when it collides with nucleus. Then, it makes pair of electron and positron and gives kinetic energy to each particle.
	
	Basically, these interactions are ruled by three kind of the law of conservation: total energy, momentum, and electric charge.
	
	During the creation of pairs, the photon absorbed in the electric field of the nucleus can generate an electron-positron pair. For the interaction to take place, the energy of the photon must be greater than the $2m_0c^2$ (about $1.02$ [MeV]) that is to say the energy at rest of the electron-positron pair.

	This effect is important for high energies and high atomic numbers. The positron created is braked in matter just like an electron and, at the end of the course, it annihilates with an electron to give rise to two photons of $0.511$ [MeV] ("\NewTerm{annihilation photons}") emitted almost at $180^\circ$ (the whole linear momentum being transformed into energy explains the value of this angle, so the final linear momentum is zero).
	
	The creation of a pair obviously costs at least the mass energy of the electron and the positron, or $2m_0c^2$. The energy balance is then divided into the kinetic energy of the two particles:
	
	The necessity of simultaneously satisfying the conditions of conservation of the mass energy and the linear momentum the other hand imposes on the materialization effect to take place in the vicinity of a material particle (nucleus most of time) which participates in the phenomenon. Indeed, in vacuum, the two conditions are contradictory! The linear momentum of each electron is:
	
	We have proved in the section of Special relativity that:
	
	And therefore:
	
	Therefore:
	
	The original photon has:
	
	which we introduce into the energy conservation equation and with the help of the relation $p_{\beta^+,\beta^-}$ we have:
	
	which shows that by the term $c^2/v>1$ that the nucleus must take away a part of the linear momentum since:
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/pair_creation_linear_moment_conservation.jpg}
	\end{figure}
	The presence of an electric field of a heavy atom such as lead or uranium is then essential in order to satisfy conservation of momentum and energy. In order to satisfy both conservation of momentum and energy, the atomic nucleus must receive some momentum.
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/positron_electron_pair_creation.jpg}
		\caption{Electron-Positron pair production in a bubble chamber}
	\end{figure}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Pair production is invoked to predict the existence of hypothetical Hawking radiation. According to quantum mechanics, particle pairs are constantly appearing and disappearing as a quantum foam. In a region of strong gravitational tidal forces, the two particles in a pair may sometimes be wrenched apart before they have a chance to mutually annihilate. When this happens in the region around a black hole, one particle may escape while its antiparticle partner is captured by the black hole.
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Liquid Drop Model of Nucleus}
	At the beginning of the 21st century there is no general theory that underlies all the properties experimentally discovered relative to the nuclei. The nuclei has multiple facets which to this day can not be reconciled in a single theory (quantum chromodynamics not being able to model the nucleus).

	The most accurate information on the radii of nuclear nuclei and more generally on the charge density of nuclei comes from measurements of electron scattering. The latter, considered to date as elementary particles, do not undergo the strong nuclear force guaranteeing the cohesion of the nucleus and can be considered in theoretical treatments as punctual.

	Here is basically the state of our knowledge about the nuclei at the beginning of the 21st century:
	\begin{enumerate}
		\item The nucleon-nucleon nuclear interaction potential is a priori attractive at the nucleus level but sometimes also repulsive if the distance becomes too small between its constituents.

		\item The nuclear interaction is small in scope and presents a phenomenon of saturation such as if each nucleon were directly linked only to its direct neighbors (after having bound to a few nucleons its possibility of binding is exhausted), unlike  the electrostatic force.

		\item The nuclear force is a priori independent of the electric charge. It acts as well between neutrons, as between neutrons and protons and protons-protons.
	\end{enumerate}
	In the naive model that we will study here (which explains quite well the binding energy of the nucleons), the nucleus can be assimilated to a nuclear liquid drop in a first spherical approximation and incompressible, of constant density by volume:
	
	where $A$ still represents in the framework of this section the number of mass.

	Which leads to:
	
	with:
	
	However, the modeling is already failing here because the isotopes of mercury have larger radii than those predicted in the framework of this model and this with strong variations (these variations being irreconcilable with the regular evolution of a drop as a function of its number of constituents). There are even cases where by removing neutrons, the radius of the nucleus increases very significantly!

	Let us now examine the different energies involved. We will construct on the path the famous semi-empirical formula of von Weizsäcker.
	
	\subsubsection{Volumic binding energy}
	The strong nuclear interaction confers a "\NewTerm{volumic binding energy}" of the form:
	
	named the "\NewTerm{volume term}", where is $a_V$ is a predetermined constant (in a first time...) experimentally being approximately equal to:
	
	and we will prove how to theoretically determine its value a little further below during our determination of the "Pauli energy".

	The schematic representation of the idea of "volumic binding energy" (with directly related neighbors) gives:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/liquid_drop_model_volumic_binding_energy.jpg}
	\end{figure}
	where the colors (red, white) are there only to illustrate the set of binding energies that we will also see later. However, these are obviously nucleons (protons and neutrons).
	
	\subsubsection{Superficial binding energy}
	The nucleons close to the outer surface of the nucleus are less linked via the strong interaction than the deep nucleons since they have fewer direct neighbors. It is therefore necessary to depart from the idea that each constituent has the same volumic binding energy and subtract from the totality of the latter a "\NewTerm{superficial binding energy}" proportional to the surface of the nucleus which is:
	
	We then have for the superficial energy:
	
	named the "\NewTerm{surface term}" where $a_s$ is a constant determined experimentally as being approximately equal to:
	
	The schematic representation of the idea of "superficial binding energy" (with directly related neighbors) gives:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/liquid_drop_model_superficial_binding_energy.jpg}
	\end{figure}
	It is this term that makes we think to a liquid drop. Indeed, in a liquid drop, the forces are also assumed to be short-range (Van der Waals forces), and thus saturate what immediately causes a surface tension.
	
	We thus have up to now the total potential binding energy of the nucleus which is given by a part of the "von Weizsäcker semi-empirical formula" (or "Bethe-Weizsäcker formula"):
	
	
	\subsubsection{Coulomb electrostatic repulsive energy}
	We must also take into account the usual "\NewTerm{electrostatic binding energy}" or "\NewTerm{Coulomb binding energy}" which results from the electrostatic repulsion force between protons:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/liquid_drop_model_coulomb_repulsive_energy.jpg}
	\end{figure}
	As it is repulsive, it decreases the binding energy (so this will be a negative term). To get the electrical potential energy, let us recall that we have already proved in the section of Astronomy that for the gravitational energy we had:
	
	It then comes immediately for the electrostatic (coulombian) case:
	
	named the "\NewTerm{Coulomb term}" with the computable constant:
	
	We thus have up to now the total potential binding energy of the nucleus which is given by a part of the von Weizsäcker semi-empirical formula:
	
	
	\subsubsection{Energy of asymmetry (Pauli energy)}
	This energy term is inspired by the model of the nucleus based on the Fermi gas where we consider the nucleus as a set of $A$ (quasi) free nucleons enclosed in a rectangular box having the dimensions of the nucleus and respecting quantization rules (what experiment seems to confirm). Quantum physics is therefore not unique to the atomic world but also to the structure of the nucleus (we could suspect it).

	We have then proved in the section of Electrokinetics (in our study of the theory of bands) that under certain (strong!) precise conditions, the maximum number of states in a spherical volume was given for fermions by:
	
	where for recall $k_F$ is the Fermi wave number\index{Fermi wave number} that it is more usage in the nuclear field to write in another way using the de Broglie relation (\SeeChapter{see section Wave Quantum Physics}):
	
	Thus:
	
	We can then have a number of neutrons ($N$) and a number of protons ($Z$) respectively equal at most to:
	
	Knowing the expression of the volume of a nucleus, under the hypothesis of modeling by a spherical and incompressible liquid drop, we have explicitly:
	
	We have, therefore, in extenso:
	
	Assuming that:
	
	We then have for the linear momentum (of the nucleus considered as (quasi) free in the nucleus ... by construction of the assumptions of the theory of bands):
	
	Knowing experimentally $R_0$, we extract a numerical value of it. From this we can derive, taking the classical formulation of energy (hence non-relativistic), that the state of maximum energy state (fermi level of the nucleus) is then:
	
	This being done, let us now calculate the average kinetic energy per nucleon. We then:
	
	The total kinetic energy of the nucleus is then (by approximating the mass of the neutron as being equal to that of the proton):
	
	However, as we have proved above that:
	
	Then it comes:
	
	And if we put:
	
	Then we have:
	
	We know that:
	
	We will seek to get a similar relation by making an clever approximation. For this, let us recall that $A = Z + N$, we will consider the ratio:
	
	Which will be supposed small ... We have then:
	
	We have also:
	
	It then comes:
	
	The Taylor's second-order expansion in $I$ gives:
	
	and therefore:
	
	We then have:
	
	Thus we have kill two birds with one stone: on the one hand we have identified the value of the coefficient of the term of volumetric bonding energy $a_V$ as we promised earlier and on the other hand we deduce from it the term energy of asymmetry:
	
	We then have a term of potential energy which appears in the form:
	
	with:
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/liquid_drop_model_asymmetry_energy.jpg}
	\end{figure}
	Since this term is zero when the number of neutrons is equal to the number of protons, we can understand the origin of the valley of stability a little bit better.
	
	We thus have up to now the total potential binding energy of the nucleus which is given by a part of the on Weizsäcker semi-empirical formula:
	
	
	\subsubsection{Pairing Energy}
	A systematic study of nuclei shows that they are more stable when they consist of an even number of neutrons or protons. Empirically, we write this fact by subtracting the the following pairing energy:
	
	where $\delta$ is equal to $-11.2$ [MeV] if $Z$ and $N$ are even, $0$ if $A$ is odd and $11.2$ [MeV] if $Z$ and $N$ are odd.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/liquid_drop_pairing_energy.jpg}
	\end{figure}
	Finally, we have the total potential binding energy of the nucleus given by the "\NewTerm{semi-empirical formula of von Weizsäcker}\index{semi-empirical formula of von Weizsäcker}\index{von Weizsäcker semi-empirical formula}":
	
	that we also found in some textbooks in the following form:
	
	The mass energy of the nucleus can then be written:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The numerical values of the constants are not yet internationally standardized and we can find in the literature several sets of different values.
	\end{tcolorbox}
	Let us notice that the model of the liquid drop also fails to explain that the elements of fissions are not of symmetrical size (the model of liquid drop favoring a fission in two cores of the same size).

	We have for the theoretical model above the corresponding graphical representation:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/von_weizsacker_energies_plot.jpg}
		\caption{Schematic representation of the semi-empirical formula of von Weizsäcker}
	\end{figure}
	We see above that the average binding energy can be considered very approximately as constant. This can also be interpreted as a force exerted by a limited number of partners. We speak of "saturating force". By "saturating force" we mean that for a given force there is a limit to the number of nucleons placed side by side from which the addition of a nucleon only provides a constant additional binding energy. It is therefore the close neighbors who bring the force to its level, the arrival of new neighbors only subsequently supporting the average value reached. This is why we say that the energy of volume is calculated only with direct neighbors and that the model is also assimilable to a liquid drop (the molecules of a drop of water being sensitive only to directly adjacent molecules).

	This approach also explains the increase in binding energy per nucleon for low masses. Indeed, consider a so-named force of type "F2". We have then the construction of the nuclei thanks to this type of geometry:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/saturing_f2_force_type.jpg}
		\caption[]{Schematic representation of the saturating force of type F2}
	\end{figure}
	Each nucleon has two bonds (except for the case $A = 2$ which explains that the binding energy increases for the small $A$) and exhaust the possibilities of the force of the F2 type. This limitation imposes no restrictions on the size of the objects to be built and we can imagine constructions as large as we want, stabilized by this type of force.
	
	If we calculate the total binding energy, we will have $1\cdot E_p$ for $A = 2$, where $E_p$ is the potential binding energy provided by a bond. Similarly, we will have $3\cdot E_p$ for $A = 3$, $4\cdot E_p$ for $A = 4$, $5\cdot E_p$ for $A = 5$, and so on... The binding energy per nucleon will then be $E_p/2 $for $A = 2$ and then $E_p$ for all the other masses, for a constant force of type F2. Therefore this force saturates beyond $A = 2$.

	If we take a F3 force type, we will have:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/atomistic/saturing_f3_force_type.jpg}
		\caption[]{Schematic representation of the saturating force of type F3}
	\end{figure}
	If we calculate the total binding energy, we will have $1\cdot E_p$ for $A = 2$, where $E_p$ still remains the potential binding energy provided by a bond. Similarly, we will have the $3\cdot E_p$ for $A = 3$, $6\cdot E_p$ for $A = 4$, $7\cdot E_p$ for $A = 5$, etc.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We notice that for $A = 5$ only $2$ bounds can start from the last vertex of the pentagon, otherwise another vertex of the pentagon would have $4$ bounds and not $3$ (force F3).
	\end{tcolorbox}
	The asymptotic value of the binding energy per nucleon will then be $1.5\cdot E_p$ for all masses greater than $A = 3$, for a constant force F3. Therefore this force saturates beyond $A = 3$.

	We can continue so on with nuclei that include more and more nucleons.

	In the particular case of a force that could interact with all other surrounding nucleons, there will be as we have proved in the section of Graph Theory for complete graphs ("graph size"):
	
	bounds and thus a behavior in $(A-1)/2$ in the average binding energy (since it is simply a matter of dividing by $A$ the number of bonds). Thus, in the classical case, the binding energy would only increase what is not compatible with the experiment.
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us see some applications of this model starting by examining the predictions on the fission of the uranium $_{92}^{236}\mathrm{U}$ in two equal sub-products in mass and in charge:
	
	and this by using the liquid drop model by neglecting the terms of asymmetry and pairing energy.

	For this, let us recall that:
	
	and we will take as values of the constants:
	
	Let us evaluate the difference of energy between on the one hand the $2$ nuclei resulting from the fission and on the other hand the starting nucleus.

	We have then:
	
	as:
	
	The development is simplified in:
	
	\end{tcolorbox}
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	If there is fission, we will have $\Delta E\ge 0$, therefore:
	
	thus:
	
	Or rearranged:
	
	Either by putting the constants, it gives:
	
	But for $_{92}^{236}\mathrm{U}$, we have:
	
	So according to the above result $_{92}^{236}\mathrm{U}$ can fission symmetrically but this is not the case in the experiment, because in reality we have:
	
	\end{tcolorbox}
	There is, however, another possible theoretical approach which gives a result more in agreement with the experiment. Indeed, we can imagine that a nucleus can undergo fission if the force derived from the superficial energy is exactly compensated by the Coulomb force.

	In the end, we will compare the ratio $Z^2/A_m$ with the result we will get with the new approach..

	Let us first recall that we have seen in the section of Electrostatics (among others ...) that the electrostatic force derives from the electrostatic potential:
	
	where the radius $R$ has in our case the value of the nuclear radius, with for recall:
	
	We then have:
	
	Therefore:
	
	If the Coulomb repulsion prevails, the fission prevails and we then have:
	
	Therefore:
	
	After simplification, it remains:
	
	Therefore:
	
	Since this ratio for $_{92}^{236}\mathrm{U}$ is close to $36$ and therefore less than $52$, this approach naively explains why there can be no symmetrical fission (as experience confirms it!).
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/atomistic/liquid_drop_model.jpg}
		\caption{Liquid drop model of Uranium 235 nuclear fission (source: OpenStax)}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.55]{img/atomistic/nuclear_plant.jpg}
		\caption{Pressurized water reactor (author: Gloria Faccanoni)}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/atomistic/atom_size.jpg}
		\caption{Typical atome aize (source: ?)}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/atomistic/chain_reaction.jpg}
		\caption{Uranium 235 fission chain reaction (source: OpenStax)}
	\end{figure}
	
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{80} & \pbox{20cm}{\score{4}{5} \\ {\tiny 95 votes,  71.58\%}} 
	\end{tabular} 
	\end{flushright}

	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Quantum Field Theory}
	\lettrine[lines=4]{\color{BrickRed}B}efore the formulation of quantum physics, particles and fields were considered separate but linked. The particles have certain intrinsic characteristics (such as mass and electric charge) and produce fields (electromagnetic and gravitational). Each force field emanating from the particles and fills the space around them. The fields can store and transport energy; they are, in this sense, the real continuum that bind particles and communicate the interactions between them. It was considered that the particles were composed of matter and energy fields were compounds. The concept of force field was the alternative of the 19th century old and mysterious action at a distance. Particles that don't react to any force field are unobservable and physically are no sense. Similarly, force fields that act on no particles are also meaningless. The concepts of particles and fields thus have a sense only when they are connected.
	
	The notion of field began to be fundamentally changed with the introduction by Albert Einstein of the concept of "photon". Under this new concept, the electromagnetic field has no energy distributed in a continuous manner in space. The photon is the "\NewTerm{quantum of the electromagnetic field}\index{quantum of the electromagnetic field}". It carries the energy and momentum of the field. The electromagnetic interaction between two charged particles and the transfer of energy and the amount of movement of a particle to another must therefore take place by the exchange of electromagnetic quanta of energy: the photons. The theory of such interactions (between charged particles), named "\NewTerm{quantum electrodynamics}\index{quantum electrodynamics}" (QED), was the first successful application of these ideas (it allows to prove the fine structure of the Sommerfeld model, to explain the spin of the electron, etc.) and it is to it that we will look here.
	
	In this section, we do not wish to make a complete course of quantum filed theory because for reminder, the entire book is only intended to give the basics of what an engineer must know in the early 21st century and incidentally the author to have fun studying subjects that he had not seen during his school career. As such, the interested reader to deepen over this matter may be read to best book we've ever had hands so far (detailed, simple and pedagogical developments with many practical cases) on the subject which is \cite{desai2010quantum}.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Quantum field theory is the application of quantum mechanics to fields. It provides a framework widely used in particle physics and condensed matter physics. The basics of quantum field theory to which we will limit our study were developed between 1935 and 1955, mainly by Paul Dirac, Wolfgang Pauli, Sin-Itiro Tomonaga, Julian Schwinger, Richard Feynman and Freeman Dyson.
	\end{tcolorbox}
	
	Before we launch into the calculations (see below), let us show that the approach suggested above, can be considerate with a simple formalism (pedagogical).
	
	Readers should however remember that simple approaches require sometimes erroneous mental constructions (overly simplistic) compared to reality, but which meet the objective: to have a more or less understandable and intuitive model.
	
	Consider at this purpose the figure below (representation of the elastic collision of two electrons):
	\begin{figure}[H]
		\begin{center}
		\includegraphics{img/atomistic/feynman_pseudo_diagram.jpg}
		\end{center}	
		\caption{Example of pseudo Feynman-diagram}
	\end{figure}
	This figure is named, and wrongly (!), in many educational books \NewTerm{Feynman diagram}\index{Feynman diagram}" (in reality it is only a diagram that looks like a little bit because the real Feynman diagrams are intended to calculate Wick products in a perturbation series).
	
	Let us suppose that two electrons which are represented, initially move at the same speed. They approach first and then move away from each other along a straight line in space that is projected on the time axis, in the direction of increasing time. The left electron emits a photon (the wavy line), and for a time $\Delta t$, there are two electron and a photon. The electron on the right then absorbs the photon and the interaction is temporarily ended; other photons will eventually go back and forth between the electrons. The average force is proportional to the transfer rate of the amount of movement due to the exchange of photons. The probability of the emission or absorption of photons by a particle is connected to its electric charge. The force must be proportional to the product of the charges in interaction (according to Coulomb's law). Think of the repulsive force between two astronauts floating in space and sharing a ball in one direction and then the other (this is an educational approach to the problem but does not apply for example to the attraction between two particles of opposite charges!). However, the opposite phenomenon of attraction cannot be viewed in this way but only in formal mathematical form.
	
	The collision shown in the figure above is elastic; the energy of each electron is unchanged in the collision. Despite this, for a time $\Delta t$, the system contains an additional energy $hv$ corresponding to the photon. Meanwhile this time $\Delta t$, conservation of energy is apparently non respected! Can we tolerate this situation? The answer given by modern physics, is: Yes! But it can never be observed (...). In other words, because there is always some uncertainty $\Delta E$ on the measured value of the energy of a system. The Heisenberg uncertainty principle involving (see the simplified proof in the section of Wave Quantum Physics) that:
	
	A violation of the energy conservation law until a given amount $\Delta E$ will be hidden by uncertainty on energy provided that the time available to make the observation is sufficiently small as having for upper value:
	
	obviously a value less than $\Delta t$ also satisfies the condition. We can therefore write:
	
	The uncertainty on the energy exceeds the energy of a photon $hv$ if the photon exists for a shorter time than:
	
	The photon can then be observed on a maximum distance of:
	
	and as the frequency $v$ can be arbitrarily small, the scope of the transmitted force by the photon without mass is unlimited. It may seem in this relation that the scope is limited to a free photon. But this would be forgetting (\SeeChapter{see section Wave Quantum Physics}) that a free photon does not exist, because it would have a totally unknown frequency. Therefore the interaction distance would be also undetermined.
	
	These exchange of quanta, that are unobservable, are named "\NewTerm{virtual photons}\index{virtual photons}". Since photons are not charges, we also say that the interaction is made by "\NewTerm{neutral current}\index{neutral current}".
	
	A much more satisfactory approach is that of using mass as energy term:
	
	Thanks to this relation, it is possible to know the time during which a virtual particle can travel a distance that would be:
	
	We will see later how to approximately determine the mass of virtual particles that mediate in the nuclear forces which will allow us to estimate the duration of interactions as being of the order of $10^{-24}\; [\text{s}]$.
	
	In the late 1920s, it became clear that we could consider each known particle (proton, electron, etc.) as the quantum of a specific field. In this vision, there is an electron field, a proton field, and so on as will be proven below (the Universe would be a set of unified field). Any object is in reality a set of manifestations of observables of field quantas.
	
	Furthermore, we saw that the writing of wave equations for relativistic particles (Dirac equation and Klein-Gordon equation proven in the section of Relativistic Quantum Physics) brings intractable problems in a classical point of view, including negative energies. In fact, this approach is not justified, because according to Einstein equation mass and energy are equivalent, and if we add to that the energy-time Heisenberg's uncertainty principle we find that an infinite number of particles can be created or annihilated, hence the need for a model to take into account not only the properties of a single particle, but of a set of particles, both real and virtual!
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	When Fermi formulated his theory of weak interactions in 1932, he used the same principles those of quantum electrodynamics (this is one reason why QED is named the "jewel of physics" - the Standard Model is also based on this theory). Two years later, the Japanese physicist H. Yukawa proposed that the weak interaction was due to the exchange of a massive virtual boson.
	\end{tcolorbox}
	
	\subsection{Yukawa potential}
	The best way to argue the example of quantum remains the "proof" of the Coulomb's law (and also of Newton's Law) from the results we have achieved in wave quantum physics (we own these developments to the physicist Hideki Yukawa).

	A simplified version of this proof as we like them in this book is to first remember the free Klein-Gordon equation (\SeeChapter{see section Wave Quantum Physics}):
	
	this equation describes the amplitude dynamics of presence of a particle without spin in time in a given potential.
	
	Consider a static component equation (time-independent) with spherical symmetrical:
	Let us consider a static component of $\Psi$ (time-independent) with spherical symmetry:
	
	The Klein-Gordon equation is then reduced to:
	
	If we divide on the both side of the equalitby by $\hbar^2 c^2$ we get obviously:
	
	Let us recall (\SeeChapter{see section Vector Calculus}) the notation of the Laplacian of a scalar field:
	
	and also its expression in spherical coordinates where $r=0$ is identified to the source of the field (\SeeChapter{see section Vector Calculus}):
	
	As the field $U(r)$ has a spherical symmetry  (depending on $r$ only) the Laplacian is reduced to:
	
	So the equation of the field U (r) is written:
	
	This differential equation has for solution (we guess easily enough that the exponential is a possible solution):
	
	where $C$ is an integration constant.
	
	In the context of the use of natural units (which is most common at this level in the scientific literature) this potential is written:
	
	and is named "\NewTerm{Yukawa potential}\index{Yukawa potential}".

	The reader will notice that apart from the distance $r$, the other variable in the exponential is the mass (the other terms are universal constants). Consequence: the Yukawa potential is as good a "\NewTerm{scalar field}\index{scalar field}" in the case where mass is zero (see example below) that a "\NewTerm{mass field}\index{mass field}" if the mass is not zero!
	
	This leads us to the following hypothesis: if it is the electric field that holds charged particles in the atom between them (see the treatment of non-mass fields below), it is the mass field which keeps the uncharged particles together in the atom.

	In other words, if particles interact via a mass field of mass $m_0$ (instead of interacting with massless photons), their mutual strength will decrease exponentially (which is very fast).

	Yukawa's approach allows a new point of view to the interpretation of nuclear phenomena, but it is however too naive to adequately explain the strong interactions in general.
	
	\pagebreak
	\subsubsection{Mass fields}
	The physicist H. Yukawa therefore proposed in 1935 that nuclear power was own its very short range by the fact that it is transmitted by massive particles (more the mass of exchanged quanta is big, the greater the range of the interaction is reduced) described by the mass field above.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	In the historical context of the time, these hypothetical particles were "\NewTerm{mesons}\index{mesons}". But we will see that this assumption did not hold up very long.
	\end{tcolorbox}
	Let's take a closer look to this. Let us write the Yukawa potential as follows:
	
	with:
	
	This notation is not innocent, because as we will see it in detail later, when $R\rightarrow +\infty$ (the case of electromagnetic and gravitational interaction) then $m_0\rightarrow 0$ and then we fall back on the fundamental law of electrodynamics or of gravitation where the particle interaction is respectively the photon (null mass) and the graviton.
	
	Thus, assuming that the radius of the strong nuclear force (cohesion of nucleons between them) is around $R\cong 1\cdot 10^{-15}\;[\text{m}]$ and that of the weak nuclear interaction (which should be the cause of the beta disintegration as we already mentioned in the section of Nuclear Physics) around $R\cong 1\cdot 10^{-18}\;[\text{m}]$, we get the binding interaction energies and their approximate weight immediately:
	\begin{itemize}
		\item For the "\NewTerm{strong interaction}\index{strong interaction}":
		
		that is to say approximately $386$ times the mass of the electron ($m_e$) and $1/5$ the mass of the proton ($m_p$).
	
		Two years after this prediction of Yukawa, physicists discovered a particle corresponding to this mass: the meson $\mu^{-}$. It was later that it was not good the good particle but a particle of the same type as the electron particle, that is to say a lepton and therefore a fermion\footnote{Go see the table of fundamental particles in the section Elementary Particle Physics to see how physicists classify elementary particles of the standard model} (so it can not be a messenger particle!!). Furthermore, the experiences of diffusion and collisions with protons, deuterons, etc. at energies higher and higher showed that there was a change in the intensity and shape of the strong interaction incompatible with the hypothesis of a single meson. Furthermore, the hadronic resonances showed that there was meson excited states which is hard to imagine for particles considered fundamental in analogy with the photon!!
	
		The particles detected in the laboratory and that seemed to be the best candidates at that time time (since there were several ...) for the strong nuclear force were "\NewTerm{pions}\index{pions (particle)}" (or "\NewTerm{pi mesons}\index{pi mesons}") which are know under three forms (at least at the time we write these lines):
		\begin{gather*}
			\pi^+,\pi^-,\pi^0
		\end{gather*}
		
		and which are almost $270$ times more massive than the electron. So this mass difference clearly indicates that the model of Yukawa is not entirely accurate.

		Before the discovery of quarks (which mesons are made of), the pi mesons (pions) were therefore considered to be the vectors of the strong interaction (today we know that in fact these vectors are gluons).
	
		\item For the "\NewTerm{weak interaction}\index{weak interaction}":
		
		It is therefore a colossal mass, a hundred times the mass of the proton! The vectors of interaction have been candidates that were identified in 1983 in the CERN accelerator. These carrier particles of the weak nuclear force are named "\NewTerm{intermediate bosons}\index{intermediate bosons}" and denoted by $W^+$, $W^-$, $Z^0$.
	\end{itemize}
	These observations led the hypothesis that Yukawa's theory was not a fundamental theory enough even if it explain quite well some properties...

	
	\subsubsection{Non-mass fields}
	Let us now imagine a static scalar field with a spherical symmetric which the photon (particle without spin) is the hypothetical quantum exchange. As the photon's mass is zero, the expression $U (r)$ is reduced 
to:
	
	If we interpret $U (r)$ as the electrostatic potential source of a quantity $Q=nq$ of elementary electric charges $q$, then the constant $C$ in our metric is:
	
	Such that:
	
	As we have:
	
	We get:
	
	Which give us:
	
	We can conclude that if a particle is in a potential field with spherical symmetry $U (r)$, whose photon is assumed to be initially the interacting quantum, then we are dealing with an electrostatic field whose expression is identical to the Coulomb law (this valid once again in a big way the theory Wave Quantum Physics).
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Hubert Reeves and his colleagues astrophysicists proved that at the time of the genesis of the Universe, the smallest deviance in coupling constants would have caused the instability of nucleons and would have condemned cosmic evolution.
	\end{tcolorbox}
	The photon is therefore indeed the quantum interaction of the electric field with spherical symmetry (when the electric charges have a relativistic speed the electric field is not spherical anymore, and the equations become a bit more complicated - see the section of Special Relativity for some details about this) and we should not talk of electric charges but of "transparency" to photons. Indeed, the neutron being globally neutral it should not interact with the electric field, but as it is composed of charged particles (quarks) experiments show an influence in the presence of an electromagnetic field (which photon is the quantum interaction).
	That said, applying the same reasoning, we can still find the Newton's gravitational potential:
	
	This would imply that the quantum interaction of the gravitational field is massless (at least in the case of small masses as we know the Newton's potential  is only an approximation of General Relativity in the case of small masses). Since the gravitational field does not seem to interact with the presence of a magnetic or electrostatic field, this leads us to speculate that the quantum of interaction is not the photon but another particle, we name the "\NewTerm{graviton}\index{graviton}".
	
	\pagebreak
	\subsection{Euler-Lagrange equation for Fields}
	How field theory was introduced from the elementary particles by Dirac is known for historical reasons under the name of "\NewTerm{second quantization}\index{second quantization}".

	It may be useful to identify a possible source of confusion: the fields are not related to the wave-particle duality. What we mean by "field" is a concept that allows the creation or annihilation of particles at any point in space as we will discussed it in the mathematical developments below.

	Recall that we have defined in the section of Wave Quantum Physics during the study of the Schrödingerps evolution equation the Heisenberg's operator necessary for the De Broglie normalization condition:
	
	Differentiating this operator with respect to time, we trivially have trivially (once known the notation seen in the section of Wave Quantum Physics):
	
	where for recall, the commutator of two operators is given (as we have already seen it in our study of adjoint and  Hermitian operators in the section of Wave Quantum Physics) by definition:
	
	This is the Hamiltonian $H$ which appears first in the previous prior previous relation.
	
	Now, as we know, we can substitute to $\dot{X}(t)$ observable that are also well known to us such that we get in the cas of a constant hamiltonian:
	
	But we can just as easily substitute a time-dependent Hamiltonian $H(t)$ such that:
	
	As we know (\SeeChapter{see section Wave Quantum Physics}) the explicit expression of $X$ when it is replaced by the operators $q(t)\rightarrow \hat{q}(t)$ or $p(t)\rightarrow -\mathrm{i}\hbar\partial_k$ then we see that we can simplify the previous relation by:
	
	This is named "\NewTerm{Heisenberg equation of motion}\index{Heisenberg equation of motion}". 
	
	What is interesting in the two relations $\dot{q},\dot{p}$ obtained previously, is the way in which is realized the connection between quantum physics and classical mechanics. Indeed, we proved in the section of Analytical Mechanics the following relations are will always be valid whatever the subject area:
	
	and also that:
	
	and assuming the generalization to several degrees of freedom as intuitive:
	
	The generalization to several degrees of freedom is immediate and gives us all the following relations (we simplify the notation by omitting the explicit time dependence):
	
	 and also:
	
	We still need two other important relations that we will immediately determined. First, according to the definitions of the commutators, we already prove in the section of Analytical Mechanics:
	
	By cons, it is a little more subtle to prove the value of $[q,p]$ (we joke...). Recall that we had prove in our study functional linear operators (we restrict ourselves to the case of the $x$ here):
	
	and that $q$ represents a generalized coordinate ($x$ for example ...). So we have (results already prove in the section of Wave Quantum Physics...):
	
	The last two relations can be generalized to all necessary components such as:
	
	with for recall (\SeeChapter{see section Tensor Calculus}):
	
	which is the Kronecker symbol.
	
	To arrive finally to quantum field theory, we still need to generalize to a continues infinite number of degrees of freedom. Even the simplest of fields is characterized, at a time $t$, by an infinity of continuous quantities:
	
	for any $\vec{x}$. We could then imagine to represent the function $\Phi$ by its values $\Phi(\vec{x},t)$ in a discrete set of points $\vec{x}_i$ that we will ultimately make infinitely dense (beware of the fact that we use the concept of density!). We can also work, to begin, not in all space, but in a finite volume that we will eventually make very large. By doing so, we can find how to generalize the canonical formalism and the quantification process. At the formal level, notwithstanding subtle issues of convergence (see the mathematical parts of this book), the generalization to continuous systems is mainly to replace the quantities with index $i$ by integrals of the arguments $\vec{x}$, and the deltas of Kronecker by the delta of Dirac (on space-time):
	
	Considering then the variational principle as we have studied it in the section of Analytical Mechanics:
	
	and the principle of least action requiring us to have:
	
	where the Lagrangian will be now a function of the field $\Phi(\vec{x},t)$ and of derivative with respect to $\phi(\vec{x},t)$ (since there is no concept of momentum for a field!).
	
	If we divide the above equation by $\delta q(t)=\delta q\neq 0$ we get:
	
	what gives us the right to write:
	
	and imposing an analogy with the concept of field:
	
	where $x:=(\vec{x},t)$ and $\mathrm{d}x^4=\mathrm{d}x_1\mathrm{d}x_2\mathrm{d}x_3\mathrm{d}x_t$.
	
	Finally, since all the following terms are equal to zero, they are equal (we introduce the Euler-Lagrange equation proved in the section of Analytical Mechanics):
	
	in analogy with the field $\Phi^2$ we get (thus it is an intuitive approach with the physicist way of life...):
	
	However, since this writing is not very convenient, it is customary to write the partial differentials (using the natural units of physics) of the components $\partial/\partial t$, $\partial/\partial y$, $\partial / \partial z$, $\partial / \partial x$ in the form $\partial_\mu$, which finally gives us:
	
	Obviously, if we do not use natural units, we should adopt the notation:
	
	The fact that the partial derivation $\partial_\mu$ now acts on all the components and not only on $t$ is due to the change of generalized coordinate $q$ function only from $t$ to a function of the field $\Phi^A$ dependent on $x$, $y$, $z$ and $t$. The reason for this lies in the fact that time and spatial coordinates play the same role, that of describing the space-time continuum on which the physical system evolves.

	This also leads us to write the principle of least action in the following form:
	
	With the action of fields denoted more traditionally:
	
	or to differentiate Lagrangian and Lagrangian density (we "stylized" sometimes the $L$ to makes the difference between the both):
	
	to be compared to the action of the particle:
	
	In analogy with $p_n=\dfrac{\partial L}{\partial \dot{q}_n}$, we will write:
	
	and in analogy with $H=\sum p_n\dot{q}_n-L$ we write:
	
	But a field is a continuous medium! The sigma sum  is therefore no longer adapted and we must move to an integration over all space-time such that:
	
	In analogy with the Heisenberg equations of motion (this way of doing things is often named the "\NewTerm{principle of correspondence}\index{principle of correspondence}"), we write:
	
	Let us now turn to quantum theory by postulating corresponding of "\NewTerm{Heisenberg field operators}\index{Heisenberg field operators}". Let us recall that we had obtained earlier above that:
	
	which gives us:
	
	If we summarize a little and we display the comparison with Wave Quantum Physics, we have finally:
	\begin{enumerate}
		\item In Wave Quantum Physics (it's nice to look at it isn't it?):
		
		
		\item And the equivalent by principle of correspondence in Quantum Field Theory (here it don't just look nice... it becomes art!):
		
	\end{enumerate}
	And voila! We have just passed from the parameters of Wave Quantum Physics where the punctual bodies are described by wave functions, to a Quantum Physics where the punctual bodies become continuous fields.

	It remains only to apply this general scheme to concrete examples:

	We will start with a first example taking into account the relativistic aspect. Thus, the simplest non-trivial Lagrangian density that we can construct is of the form (you will see immediately what it will lead us to, which will confirm its theoretical validity - moreover, the development that follows could very well have been presented backwards):
	
	Or more explicitly:
	
	that the physicists name "\NewTerm{scalar field for a free and spinless particle}\index{scalar field for a free and spinless particle}" or "\NewTerm{Klein-Gordon Lagrangian}\index{Klein-Gordon Lagrangian}" for a spin-free particle where we use the usual condensed notations:
	
	and the natural units:
	
	Let us calculate the Euler-Lagrange equation relative to it (normally it is trivial) without forgetting that it is a functional derivative, which greatly simplifies the calculation:
	
	Following the request of a reader here are the details allowing to arrive at this result:
	
	Hence the equation of motion in natural units and daring a dangerous notation for the double partial derivative (the Einstein summation is then implicit...):
	
	or as we will quick see it, in order to fall back on the results obtained in the Relativistic Quantum Physics section, we are obliged to introduce the contravariant and covariant differential operator (\SeeChapter{see section Tensor Calculus}) with the signature $-, + , +, +$:
	
	Let us recall now for comparison purpose that in the section of Wave Quantum Quantum Physics we had obtained the following free Klein-Gordon equation:
	
	also with the signature $-, +, +, +$ thus with the Alembertien operator:
	
	We then have a perfect correspondence between the free Klein-Gordon equation and the field equation (written below in natural units as usual):
	
	and it is here that one can eventually feel a shiver in the back and stay admiring in front of the power of mathematical formalism opening new perspectives on how to see the gears of our Universe... Thus, in Quantum Field Theory, the Klein-Gordon equation can be reinterpreted as a field equation! 

	The form of the free Klein-Gordon equation involving fields is sometimes named the "\NewTerm{Klein-Gordon field equation}\index{Klein-Gordon field equation}".
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We can generalize the Klein-Gordon field equation to curved space. Indeed, as:
	
	and therefore:
	
	Then the action $S$ must be corrected using the invariant volume introduced in the section of Tensor Calculus such that:
	
	\end{tcolorbox}
	And even... better ... you will see, we'll do it a little in a blind way ... and therefore!!!: Let us now consider the following Lagrangian (which we assume will be obtained by successive trials and errors... but again we could have done the development backwards), intending to express the "\NewTerm{interaction of an electromagnetic field with a current density}":
	
	where we recognize there the tensors of the electromagnetic field proved and determined in the section of Electrodynamics and Special Relativity and for which, for recall, we have:
	
	In this Lagrangian, let treat the vector potential as a field such that:
	
	Therefore, by decomposing the developments, we get very easily:
	
	In a first step, the reader will verify by doing some relatively elementary tensor calculus that:
	
	Then:
	
	Therefore, the equation of the field is written:
	
	hence:
	
	We must admit that the result is quite beautiful even if the approach is not the most rigorous one! We thus fall back on the Maxwell equation with sources with the same Lagrangian of the field (\SeeChapter{see section of Electrodynamics}). Thus, this massless lagrangian is assimilated to the Lagrangian of the vector field of spin $1$ assimilated to the bosons (\SeeChapter{see section Statistical Mechanics}).

	Let us recall now that we had obtained in the section on Electrodynamics the following action for a charged particle in an electromagnetic field (before a long development which had brought us to the tensor of the electromagnetic field...):
	
	and remembering that (\SeeChapter{see section Electrodynamics}):
	
	it comes:
	
	The corresponding Lagrangian density is therefore:
	
	So we have finally:
	\begin{enumerate}
		\item The Lagrangian (Lagrangian density) of a charged particle in an electromagnetic field (which we have just obtained):
		
		\item The Lagrangian (Lagrangian density) we get just before (which allowed us to fall back on Maxwell's equations with sources):
		
	\end{enumerate}
	Hence, it is natural to write the "Lagrangian (total Lagrangian density) of the electromagnetic field":
	
	Hence, it is natural to write the "\NewTerm{Lagrangian (total Lagrangian density) of the electromagnetic field}\index{Lagrangian of the electromagnetic field}":
	
	Let us now continue our way with the free Dirac equation! Let us recall that in the Relativistic Quantum Physics section we obtained the free Dirac equation in the form (basically, let us also recall that it is a relativistic equation):
	
	Now let us recall (\SeeChapter{see section Linear Algebra}) that $(AB)^\mathrm{i}=B^\mathrm{i}A\mathrm{i}$. Therefore, it comes:
	
	where we have introduced the "\NewTerm{Feynman slash notation}\index{Feynman slash notation}" $\cancel{\partial}$ (less commonly known as the "\NewTerm{Dirac slash notation}\index{Dirac slash notation}").
	
	But, $\partial_\mu^\dagger=\partial_\mu$ and it is super easy to verify (do not forget that we use the Dirac representative form of Pauli matrices!!!):
	
	which leads us to write:
	
	It is then convenient to introduce the "\NewTerm{Dirac adjoint}\index{Dirac adjoint}":
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Let us recall that $\Psi$ is a matrix-column and $\Psi^\dagger$ a matrix-line. So it comes that $\overline{\Psi}$ is also a matrix-line!
	\end{tcolorbox}
	Using the fact that in the Dirac representation $\gamma^0 \gamma^0 =\mathds{1}$ we can write:
	
	by simplifying the $\gamma^0$ it comes the "\NewTerm{free adjoint Dirac equation}":
	
	What we traditionally write:
	
	The notation $\overleftarrow{\cancel{\partial}}$ meaning that the operator $\cancel{\partial}$ operates on $\overline{\Psi}$ on the left such that:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Some authors write $(\mathrm{i}\cancel{\partial}+m_0)\overline{\Psi}=0$ but this is false because $\overline{\Psi}$ is a line matrix as we pointed out above !!!
	\end{tcolorbox}
	Finally we have for the free Dirac equations:
	
	Let us now suppose that the "\NewTerm{Lagrangian of the free Dirac spinor field}\index{Lagrangian of the free Dirac spinor field}" is of the form (because in the end it is the Lagrangian that interests us!):
	
	where we have for recall $\bar{\Psi}=\Psi^\dagger \gamma^0$. It is therefore the Lagrangian of the spinor field for the particles of spin $1/2$ which are therefore free fermions.

	Considering the quantities $\bar{\Psi}$, $\Psi$ as independent (this is what they are anyway since they are orthogonal) and choosing the spinor field as being $\Psi^\dagger$, we have the Euler-Lagrange equation:
	
	The second term is equal to zero since the Dirac Lagrangian does not contain terms with $\partial_\mu\Psi^\dagger$. In fact it remains:
	
	So we fall back on the free Dirac equation (the same development that can be done for the free adjoint Dirac equation)! Thus, in this framework, the only way to explain the quantum properties of the material containing particles with spin $1/2$ is to involve the fields $\bar{\Psi}$, $\Psi$ representing electrically charged particles, electrons and positrons as we know. We then name then these entities "\NewTerm{Dirac field}\index{Dirac field}" also named \NewTerm{spinor field}\index{spinor field}" or \NewTerm{spin $1/2$ fermion field }\index{spin $1/2$ fermion field}".

	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Often, when using the Dirac equation one finds the slash notation introduced in the section of Relativistic Quantum Physics used on four-momentum. Using the Dirac basis for the gamma matrices:
	
	as well as the definition of four momentum (\SeeChapter{see section Special Relativity}):
	
	we see explicitly that:
	
	Similar results hold in other bases, such as the Weyl basis.
	\end{tcolorbox}
	
	\subsection{Gauge Theories}
	We will now see a simple approach to a tool that revolutionized the approach of modern particle physics in the mid-$20$th century and that has earned several Nobel Prizes to those who contributed to it.

	We advise very strongly before reading what follows that the reader will also take a preliminary look at the sub-section of Gauge Theory of the Electrodynamics section, as it contains a first example of an invariance of Gauge making emerging a field (the vector potential) necessary to explain certain phenomena at the quantum scale as clearly explained by Pauli's equation (\SeeChapter{see section Relativistic Quantum Physics}).

	Since the early 1980s, popularization magazines have talked a lot when dealing about Quantum Physics on Gauge theories. Electromagnetic interactions and Weak interactions are described jointly by a Gauge theory developed by Glashow, Weinberg and Salam. Strong interactions also seem correctly described by a Gauge theory. It is within the framework of these gauge theories that theoretical physicists try to unify the various fundamental interactions of nature. It is therefore appropriate, even in this book which treats in an elementary way of Quantum Physics, to speak of Gauge theory within the framework of this study domain.
	
	To do this, we will already consider the context that led to the discovery of Gauge invariance in the context of Electrodynamics (see the section of the same name for details) and make a comparison with certain developments seen in the section of General Relativity and the role played by Weyl in the proof of the fundamental principles of a Gauge theory.

	Let us recall that the Special Relativity and General Relativity are based on the premise that there is no absolute reference frame in the universe. We have seen in the section of Special Relativity in a long and broad way that the relations which make it possible to pass the laws of physics from one reference frame to another one depend only on the relative speed between them. Thus, Special Relativity is a theory with global symmetry. We have also seen in the section of General Relativity that the affine connection is the link between the referentials of the local theory (weak field approximation) that is General Relativity.
	
	In 1919, the first experimental observation of the deviation of the light of a star by the gravitational field of the Sun took place. This spectacular confirmation of the theory of General Relativity inspired Hermann Weyl, who proposed the same year a revolutionary conception of Gauge invariance: If the effects of a gravitational field can be described by a connection expressing the relative orientation between local repositories of space-time, can other forces of nature such as electromagnetism also be associated with similar connections?

	We consider two types of Gauge symmetry: one known as "\NewTerm{global Gauge}\index{global Gauge}" and the other named "\NewTerm{local Gauge}\index{local Gauge}". They are distinguished by the parameter characterizing the phase change of the wave function (we will see this in detail further below).
	
	\subsubsection{Global Gauge invariance}
	We will therefore study the Gauge invariance using the Schrödinger equation and show that even if the results may seem confusing (in the context of complex set $\mathbb{C}$ applications), they nevertheless remain mathematically correct!
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The global gauge invariance is rigorously referred to as "global symmetry".
	\end{tcolorbox}
	Let us consider the Schrödinger's equation:
	
	with as we have shown:
	
	with $\Psi=\Psi(\vec{r},t)$. Either in the case of a free particle:
	
	This operator is obviously invariant in the transformation which passes from $\Psi$ to $\Psi'$ with (change of the phase of a plane wave by an angle $\alpha$):
	
	where $g$ is a coupling constant (to ensure the homogeneity of the units and the amplitude) being considered as a real number and $\alpha$ a real parameter independent of the coordinates (in a first time ...) of space and time.
	
	Then:
	
	becomes:
	
	and as an $\alpha$ does not depend on $x$, $y$, $z$, $t$ then:
	
	Either after simplification:
	
	The form of the equation remained the same when we made the change from $\Psi$ to $\Psi'$.
	
	Thus, the description of a free system is not affected by the global phase change. In the language of group theory (\SeeChapter{see section Set Algebra}), we speak of invariance under the $\text{U}(1)$ group of  phases.

	In other words to speak like the physicists...:
	
	defines a Gauge transformation by the rotation $\alpha$ (the parameter in the sense of the Lie groups) in the complex plane (but that corresponds to a phase change in the field of wave theory).
	
	The set of rotations forms a group denoted $\text{U}(1)$ which the usage make we name it the "\NewTerm{Gauge group}\index{Gauge group}" (isomorphic to $\text{SO}(2)$ as we have seen it in the section of Set Algebra).
	
	The set of all $e^{\mathrm{i}g\alpha}$ forms a monodimensional representation of the group $\text{U}(1)$ which we call the "representation $g$". There is, of course, an infinity of representations $g$ (as much as there are values of $g$!).
	
	Since the parameter $\alpha$ does not depend on position and time, we say that the system is "invariant by global gauge" transformation (everywhere at the same time) or simply an invariant of $\text{U}(1)$ in time and space.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	In fact, global Gauge transformations are a subset of local gauge transformation: changing the same amount everywhere is a special case (ie, more restricting) of changing the phase of each point independently. A local Gauge transformation is therefore not a subset of a global Gauge transformation. In this respect the name is a bit misleading.
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{Local Gauge invariance}
	But but... either the Global gauge invariance shows that we have an equation that remains valid as part of a fixed phase change. But now in a laboratory this Schrödinger equation must be valid even if the phase depends on position and time. This constraint is named a "\NewTerm{local Gauge invariance}\index{local Gauge invariance}".

	We consider this time that $\alpha$ is a function $\alpha(\vec{r},t)$ and the idea obviously is to check whether the Schrödinger equation remains invariant in the transformation:
	
	It is therefore obvious that Schrödinger's equation:
	
	is no longer invariant. Indeed, we quickly see that just the operator $\vec{\nabla}^2$ in the Hamiltonian will pose problem by making appear annoying terms that will not cancel out:
	
	To solve this problem, we introduce the force field associated with the vector potential and the electrical potential and we will see that it guarantees the local invariance (therefore it is impossible to envisage an invariant phase change without the presence of a force field of this type). The local invariance imposes that the particle is no longer free (there are therefore no free charged particles!).

	To do this, let us take again the Hamiltonian of the Pauli equation (\SeeChapter{see section Relativistic Quantum Physics}):
	
	and let us neglect the interaction between the spin and the magnetic field such that the Hamiltonian becomes:
	
	Therefore:
	
	We thus get the following Schrödinger equation:
	
	What in comparison to the free Schrödinger equation:
	
	involves the following correspondences (these are operators):
	
	Let us consider the followoing Gauge transformation (\SeeChapter{see section Electrodynamics}) by denoting from now the electric potential by the letter $V$:
	
	where $f=f(\vec{r},t)$.
	
	First, we immediately see that the operators are invariant. Indeed, for the first one $\vec{D}$:
	
	But, if $g$ is put as being $q/\hbar$ and $f$ as $\alpha$ then we have:
	
	Thus simply:
	
	Similarly, knowing now that $f$ is $\alpha$ we have for the operator $D^0$:
	
	Therefore we have:
	
	Thus:
	
	The relation:
	
	Becomes then with the new correspondences:
	
	and with the previous developments, we have:
	
	Thus:
	
	Which gives after simplification:
	
	Thus, by requesting the Gauge invariance, we have make appear an interaction ... and we know well which interaction it is!

	The Schrödinger equation of a particle moving in an electromagnetic field is therefore invariant under the local phase transformation. The phase of a wave function is indeed a new local variable in the Weyl sense and the electromagnetic potential can be interpreted, according to Weyl, as a connection binding the phases to different points.

	We conclude that the electromagnetic field is a consequence of the local gauge invariance based on the group $\text{U}(1)$, a group of one-dimensional unitary matrices (\SeeChapter{see section Set Algebra}). The interest is to build Gauge theories on more complicated (non-abelian) groups: these theories are named "\NewTerm{Yang-Mills theories}".

	Now let us go a little further but without going too deep (as this book is only on \underline{Elementary} Applied Mathematics)... We showed above that the Lagrangian of the free Dirac equation was:
	
	Now, since this lagrangian does not reveal the electromagnetic field, we strongly suspect that it does not carry in it a local Gauge invariance...
	
	Therefore the operator $\partial_\mu$ has somewhere a lack... But, the equivalent of the divergence operator $\vec{\nabla}$ in the free Schrödinger equation is here the covariant derivative $\partial_\mu$. So in the same way that we have associated the following operators to guarantee the local Gauge invariance of the free Schrödinger equation:
	
	It is tempting to combine the whole into a new operator:
	
	with:
	
	The Lagrangian of the free Dirac equation would then be written:
	
	or in Feynman notation notation:
	
	Therefore:
	
	with:
	
	It only remains to add the term of the field to obtain the total Lagrangian of the Dirac equation (it would have been relatively hard to find it in another way ...):
	
	which corresponds to the "\NewTerm{Dirac-Maxwell equations}\index{Dirac-Maxwell equations}" and which is the "\NewTerm{Quantum electrodynamic Lagrangian}\index{Quantum electrodynamic Lagrangian}" (in natural units) where on the left we have the term of the fermions and on the right the interaction part of the bosons of zero mass (photons).

	Thus the fact of having added to the free Lagrangian a condition of invariance by local transformations, has led us to a theory with interaction that we can write with more rigor and in developed form:
	
	Or in natural units and with the charge of the electron:
	
	However, quantum electrodynamics was lacking in the 1940s to describe a large number of particles that were detected by accelerators. Certainly, in a way, it has been extended to describe new particles. But many of them seemed to enjoy properties that quantum electrodynamics could not account for.

	In fact, the reason is simple ... it is a theory in which no exact solution is known so far, a situation that persists until the day these lines are written (2008). The only available calculation method is named "perturbative development". The idea is essentially the same as that of limited development practiced in the field of differential calculus. In this case, if we do not know how to calculate the value of a function, we decompose it into a sequence of polynomials and the approximation is refined as we take into account terms of higher order. Such a series development begins with a zero order term, which is just the value of the unknown function at a certain point where we know how to calculate this function.
	
	In the case of the perturbative development of quantum electrodynamics, the zero order term represents pure propagation without interaction (the intensity of the interaction between the electron and the magnetic field is set to zero). In this approximation, quantum electrodynamics is a theory of free particles and it is exactly calculable. We have electrons, positrons and photons but they cross without influencing each other. The next term in the series development, that of the first order, is also exactly calculable. In this approximation, the theory seems to reflect the real world fairly accurately. Very interesting physical phenomena appear in this first-order approximation of the real theory of photon-electron interaction and the theory agrees well with the experimental results.

	Unfortunately, it soon became apparent that the calculation of second-order terms and higher terms seemed meaningless as they yielded infinite values... today there still exist only approximate methods of resolution and that are not completely satisfactory. Since then physicists have been obliged to look for another technique of approximation based on a renormalization of the equations... and the results are extraordinarily good (to the $11$th decimal by!) but in the facts it looks like a bit like do-it-yourself physics...
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{50} & \pbox{20cm}{\score{3}{5} \\ {\tiny 47 votes,  69.36\%}} 
	\end{tabular} 
	\end{flushright}

	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Elementary Particle Physics}
	\lettrine[lines=4]{\color{BrickRed}W}e have already mentioned in the section of Nuclear Physics that we experiments  that the radioactive nuclei do not emit neutrons or protons. But we can ask ourselves: How do they synthesize an alpha particle, or transform a proton into a neutron or vice versa? To answer these questions, let us examine the forces in presence.
	
	Before the discovery of radioactivity, physicists had identified two fundamental forces: the gravitational force and the electromagnetic force. The discovery of radioactivity and studies on the atomic nucleus led physicists to introduce not one but two new fundamental forces!
	

	\begin{figure}[H]
		\centering
		\begin{tikzpicture} [scale=0.45,>=latex, inner sep=2pt, outer sep=2pt]
			\coordinate (Newton) at (168.7, 0);
			\coordinate (Einstein) at (191.5, 0);
			\coordinate (Coulomb) at (178, -5);
			\coordinate (Maxwell) at (186.4, -7.5);
			\coordinate (Biot) at (182, -10);
			\coordinate (Feynman) at (194.9, -7.5);
			\coordinate (Becquerel) at (189.6, -15);
			\coordinate (Fermi) at (193.4, -15);
			\coordinate (Unif1) at (196.1,-11.25);
			\coordinate (Yukawa) at (193.5,- 20);
			\coordinate (Unif2) at (197.3, -15.62);
			\coordinate (Unif3) at (201, -7.81);
			\draw[*-] (Newton)  node[above right]{\small 1687 - Newton} -- (Einstein);
			\draw[*-] (Einstein)  node[above right]{ \small 1915 - Einstein}--(201,0);
			\draw[dashed] (168.7,-5)--(Coulomb);
			\draw[*-] (Coulomb) node[below]{1780} node[above]{\small Coulomb} -|  (Maxwell);
			\draw[dashed] (168.7,-10) --(Biot);
			\draw[*-] (Biot) node[below]{1820} node[above]{\small  Biot et Savart} -|  (Maxwell);
			\draw[o-*] (Maxwell)  node[below right]{1864} node[above right]{\small Maxwell} --  (Feynman);
			\draw[] (Feynman) node[below]{1949} node[above]{\small Feynman \emph{et al}}-| (Unif1);
			\draw[dashed] (168.7,-15)--(Becquerel);
			\draw[*-] (Becquerel) node[below]{1896} node[above]{\small Becquerel} --(Fermi);
			\draw[*-] (Fermi) node[below]{1934} node[above]{\small Fermi} -|  (Unif1);
			\draw[o-] (Unif1) node[below right, fill=white]{1961}  -| (Unif2);
			\draw[dashed] (168.7,-20) --(Yukawa);
			\draw[*-] (Yukawa) node[below]{1935} node[above]{\small Yukawa}   -| (Unif2);
			\draw[o-] (Unif2) node[below right]{1973}  -| (201,0);
			\draw (Unif3) node[fill=white,text width=2cm,inner sep=5pt]{Physique Unifi\'ee ?} ;
			\draw (168.7,0) node[below right]{\small \textbf{Gravitation}};
			\draw (168.7,-5) node[below right]{\small \textbf{\'Electrostatique}};
			\draw (168.7,-10) node[below right]{\small \textbf{Magn\'etisme}};
			\draw (168.7,-15) node[below right]{ \small\textbf{Interaction Faible}};
			\draw (168.7,-20) node[below right]{\small \textbf{Interaction Forte}};
		\end{tikzpicture}
		\caption{Interactions discoveries (source:  http://femto-physique.fr author: Jimmy Roussel)}
	\end{figure}
	
	
	Even before knowing the exact composition of the nuclei, to explain the existence of these tiny systems and wearing sometimes strong positive charges, physicists had foreseen the need for a powerful cohesive force capable of dominating the electrostatic repulsion exerted between these charges (remember that we saw in the section of Classical Mechanics that the gravitational force between two bodies of equivalent masses to those of particles is completely negligible). Since the nucleus is small, this "nuclear force" had to have influence at very short distances. When J. Chadwick discovered the neutron, it was shown experimentally that attractive force is also well practiced between two neutrons, two protons and between a neutron and a proton. In 1935, H. Yukawa elaborated a theory about the nuclear force whose outlines seems to be still accepted, but must still be improved following some issues that have been identified (\SeeChapter{see section Quantum Field Theory}).
	
	However, as we already know, this nuclear force does not explain the transformation of a proton into a neutron, which takes place in the beta radioactivity. It was necessary to introduce a fourth fundamental force, of lower intensity, named for this reason "\NewTerm{weak interaction}\index{weak interaction}", the nuclear force becomes ipso facto the "\NewTerm{strong interaction}\index{strong interaction}".
	
	Thus, in principle, the radioactivity involves the four fundamental forces of nature: gravity and the electromagnetic force, as alpha and beta particles have mass and charge, and the two nuclear forces, strong and weak (in fact , the gravitation, of lesser intensity than the other three is often neglected  sub-atomic scales).
	
	Thus, in principle, the radioactivity involves the four fundamental forces of nature: gravity and the electromagnetic force, as alpha and beta particles have mass and charge, and the two nuclear forces, strong and weak (in fact , the gravitation, of lesser intensity than the other three is often neglected  sub-atomic scales).
	
	We partially addressed in the section of Quantum Field Theory the fundamental interactions and their interactions vectors. Let's do some reminders:
	\begin{enumerate}
		\item We have proved in the section of Special Relativity that there are two types of particles: those with a mass and will never go at the speed of light (because it will required an infinite energy to get them at this velocity) and those which have zero mass and which are therefore necessarily at the speed of light.
		\item We proved in the section of Quantum Field Theory that more a particle has energy, that more following the Heisenberg uncertainty, it may have long virtual life time and travel considerable distances. We distinguish therefore the interaction that have infinite range of interaction whose particles have no mass and a the interactions that have finite range whose interaction particles have a mass.
	\end{enumerate}
	Before starting with some arduous calculations, it is desirable first to acquire a vocabulary that is common use among theoretical physicists.
	
	The easiest concept to be addressed in the field of elementary particle physics is the comparison of the four elemental forces via their respective coupling constant (that is something that physicists like to do...).
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Hubert Reeves and his colleagues astrophysicists proved that at the time of the genesis of the Universe, the smallest deviance in coupling constants would have caused the instability of nucleons and would have condemned cosmic evolution.
	\end{tcolorbox}
	
	\subsection{Coupling Constants}
	
	We will try here to rank the four forces according to their intensity through the use of "coupling constants".
	
	To do this, we need to calculate the four interactions for two same particles, for example two protons at identical distances, so of nuclear type and compare them to a common value of the units so that their ratio provides a dimensionless number.
	
	This common value will be chosen as the product:
	
	Thus we find:
	\begin{enumerate}
		\item For the gravitational force (\SeeChapter{see section Astronomy}) where:
		
		with the proton mass such that $m=M\cong 1.67\cdot 10^{-27}\; [\text{kg}]$, the coupling constant of the gravitational force is therefore by definition:
		
		\item For the electrical force (\SeeChapter{see section Electrostatic}) where:
		
		with the proton chart such that $q=Q\cong 1.602\cdot 10^{-19}\; [C]$, the coupling constant of the electric force is therefore by definition:
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		Here we find back the "\NewTerm{fine structure constant}\index{fine structure constant}" that we had already seen in the section of Corpuscular Quantum Physics. We therefore better understand the start choice for the relative comparison of interactions.
		\end{tcolorbox}
		\item For the strong nuclear force, where $F$ represents the "\NewTerm{strong nuclear charge}\index{strong nuclear charge}", the strong coupling constant has for value (caution! the value depends on the chosen theoretical model!):
		
		hence its name.
		\item For the weak nuclear force responsible for the decay of particles, $f$ represents the "\NewTerm{weak nuclear charge}\index{weak nuclear charge}", and it weak coupling constant value is then (caution! the value depends on the chosen theoretical model!):
		
	\end{enumerate}
	 So all this can be summarized in the following phenomenological table: 
	 %\setlength\extrarowheight{10pt}
	 
	 %\setlength\extrarowheight{0pt}
	 or also with the following diagram (more interesting) where we can see, taking into account the results we have found during our study of the massic fields and non-massic fields in the context of the Yukawa model (\SeeChapter{see section Quantum Field Theory}) : 
	\begin{enumerate}
		\item In ordinate the intensity of the forces as previously calculated according to the distance as given by the Yukawa model of massic fields (weak and strong interactions) and non-massic fields (electromagnetic and gravitational interactions).

		\item The representative interaction patterns (Feynman diagrams) in accordance with the results obtained and particles already mentioned in the section of Quantum Field Theory.
	\end{enumerate}
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=0.8]{img/atomistic/fundamental_interactions.jpg}
		\end{center}	
		\caption{Feynman diagrams and interaction distance of the fundamental forces}
	\end{figure}
	 
	 \begin{figure}[H]
		\begin{center}
		\includegraphics{img/atomistic/summary_force_carriers.jpg}
		\end{center}	
		\caption{Force Carry subparticles}
	\end{figure}
	
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=0.6]{img/atomistic/standard_model.jpg}
		\end{center}	
		\caption{Subparticles Standard Model (source: Wikipedia)}
	\end{figure}
	Above we can see the $12$ fundamental fermions and $4$ fundamental bosons. Brown loops indicate which bosons (red) couple to which fermions (purple and green). Please note that the masses of certain particles are subject to periodic reevaluation by the scientific community. The values currently reflected in this graphic are as of 2008 and may have been adjusted since. For the latest consensus, please visit the Particle Data Group website linked below.
	
	It should be precised for the general knowledge of the reader that these four forces are described by respectively four theories:
	\begin{enumerate}
		\item General Relativity (includes Classical Mechanics) for gravitation.

		\item The quantum electrodynamics (includes electrodynamics) for the electromagnetic force.

		\item The electroweak theory (which includes quantum electrodynamics) for the weak interaction

		\item The quantum chromodynamics for the strong interaction
	\end{enumerate}
	The last three are grouped in the "\NewTerm{Standard Model}\index{Standard Model}".
	
	\pagebreak
	\subsection{Spin magnetic resonance}
	We have hesitated to treat the subject of spin resonance in this section but after reflection, it's not nuclear physics because the calculations does not apply only well to the nucleus of atoms and this is not really pure relativistic quantum physics because it does not only apply well to elementary particles like electrons (as assumed in our developments in the section  of Relativistic Quantum Physics).

	During our proof of the Pauli equation in the section of Relativistic Quantum Physics using physicist art of life... we saw that for a particle of spin $\sfrac{1}{2}$ (which could be a nuclear nucleus of spin $\sfrac{1}{2}$), we had:
	
	where there is therefore a terme relatively to the spin in the Hamiltonian, that is:
	
	By putting:
	
	Being for reminder the "\NewTerm{Lande factor}\index{Lande factor}" or "\NewTerm{gyromagnetic factor}\index{gyromagnetic factor}" (\SeeChapter{see section Relativistic Quantum Physics}).
	
	Now let us dive the particle that will be named "probe" in a stable oscillating magnetic field following a horizontal plane and constant on the vertical plane of the type:
	
	where in practice the field $B_0$ goes form $0.1$ to $17$ Teslas.

	Therefore it comes:
	
	We then have by focusing only on this Hamiltonian:
	
	And as $\phi_{a0}$ is the double component of a spinor, let us write it explicitly:
	
	Thus after rearrangement:
	
	And let us recall that we proved at the very end of the section of Relativistic Quantum Physics that:
	
	It comes then naturally to write:
	
	This gives us:
	
	that is to say a system of two partial differential equation:
	
	If we assume that the spinor is orient to the top initially (initial conditions), then:
	
	Now to solve this system of differential equations we will use the laborious trial and error work already done by our predecessors by assuming that the answer is probably of the type:
	
	that is to say... wave functions!!!
	
	So by injecting these solutions in the system of differential equations we get:
	
	That is to say:
	
	and after simplification by the exponential and the pure imaginary number:
	
	Hence:
	
	We can get rid of the time dependence above by putting:
	
	Our system then reduces to:
	
	Thus by rearranging:
	
	which can be written in matrix form:
	
	To get a consistent solution (not all zero solutions) and unique one, the determinant of the matrix must be zero to be reversible (\SeeChapter{see section Linear Algebra}). But the determinant of this matrix is:
	
	But the roots of this polynomial of second degree in $\omega_{-}$ are (\SeeChapter{see section Calculus}):
	
	So we deduce:
	
	So we have for summary:
	
	Results to inject in:
	
	So as each pulsation has two solution, the general solution will be the sum of the special solutions (\SeeChapter{see section Differential and Integral Calculus}). Let us focus on the second relation. We then have:
	
	But let us recall the initial condition that we required for the second component of the spinor (due to the fact that the spinor is oriented to the top):
	
	Then we have:
	
	Therefore:
	
	with:
	
	and:
	
	Now, to determine $A$, we will use in a clever way the differential we started from earlier above:
	
	without forgetting the chosen initial conditions, we have:
	
	Therefore:
	
	We must have the equality between the both expressions:
	
	and this can be satisfied obviously if and only if:
	
	Then we have:
	
	Thus explicitly:
	
	As the second component of the spinor represents the angle and what we have above represents the angular position over time, the term:
	
	can be seen as the maximum value of the amplitude of the angle (Larmor rotation of the spin of the electron around the magnetic field). This maximum amplitude has itself a maximum if the denominator is as small as possible and therefore:
	
	Then we say that there is "\NewTerm{spin resonance}\index{spin resonance}".

	So the magnetic field may make switch the energy state of each spin if the oscillation amplitude is maximum. As we have just proved above for an isolated particle, the energy change from one state to the other is:
	
	and therefore when a spin flip it generates the emission of radiation named "\NewTerm{free induction decay}\index{free induction decay}". The collected signal depends on several parameters that characterize the particle or the nuclear core. In short, in practice it is $99\%$ engineering and $1\%$ theory!
	
	When we apply these theoretical results to an unpaired electron, that is to say, for the "\NewTerm{electronic magnetic resonance}\index{electronic magnetic resonance}", we have the following experimental data for a constant field $B_0\cong 0.3$ [T]:
	
	and therefore the electronic magnetic resonance technology is based on the detection of microwaves.
	When we apply these theoretical results to a nuclear core of a single proton, that is to say, for the "\NewTerm{nuclear magnetic resonance}\index{nuclear magnetic resonance}", we have the following experimental data for a constant field $B_0\cong 1$ [T]:
	
	and therefore the nuclear magnetic resonance technology is based on the detection of radio waves.
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{10} & \pbox{20cm}{\score{3}{5} \\ {\tiny 26 votes,  56.92\%}} 
	\end{tabular} 
	\end{flushright}