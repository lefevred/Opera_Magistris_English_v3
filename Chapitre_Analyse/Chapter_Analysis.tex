	%to force start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Analyse Fonctionelle}
	
\lettrine[lines=4]{\color{BrickRed}L}'analyse fonctionelle est une branche des mathématiques dédiée à l'études des espaces de fonctions. Elle puise se racines dans l'étude des transformations, telles que la transformée de Fourrier, des équations différentielles et des intégrales. C'est plus un champs d'étude transversal à beaucoup de domaines, qui justifie cette section du compendum. De plus, il est difficile de définir avec précision les domaines transverses, donc le lecteur trouvera le théorème du calcul fondamental dans le chapitre "Intégration et dérivation" plutôt qu'ici.

Pourquoi utiliser le terme "analyse" dans le cas particulier des fonctions ? La réponse se trouve dans lhistoire de l'étude de divers phénomènes naturels, de la résolution de divers problèmes techniques et bien sur des mathématiques, qui nous poussent à considérer la variation d'un paramètre lié à la la variation d'un autre (ou de plusieurs autres). Pour étudier les variations, il y a de multiples approches :

\begin{itemize}
	\item L'ingénieur, par exemple, utilise fréquement des graphiques (en coordonées cartésiennes, polaires ou logarithmiques... nous verrons ces concepts en détail plus tard) pour déterminer les relations mathématiques (aussi appelées "loi") reliant les variables entres elles. Assurément, ce type de méthode est (parfois...) inesthétique mais les étudiants connaissent bien la difficulté de transcrire les points mesurés sur une feuille de papier, ou un ordinateur, et les consultants connaissent pertinement les conséquences désastreuses de produire un graphe sans base scientifique. C4est une étape malheureusement nécessaire afin de comprendre comment nos prédécesseurs ont obtenus les résultats qui ont permis les avancées contemporaines de physique théorique.
	\item Le mathématicien et le physicien détestent utiliser la méthode papier crayon. Néanmoins, Le role du mathématicien ou du physicien est de développer de nouvelles théories à partir d'axiomes ou de principes sans recourir aux graphiques ou les mesures expérimentales qui leurs sont rattachées.
\end{itemize}

	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
Avant de lire la suiteil peut-être intéressant de rappeler la définition du concept de "fonction" (et des propriétés basiques inhérentes). Ceci ce trouve dans la section "Théorie des ensembles".
	\end{tcolorbox}	

	L'analyse fonctionnelle est fortement liée au calcul Vectoriel (mais pas seulement). Pour ceux qui veulent approfondir les fondamentaux de l'analyse fonctionnelle, nous recommendonc vivement de commencer par le calcul vectoriel.

\pagebreak
\subsection{Représentations}

	Nous verrons par la suite, comment représenter différentes valeurs liées avec des tableaux et graphiques (oui ! Cela aide à comprendre les choses complexes) et ensuite comment mathématiquement analiser les propriétés de ces représentations uniquement en utilisant des outils mathématiques abstraits.

	\textbf{Definition (\#\mydef):} Une fonction est appelée "\NewTerm{fonction univalente}\index{fonction univalente}" ou  "\NewTerm{fonction unitaire}\index{fonction unitaire}" si le nombre de ses arguments (parametres ou variables) vaut 1. Dans le cas d'une fonction à 2 arguments, on parle d'une "\NewTerm{fonction bivalente}\index{fonction bivalente}" ou "\NewTerm{fonction bivalente}\index{fonction bivalente}", etc. Formellement, une fonction est $n$-variante si :
	

\subsubsection{Representation Tableau}

Parmis les représentations visuelles possibles des fonctions, la plus ancienne et intuitive est celle où l'on trouve en ligne ou en colonne d'un tableau, les variables indépendantes classées $x_1,x_2,...,x_n$ et leur valeur correspondantes, nommées les "\NewTerm{variables transformées}\index{variables transformées}" de la fonction $y_1,y_2,...,y_n$ dans une autre ligne ou colonne juxtaposée :

		
	
	Dans le vocabulaire :
	
	on dit que $a_1,a_2,...,a_n$ sont les arguments "\NewTerm{arguments}\index{arguments}" of $f$.

On a, par exemple, les tables de trigonometrie, les tables de logarithmes, etc. et durant l'étude expérimentale des tableaux de certains phénomènes qui expriment la dépendance entre la quantité mesurée physiquement (telle que la lecture de températures d'une station météorologique pendant une journée) et un paramètre (l'heure de la journée).

Bien sur, ce concept peut être généralisé à n'importe quelle fonction multivariables, en regard de son domaine de définition.

Cependant, cette méthode est laborieuse et ne permet pas directement de voir le comportement de la fonction, contrairement à une simple représentation visuelle de l'analyse de ses propriétés. Cela a toujours l'avantage de ne pas faire appel à des outils spécialisés ou des mathématiques avancées.

	\pagebreak
	\subsubsection{Representation graphique}

Les entiers naturels, relatifs ou les nombres purement imaginaires (\SeeChapter{cf la section Nombres}) peuvent tous être représentés comme de simples points sur un axe infini numéroté (une ligne droite).

Pour se faire, on choisit cet axe :
	\begin{enumerate}
		\item Un point O nommé "\NewTerm{origine}\index{origine}"
		\item une direction avec un sens positif, indiqué par une flèche horizontale
		\item Une unité de mesure (souvent représentée par des petits traits verticaux : les "\NewTerm{graduations}\index{graduations}")
	\end{enumerate}
De la manière suivante :
\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/representative_1d.eps}
\caption{Exemple de représentation typique d'un axe orienté infini avec son origine}
\end{figure}
Dans la plupart des cas on pose (traditionellement) l'axe avec une direction horizontale et on choisit le sens de gauche à droite (au moins quand il y a un seul axe...).
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
Le point (lettre) $O$, représente fréquement le nombre zéro en mathématiques mais nous aurions pu choisir de placer l'origine n'importe où. Par exemple, en physique, le point $O$ est souvent positioné au centre d'un système. 
	\end{tcolorbox}
Il est évident que le fait que les ensembles de nombres, dont nous discutons à la section Nombres, soit ordonnés implique que chaque nombre est représenté par un point sur cet axe. Ainsi, deux points distincts correspondent à deux nombres différents sur l'axe.

Ainsi, la correspondance entre tous les nombre et tous les points de l'axe (dans le cas des nombres réels et complexes, il correspond non pas un nombre à chaque graduation, mais à chaque \underline{point} de l'axe !). Donc, chaque nombre représente un point ou une graduation unique et inversement, chaque point ou graduation est l'image d'un nombre.

\pagebreak
\paragraph{représentations 2D}\mbox{}\\\\

Aux représentations en une dimension, viennent s'ajouter d'autres en multiples dimensions (pfiou !) : les "\NewTerm{représentations planes}\index{représentations planes}" qui nous permettent de dessiner plus que de simples points sur une ligne droite, mais des fonctions d'une variable. Voyons à quoi cela ressemble.

Un calcul à une variable, les fonctions que l'on rencontre généralement à une variable (typiquement $x$ ou $t$) qui varie sur un sous ensemble de la ligne des nombres réels (noté $\mathbb{R}$). Pour une telle fonction, disons, $y = f (x)$, le graphe de la fonction $f$ est constitué de points.
Ces points font partie du plan Euclidien, qui, dans un système de coordonnées Cartésien ou rectangulaire, est constitué de l'ensemble des paires de nombres réels ordonnés $(a,b)$. On utilise le mot "Euclidien" à propos d'un système dans lequel les règles usuelles de géométrie Euclidienne s'appliquent. On note le plan Euclidien par $\mathbb{R}^2$. L'exposant "$2$" représente ici le nombre de dimensions du plan.

Ainsi, nous pouvons pour chaque variable $x$ de l'axe horizontal, nommé par convention "\NewTerm{axe des abscisses\index{axe des abscisses}}", correspond une valeur $y$ donnée par la fonction $f$ reportée sur un axe vertical, appelé l'axe des "\NewTerm{ordonnées}\index{ordonnées}" qui traverse l'axe des abcisses aupoint de jonction défini par l'origine $O$ :
\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/representative_2d_planar.eps}
	\caption{Exemple typique d'une représentation plane avec axes othogonaux, origine $O$ et 4 quadrants}
\end{figure}
Dans le plan, désigné par les variantes $X\text{O}Y$, $XY$ or $x\text{O}y$, $\text{O}xy$, $xy$, tous les points ayant une abscisse et une ordonnée correspondante donnée par la valeur de la fonction $f$ sont appelés "\NewTerm{graphe plan}\index{graphe plan}" de cette fonction. S'il ne peut y avoir de confusion, on utilise seulement "\NewTerm{graphe}\index{graphe}".

Dans le cas d'une représentation avec un système de coordonnées réctangulaire (cartésien, polaire ou logarithmique) comme la figure précédente, nous pouvons voir que le plan entier est divisé en 4 secteurs, nommés traditionnellement "\NewTerm{quadrants}\index{quadrants}".

	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Lorsque l'on veut marquer un point particulier du graphe représentant une fonction, on dessine la plupart du temps un petit rond au point de coordonnées $(x_n,y_n)$.
	\end{tcolorbox}	

Un autre cas classique de représentation de graphe plan, connue par un grand nombre d'étudiants, est le dessin de polynômes (\SeeChapter{cf section Calculs}) à coefficients réels ou des fonctions trigonométriques (\SeeChapter{cf section Trigonometrie}).

Another classic case of plane graph representation  known by a large number of students is the plot of polynomials (\SeeChapter{see section Calculus}) with real coefficients or trigonometric functions (\SeeChapter{see section Trigonometry}).

Indeed, to solve polynomial equations of the second degree (\SeeChapter{see section Calculus}), it is common in small classes that the teacher asks his students in addition to give an algebraic expression of the roots of:
	
given by for recall (see section Calculus for the proof):
	
a graphics resolution where the two roots (in the case where there are two distinct real roots) are given by the intersection of the parabola with the $x$-axis (of course, if the equation has no solution, there are no intersections...):

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/roots_parabola.eps}
\caption{Representation of roots on a planar graph}
\end{figure}

The graphical representation can be generalized to polynomial equations of the 3rd, 4th and 5th degree (we will prove much further, using Galois theory that it is not possible to get a general algebraic expression of the roots of a polynomial equation of the 5th degree and higher).

There is another well-known and interesting example of special graph because when most young people think that after high-school they will never do maths again, in Switzerland many employees are faced to calculate in spreadsheet softwares what we name the "coordinate wage" that is a "\NewTerm{step-wise function}\index{step-wise function}" defined in year 2013 by the government as:

	

Where $R$ is a minimal value defined also by the government as being equal to 25,800.- in 2013 and the wage is denoted by the letter $S$ (for \textbf{S}alary).

When we plot such a stepwise function with for example Maple 4.00b we get:

\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{img/analysis/step_wise_function.eps}
\caption{Example of step-wise function for swiss coordinate wage with Maple 4.00b}
\end{figure}

And therefore it is obvious thank to this chart representation that the previous definition can be simplified as:

	

	That is much easier to write in any spreadsheet software or also with Maple 4.00b:

\texttt{>R:=258000;}\\
\texttt{>plot(min(max(R/8,S-7/8*R),17/8*R,S=3/4*R..100000);}

Also, graphs are powerful qualitative tools in the field of statistics (\SeeChapter{see section Statistics}) as a starting point for data analysis (histograms, cheese, box plots, radar, scatter plots, etc.). The assumptions and ideas that are generated by graphical analysis can be investigated with advanced statistical tools (for a hundred of examples see the R Software or MATLABâ„¢ companion book).

Below for example, a graph (histogram) taken from the Industrial Engineering section that is very common in the field of statistics and project management in the global industry:

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/six_sigma.eps}
\caption{Example of typical histogram in engineering companies (Six Sigma)}
\end{figure}

Histograms allow to observe distributions and determine qualitatively if it fits a particular theoretical model.

Graphics can also be used to observe changes over time (time series, control charts, residual analysis, etc.):

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/time_serie.eps}
\caption{Example of time series with moving averages in financial trading}
\end{figure}

and still many other charts... that we have already seen and other we will see throughout the pages of this book.

\paragraph{3D representations}\mbox{}\\\\
Of course, in the case of a trivalent function (three-dimensional), that is to say a parameter which depends on two other, the idea is the same as for 2D except that the number of quadrants doubles:

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/quadrants_3d.eps}
\caption{Quadrants in a 3D orthogonal system (source: Wikipedia)}
\end{figure}

This 3D method of representation and analysis of a trivalent function was time consuming at the beginning of the 20th century but with the help of computers in the end of the 20th century this time consuming problem was almost solved...

In 3D functional analysis, we will deal with functions of two or three variables (usually  $x, y, z$, respectively). The graph of the arrow of coordinates $(x, y, z)=(x,y,f(x,y))$, lies in Euclidean space. Since Euclidean can be 3-dimensional (and more or less for sure!), we denote it by $\mathbb{R}^3$.

Euclidean space has three mutually perpendicular coordinate axes ($x$, $y$ and $z$), and three
mutually perpendicular coordinate planes: the $xy$-plane, $yz$-plane and $xz$-plane:

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/algebra/euclidian_planes.eps}
\caption{Mutually perpendicular planes in $\mathbb{R}^3$}
\end{figure}

The coordinate system shown above is known as a right-handed coordinate system, because it is possible, using the right hand, to point the index finger in the positive direction of the $x$-axis, the middle finger in the positive direction of the $y$-axis, and the thumb in the positive direction of the $z$-axis, as below:

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/algebra/right_hand.eps}
\caption{Right hand system}
\end{figure}

What we are going to represent now further below (special example), purists mathematicians would notice it as follows (it's nice to have seen at least once this notation as you could meet it in other books):
	
and let us see what it gives with  Maple 4.00b:

\texttt{>restart:}\\
\texttt{>with(plots):}\\
\texttt{>f:=(x,y)->12*x/(1+x\string^ 2+y\string^ 2);}\\
\texttt{>xrange:=-10..10;yrange:=-5..5;}\\
\texttt{>plot3d(f,xrange,yrange);}

This will give:

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/representation_grid_function.eps}
\caption{Grid representation of a 3D function with Maple 4.00b}
\end{figure}

Let us improve the visual by adding a shading interpolation color with warm color to high positions and cold colors to low positions:

\texttt{>plot3d(f,xrange,yrange, style=patchnogrid, grid=[80,50], shading=ZHUE, axes=FRAME, tickmarks=[3,3,3], labels=[`x`,`y`,`f(x,y)`], labelfont=[TIMES,BOLD,12], title=`Graphique rempli`, titlefont=[TIMES,BOLD,12], scaling=unconstrained, orientation=[-107,68]);}

This will give:

\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{img/analysis/representation_shading_interp_function.eps}
\caption{Isolines representation of a 3D function with Maple 4.00b}
\end{figure}

Let us plot now the "\NewTerm{contour lines}", also named "\NewTerm{isoline}", that represents lines of the same height on the function surface\footnote{It is a cross-section of the three-dimensional graph of the function $f(x, y)$ parallel to the $x, y$ plane. In cartography, a contour line (often just named a "contour") joins points of equal elevation (height) above a given level, such as mean sea level. A contour map is a map illustrated with contour lines, for example a topographic map, which thus shows valleys and hills, and the steepness of slopes. The contour interval of a contour map is the difference in elevation between successive contour lines.} (see section of Differential Geometry for a rigorous definition):

\texttt{>plot3d(f,xrange,yrange,style=patchcontour);}

This will give:

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/representation_isoline.eps}
\caption{Shading interpolation representation of a 3D function with Maple 4.00b}
\end{figure}

It's not very nice so let us improve this a little bit:

\texttt{plot3d(f,xrange,yrange,style=patchcontour,contours=[seq(-7+k/4,k=0..60)],\\
grid=[80,50],shading=ZHUE,axes=FRAME, tickmarks=[3,3,3],\\ scaling=unconstrained,orientation=[-107,68]);}

This will give:

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/representation_nice_3d_function.eps}
\caption{Better representation of a 3D function with Maple 4.00b}
\end{figure}

With a small rotation to view from above:

\texttt{>plot3d(f,xrange,yrange, style=patchcontour, contours=[seq(-7+k/4,k=0..60)], grid=[80,50], shading=ZHUE, axes=FRAME, tickmarks=[3,3,3], scaling=unconstrained, orientation=[-90,0]);}

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/representation_nice_3d_function_above.eps}
\caption{Above representation of a 3D function with Maple 4.00b}
\end{figure}

And in section view (side view):

\texttt{>plot(f(x,2),x=xrange);}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{img/analysis/representation_nice_3d_function_section.eps}
\caption{Representation of a section of the pseudo-3D surface}
\end{figure}

Or with multiple sections views:

\texttt{>display([seq(plot(f(x,y),x=xrange),y=yrange)]);}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{img/analysis/representation_nice_3d_function_multiple_sections.eps}
\caption{Representation of multiple sections of the pseudo-3D surface}
\end{figure}

The reader can also animate the graph above with the following command:

\texttt{>display([seq(display([plot(f(x,k/5),x=xrange),}\\ \texttt{textplot([6,5,cat('y=',convert(evalf(k/5,2),string))],font=[TIMES,BOLD,16])])}\\
\texttt{,k=-25..25)],insequence=true, title='Animation',titlefont=[TIMES,BOLD,18]);}

That's all for typical and simple example of standard manipulations of an engineer hired in a company and using graphics (in practice it will instead use MATLABâ„¢ instead of Maple but the reader can refer to the free companion book on MATLABâ„¢ with a few hundreds of pages graphics).

\paragraph{2D Vector representations}\mbox{}\\\\
It is also frequently made use of graphic representations in the context of analytical geometry to simplify analysis or to prove theorems witht the help of visual representations (do not abuse of this method!).

Thus, we can easily introduce the concept of norm (see section Vector Calculus) in a very easy way by plotting the distance between two points (in 2D or in 3D) and applying the Pythagorean theorem that will be assumed to be known (see section Euclidean geometry).

The main idea of a planar vector representation in physics and engineering labs is that  a point $P_1$ of coordinates $(x_1,y_1)$ that has some physical properties (typically a velocity) will be after a given time at the point $P_2$ of coordinates $(x_2,y_2)$ supposed to be always in the same plane. In this way, the straight line between $P_1$ and $P_2$ is a visualization of the "intensity" of the velocity (and implicitly of the force). When doing that for many points we get a planar representation of a planar vector field (for more example see the companion book on MATLABâ„¢):

\begin{figure}[H]
\centering
\includegraphics{img/analysis/vector_field.jpg}
\caption[]{Typical planar vector field with MATLABâ„¢}
\end{figure}


Now let us represent three points $P_1,P_2,P_3$ on a plane graph in which has been defined a referential as presented below:

\begin{figure}[H]
\centering
\includegraphics{img/analysis/vector_plane.jpg}
\caption[]{Scenario of three points in a plane}
\end{figure}

We can consider the straight line $\overline{P_1P_2}$ as a vector but not translated at the origin of the referential (\SeeChapter{Vector Calculus}).

If $x_1\neq x_2$ and $y_1\neq y_2$ (as in the figure above), the points $P_1,P_2,P_3$ are the vertices of a perpendicular triangle. By applying the Pythagorean theorem (\SeeChapter{see section Euclidean Geometry}) we can easily calculate the metric distance $d$ as:
	
	On the figure, we see that:
	
	Since $\forall x \in \mathbb{R} \; \vert x \vert ^2 =x^2$, we can write:
	
	If $x_1=y_1=0$, we end up with a relation named "\NewTerm{norm}", "\NewTerm{module}" or "\NewTerm{distance}" that we have already defined as part of our study of vector calculus when the origin of the vector is translated on the origin of the referential (see section of the corresponding name).
	
	\begin{theorem}
	Obviously, if we consider two points $P_1(x_1,y_1),P_2(x_2,y_2)$, we can determine if a third point $P_3(x_3,y_3)$ is on the mediator (\SeeChapter{see section Euclidean Geometry}) of the first two and for this that it is obviously sufficient that (by definition of the mediator!):
	
	\end{theorem}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We hesitated to put this proof in the section of Analytical Geometry but at then end we have decided that it was a nice example of showing how visual representation can help readers to better understand some subjects.
	\end{tcolorbox}	
	\begin{dem}
	As $(x_1,y_1),(x_2,y_2)$ are known, we can easily express an "\NewTerm{analytic expression}" property of the mediator that is that for each point on the mediator we have:
	
	where $a, b$ are therefore constants and wherein any point that satisfies this relation, which is in this case the equation of a straight line, lies on the mediator.
	\begin{flushright}
		$\square$  Q.E.D.
	\end{flushright}
	\end{dem}
	Furthermore, it is easy to see that the midpoint of the line segment that coincide with the mediator is given by:
	
	So we see that with a simple visual representation, we can achieve results that are sometimes (...) more obvious for students.
	
	Let us use this example to define some concepts on that we will come back further and do some reminders.
	
	\textbf{Definition (\#\mydef):} Any function of the form of a polynomial (\SeeChapter{see section Calculus}) of degree $1$ with constant real coefficients:
	
	is the analytic expression of what we name a "\NewTerm{straight line}" of "\NewTerm{slope}" $a$ and "intercept" $b$ (when $x=0$).
	
	Obviously, if:
	
	the line is horizontal if we graphically represent it since $y$ is constant for all $x$ and is equal to $b$. Conversely, if:
	
	the straight line will be vertical in the $x\text{O}y$ referential.
	
	\paragraph{Properties of visual representations}\mbox{}\\\\
	Depending on the type of graph we visualize (especially graphics planes) it is possible to extract some basic properties. Let us see the most important one to know for univariate functions:
	
	\begin{enumerate}
		\item[P1.] The graph of a function is "\NewTerm{symmetrical about the $y$-axis}\index{graph symmetric about the $y$-axis}" if the change in from $x$ to  $-x$ in the function does not change the value of $y$ such that:
		\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/function_property_symetry_y.jpg}
		\caption{Example of symmetry through the $y$-axis of a function}
		\end{figure}
		
		\item[P2.] The graph of a function is "\NewTerm{symmetrical about the $x$-axis}\index{graph symmetric about the $x$-axis}" if the change from $y$ to $-y$ does not change the value of $x$ such that:
		\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/function_property_symetry_x.jpg}
		\caption{Example of symmetry through the $x$-axis of a function}
		\end{figure}
		
		\item[P3.] The graph of a function is "\NewTerm{symmetrical about the origin $\text{O}$}\index{graph symmetrical about the origin}" if the simultaneous change of $y$ to $-y$ and from $x$ to $-x$ gives the following result (that is to say that the change in the sign of one variable change the sign of the other):
		\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/function_property_symetry_o.jpg}
		\caption{Example of symmetry through the origin $\text{O}$ of a function}
		\end{figure}
		
		\item[P4.] Given a function $y=f(x)$, if we add a constant $c^{te} \geq 0$ to this function as:
		
		then the function $f(x)$ is shifted (or "translated") vertically upwards of a distance $c^{te}$ as presented in the figure below:
		\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/function_property_positive_translated.jpg}
		\caption{Example of a positive vertical translation of a function}
		\end{figure}
		And conversely if $c^{te} \geq 0$ but:
		
		then the function $f(x)$  is obviously translated vertically downwards:
		\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/function_property_negative_translated.jpg}
		\caption{Example of a negative vertical translation of a function}
		\end{figure}
		We can also consider horizontal translations of functions. Specifically, if we have still $c^{te}$, then $y=f(x)$ is translated horizontally to the right if we write:
		
		which graphically is represented by:
		\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/function_property_negative_horizontal_translated.jpg}
		\caption{Example of negative horizontal translation of a function}
		\end{figure}
		and conversely, translated horizontally to the left, if we write:
		
			as shown in the graph below:
		\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/function_property_positive_horizontal_translated.jpg}
		\caption{Example of positive horizontal translation of a function}
		\end{figure}
		To stretch or compress vertically a function, we simply multiply $y=f(x)$ by a constant $c^{te}>1$ and respectively $0\leq c^{te}<1$ as:
		
		and don't forget that if a function is linear then we have the special property $f(\lambda x)=\lambda f(x)$.
		This is graphically represented for the case $c^{te}>1$ by:
		\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/function_property_upscaled.jpg}
		\caption{Example of vertical stretch of a function}
		\end{figure}
		and when $0\leq c^{te}<1$ by:
		\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/function_property_downscaled.jpg}
		\caption{Example of vertical compression of a function}
		\end{figure}
		To stretch or compress a function horizontally, by the same way, we just need to multiply the variable $x$ by a constant by a constant $c^{te}>1$ and respectively $0\leq c^{te}<1$ as:
		
		This is graphically represented for the case $c^{te}>1$ by:
		\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/function_property_upscaled_horizontal.jpg}
		\caption{Example of horizontal stretch of a function}
		\end{figure}
		and when $0\leq c^{te}<1$ by:
		\begin{figure}[H]
			\centering
			\includegraphics{img/analysis/function_property_downscaled_horizontal.jpg}
			\caption{Example of horizontal downscale of a function}
		\end{figure}
	\end{enumerate}
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Translate, stretch, compress a function or apply it a symmetry is transforming it. The plot resulting from these transformations is named the "\NewTerm{transformed}\index{transformed graph}" from the initial plot.
	\end{tcolorbox}	
	
	\textbf{Definitions (\#\mydef):} We say that a function $f$ is (we simplify the definition using an univariate function):
	\begin{itemize}
		\item[D1.] A function is a "\NewTerm{constant function}\index{constant function}" on an interval $I$ if for each pair $(x_1,x_2)$ of elements of $I$ such that $x_1\neq x_2$, we have $f(x_1)=f(x_2)$. What we denote in a condensed manner by:
		
		
		\item[D2.] A function is an "\NewTerm{increasing function}\index{increasing function}" or an "\NewTerm{increasing function in the broadest sense}" on the interval $I$ if for each pair $(x_1,x_2)$ of elements of $I$ such that $x_1\leq x_2$, we have $f(x_1)\leq f(x_2)$. What we denote in a condensed manner by:
		
		
		\item[D3.] A function is an "\NewTerm{decreasing function}\index{decreasing function}" or an "\NewTerm{decreasing function in the broadest sense}" on the interval $I$ if for each pair $(x_1,x_2)$ of elements of $I$ such that $x_1\leq x_2$, we have $f(x_1)\geq f(x_2)$. What we denote in a condensed manner by:
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		A function is a "\NewTerm{monotonic function}\index{monotonic function}" or "\NewTerm{monotonic function in the broadest sense}" on an interval $I$ if it is increasing or decreasing in this interval.
		\end{tcolorbox}
		
		\item[D4.] A function is a "\NewTerm{strictly increasing function}\index{strictly increasing function}"  on the interval $I$ if for each pair $(x_1,x_2)$ of elements of $I$ such that $x_1\leq x_2$, we have $f(x_1)< f(x_2)$. What we denote in a condensed manner by:
		
		
		\item[D5.] A function is an "\NewTerm{strictly decreasing function}\index{strictly decreasing function}"  on the interval $I$ if for each pair $(x_1,x_2)$ of elements of $I$ such that $x_1\leq x_2$, we have $f(x_1)> f(x_2)$. What we denote in a condensed manner by:
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		A function is a "\NewTerm{strictly monotonic function}" on an interval $I$ if it is strictly increasing or decreasing in this interval.
		\end{tcolorbox}
	\end{itemize}
	
	\subsubsection{Analytical Representation}
	The analytical method of representation is by far the most used and consists of representing any function in an "\NewTerm{analytic expression}\index{analytic expression}" or "\NewTerm{closed form}\index{closed form}" which is a symbolic and abstract mathematical notation of all known mathematical operations that must be applied in a certain order to numbers and letters expressing constants or variables that we seek to analyze.
	
	Note that by "all known mathematical operations", we consider not only the mathematical operations seen in the chapter of Arithmetics (addition, subtraction, root extraction, etc.) but also all the operations that will be defined later in this book.
	
	If the functional dependence $y=f(x)$ is such that $f$ is an analytic expression, then we say that the "\NewTerm{function $y$ of $x$}" is "given analytically ". 

	Here are some examples of simple analytical expressions:
	
	When we have determined the equation of the mediator, we have obtained an analytical expression of the visual straight line that characterize it as a function of the type:
	
	which we recall, is the analytical expression of the equation of a straight line, also named "\NewTerm{linear equation}\index{linear equation}" or "\NewTerm{affine function}\index{affine function}", on a plane where two points are known $P_1(x_1,y_1),P_2(x_2,y_2)$, the slope is given by the ratio of vertical growth on the horizontal growth as:
	
	A friendly and trivial application is to prove analytically that two non-vertical lines are parallel if and only if they have the same slope. Thus, given two lines with the equations:
	
	The lines intersect at a point $(x, y)$ if and only if values of $y$ are equal for a certain $x$, that is to say:
	
	The last equation can be solved with respect to $x$ if and only if $a_2-a_2\neq 0$. We have therefore proved that the lines $y_1,y_2$ intersect if and only if $a_1\neq a_2$. Therefore, they do not intersect (are parallel) if and only if $a_1=a_2$.
	
	In a quite simple way by applying the Pythagorean theorem, it is not difficult (\SeeChapter{see section Analytical Geometry}) to determine the equation of a circle with center $C (h, k)$ has for equation (it is of use in mathematics not explain $y$ for the equation of the circle therefore the equation of the latter is much more visually aesthetic and speaking):
	
	In these examples the functions are expressed analytically by a single formula (equality between two analytical expressions) which defines at the same time the "natural domain of definition" of the functions.
	
	\textbf{Definition (\#\mydef):} The "\NewTerm{natural domain of definition}\index{natural domain of definition}" of a function given by an analytical expression is the set of $x$ values for which the expression on the right-hand side has a definite value.
	
	For example the function:
	
	is defined for all values of $x$ except the value $x=1$ where we have a singularity (division by zero).
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	There are an infinite number of functions and we can not expose them all here, however we will meet more than a thousand on this entire book and should amply suffice to get an idea of their study.
	\end{tcolorbox}
	
	And we have the famous following table of variations that is also considered as an analytical tool and also used by some teach to study the basics of the derivative $f'$ of a function $f$ (\SeeChapter{see section Differential and Integral Calculus}). For example with the function $x^3-3x^2+2$ (already seen in the previous mentioned section):

	\begin{minipage}{\linewidth}\centering
    \begin{variations}
     x      & \mI &    & 0 &    & 2 &    & \pI  \\
     \filet
     f'     & \ga +    & 0    &  -  &  0   & \dr+      \\
     \filet
     \m{f}  & ~  & \c  & \h{~} & \d & ~    &  \c       \\
     \end{variations}
	\end{minipage} 	
	
	Whose corresponding plot is:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/variation_plot_example.jpg}
		\caption[]{Plot of  function $x^3-3x^2+2$}
	\end{figure}
	
	\pagebreak
	\subsection{Functions}
	In mathematics, a function is a relation between a set of inputs and a set of permissible outputs with the property that each input is related to exactly one output.
	
	Functions of various kinds are the central objects of investigation in most fields of modern mathematics. There are many ways to describe or represent a function. Some functions may be defined by a formula or algorithm that tells how to compute the output for a given input. Others are given by a picture, named the "graph" of the function. In science, functions are sometimes defined by a table that gives the outputs for selected inputs. A function could be described implicitly, for example as the inverse to another function or as a solution of a differential equation.
	
	First remember the definitions already given earlier during our study of graphical representation of functions:
	
	\textbf{Definitions (\#\mydef):} We say that a function $f$ is (we simplify the definition using an univariate function):
	\begin{itemize}
		\item[D1.] A function is a "\NewTerm{constant function}\index{constant function}" on an interval $I$ if for each pair $(x_1,x_2)$ of elements of $I$ such that $x_1\neq x_2$, we have $f(x_1)=f(x_2)$. What we denote in a condensed manner by:
		
		
		\item[D2.] A function is an "\NewTerm{increasing function}\index{increasing function}" or an "\NewTerm{increasing function in the broadest sense}" on the interval $I$ if for each pair $(x_1,x_2)$ of elements of $I$ such that $x_1\leq x_2$, we have $f(x_1)\leq f(x_2)$. What we denote in a condensed manner by:
		
		
		\item[D3.] A function is an "\NewTerm{decreasing function}\index{decreasing function}" or an "\NewTerm{decreasing function in the broadest sense}" on the interval $I$ if for each pair $(x_1,x_2)$ of elements of $I$ such that $x_1\leq x_2$, we have $f(x_1)\geq f(x_2)$. What we denote in a condensed manner by:
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		A function is a "\NewTerm{monotonic function}\index{monotonic function}" or "\NewTerm{monotonic function in the broadest sense}" on an interval $I$ if it is increasing or decreasing in this interval.
		\end{tcolorbox}
		
		\item[D4.] A function is a "\NewTerm{strictly increasing function}\index{strictly increasing function}"  on the interval $I$ if for each pair $(x_1,x_2)$ of elements of $I$ such that $x_1\leq x_2$, we have $f(x_1)< f(x_2)$. What we denote in a condensed manner by:
		
		
		\item[D5.] A function is an "\NewTerm{strictly decreasing function}\index{strictly decreasing function}"  on the interval $I$ if for each pair $(x_1,x_2)$ of elements of $I$ such that $x_1\leq x_2$, we have $f(x_1)> f(x_2)$. What we denote in a condensed manner by:
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		A function is a "\NewTerm{strictly monotonic function}\index{strictly monotonic function}" on an interval $I$ if it is strictly increasing or decreasing in this interval.
		\end{tcolorbox}
	\end{itemize}
	And let us add now complementary definitions:
	
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] We say that $y$ is a function of $x$ and we will write $y=f(x),y=\varphi(x)$, etc., if for every value of the variable $x$ belonging to a certain domain of definition (set) $D$, a corresponds a value of the variable $y$ in another domain of definition (set) $E$. What we denote in various ways (the third one being the most recommended):
		
		The variable $x$ is named "\NewTerm{independent variable}\index{independent variable}" or "\NewTerm{input variable}" or even "\NewTerm{exogenous variable}\index{exogenous variable}" and $y$ the "\NewTerm{dependent variable}\index{dependent variable}" or "\NewTerm{endogenous variable}\index{endogenous variable}".
		
		The dependence between the variables $x$ and $y$ is named a "\NewTerm{functional dependency}\index{functional dependency}". The letter $f$, which in the symbolic notation of functional dependence, indicates that we need to apply some operations to $x$ to obtain the corresponding $y$ value.
		
		Sometimes we write:
		
		rather than:
		
		In the latter case the letter $y$ expresses at the same time the value of the function and the symbol of operations applied to $x$.
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		As we saw it during our study in the section Set Theory, an application (or function) may be injective, surjective or bijective: 
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.75]{img/analysis/functions_type.jpg}
			\caption{Quick summary of applications/functions types}
		\end{figure}
		It is therefore necessary that the reader for whom these concepts are unknown goes in priority read these definitions.
		\end{tcolorbox}
		
		\item[D2.] The set of $x$ values (inputs) for which the value of the function $y$ is given by the function $f (x)$ is named the "\NewTerm{range of existence}\index{range of existence}" of the function or "\NewTerm{domain of definition}\index{domain of definition}" of the function.
		
		The set of ouputs of $f(x)$ is named the "\NewTerm{image}\index{image}" or sometimes the "\NewTerm{co-domain}\index{co-domain}". When study of the point of view of the knowledge of the output values only, the set of $x$ is named the "\NewTerm{pre-image}".
		
		\item[D3.] A function $f(x)$ is named a "\NewTerm{periodic function}\index{periodic function}" if there is a constant $c^{te}$ such that the function's value does not change when we add (or subtract we) that constant to the independent variable such as:
		
		which corresponds to a translation along the $x$-axis. The smallest constant satisfying this condition is named the "\NewTerm{period}\index{period}" of the function. It is frequently denoted by the letter $T$ in physics.
		
		The most common periodic functions know by students and engineers are the trigonometric functions (see section of the corresponding name):
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.4]{img/analysis/periodic_function.jpg}
			\caption{Example of periodic function with period and amplitude}
		\end{figure}
		 \begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		The sum of two periodic functions with different periods is not necessarily periodic and there is no general formula to get the period of a function that is the sum of $n$ other functions!
		\end{tcolorbox}
		
		\item[D4.] In differential calculus (\SeeChapter{see section Differential and Integral Calculus}), the expression:
		
		with $h\neq 0$ is of particular interest. We name it a "\NewTerm{growth quotient}\index{growth quotient}" (we discuss this in much more detail in our study of differential and integral calculus).
		
		\item[D5.] We use certain properties of functions for easy graphical representation and analysis or mathematical simplifications. In particular, a function $f (x)$ is named "\NewTerm{even function}\index{even function}" if:
		
		for all $x$ in its definition domain.
		
		That is to say as we already seen previously, it is symmetric relatively with the $y$-axis:
		\begin{figure}[H]
			\centering
			\includegraphics{img/analysis/function_property_symetry_y.jpg}
			\caption{Example of even function}
		\end{figure}
		A function $f (x)$ is named "\NewTerm{odd function}\index{odd function}" if:
		
		for all $x$ in its definition domain.
		
		That is to say as we already seen previously, it is symmetric relatively with the origin:
		\begin{figure}[H]
			\centering	\includegraphics{img/analysis/function_property_symetry_o.jpg}
			\caption{Example of odd function}
		\end{figure}
		So, to summarize, an even function is a function that is independent of the sign of the variable and an odd function change of sign when we change the sign of the variable (the spiral of Cornus in the section Civil Engineering is a good practical example of odd function). This concept will be very useful to us to simplify some very useful developments in physics (such as Fourier transforms of odd or even functions for example, or the calculation of certain integrals!).
		\begin{theorem}
		Remember that this type theorem linking a general concept to a particular case and its opposite is often found in mathematics. We will see such examples in tensor calculus with the symmetric and anti-symmetric tensor (\SeeChapter{see section Tensor Calculus}) or in quantum physics with the Hermitian or non-Hermitian operators (\SeeChapter{see section Wave Quantum Physics}).
		\end{theorem}
		\begin{dem}
		Let us write:
		
		Then:
		
		If we sum then we get:
		
		and by subtracting:
		
		So there is really and odd and even decomposition of any function!!!
		\begin{flushright}
			$\square$  Q.E.D.
		\end{flushright}
		\end{dem}
		Finally, it is important to note that:
		\begin{itemize}
			\item The product of two even functions is an even function
			\item The product of two odd functions is an even function
			\item The product of an even and odd function is an odd function
		\end{itemize}
		Let us see a short proof of the last property because we will need it in the chapter on Geometry.
		\begin{dem}
		Let $g(x)$ be an even function and $h(x)$ an odd function such as:
		
		Therefore:
		
		\begin{flushright}
			$\square$  Q.E.D.
		\end{flushright}
		\end{dem}
		\item[D6.] In general, if $f (x)$ and $g (x)$ are arbitrary functions, we use the terminology and notations given in the following table:
		
		The definition domains of $f+g,f-g,f\cdot g$ are the intersection $I$ of the definition domain of $f (x)$ and g $(x)$, that is to say, the numbers which are common to both domains of definition. The domain of definition of $g/g$ est meanwhile the subset $I$ of all $x$ such that  $g(x)\neq 0$.
		
		\item[D7.] Let $y$ be a function of $f$ of $u$ such that $y=f(u)$ and $u$ a function $g$ of $x$ such that $u=g(x)$, then $y$ depends on $x$ and we have what we name a "\NewTerm{composite function}\index{composite function}" that we denote:
		
		The last equality should be read "\NewTerm{$f$ round $g$}" and not confuse with the "round" symbol with the notation of the scalar product that we have seen during our study of the section Vector Calculus.
		
		The domain of definition of the composite function is either identical to the entire domain of definition of the function $u=g(x)$ or the part of the domain in which the values of $u$ are such that the corresponding values $f (u)$ belong to the domain of definition of this function.
		
		Obviously the principle of composite function can be applied not only once, but an arbitrary number of times such that $y=f(g(h(t)))$ and so on...
		
		In computing science a function may compose with itself a given number of times $n$, such that $f(f(f(f(f...)))))=f^n$ that must not be confuse with the notation $f^2(x)$.
		
		If $u$ does not depend on another variable (or it is not itself a composite function), then we say that $f(x)$ is an "\NewTerm{elementary function}\index{elementary function}".

		Obviously there are an infinite number of elementary functions but most can be classified into families whose expression is similar to one of the following:
		
		\begin{itemize}
			\item "\NewTerm{Linear functions}\index{linear function}":
			
			The are simply functions representing straight lines of slope $a$ passing through the origin of the axis.
			
			\item "\NewTerm{Affine functions}\index{affine function}":
			
			The are simply functions representing straight lines of slope $a$ passing through the origin of the axis or not (linear function with a translation).
						
			\item "\NewTerm{Power functions}\index{power function}":
			
			where $m\in\mathbb{R}$. Functions involving roots are often named "\NewTerm{radical functions}\index{radical functions}".
			\begin{figure}[H]
				\centering
				\includegraphics{img/analysis/power_function.jpg}
				\caption{Different plots of simple power functions}
			\end{figure}
		
			\item "\NewTerm{Absolute value functions}\index{absolute value function}" (see section Arithmetic Operators for the definition and the study of the "absolute value"):
			
			For example the plots with Maple 4.00b that we get with the command:\\
			
			\texttt{>plot([(x),(cos(x)),(x\string^2-3),(x\string^3-4*x\string^2+2*x)],x=-6..6,y=-4..3,\\
			thickness=3);}	
			
			\begin{figure}[H]
				\centering
				\includegraphics{img/analysis/pre_absolute_plot_functions.jpg}
			\end{figure}
			
			and taking the absolute value:\\
			
			\texttt{>plot([abs(x),abs(cos(x)),abs(x\string^2-3),abs(x\string^3-4*x\string^2+2*x)]\\
			,x=-6..6,y=-0.5..3,thickness=3);}
			\begin{figure}[H]
				\centering
				\includegraphics{img/analysis/post_absolute_plot_functions.jpg}
			\end{figure}
		
		\item "\NewTerm{Exponential functions}\index{exponential function}":
			
			where the famous $e^x$ is only a special case and also $a\in\mathbb{R}$.
			
			When $a\geq 0$ we have typically:
			\begin{figure}[H]
				\centering
				\includegraphics{img/analysis/exponential_functions.jpg}
				\caption{Different plots of simple exponential functions $(1^2,2^x,3^x,4^x)$ with Maple 4.00b}
			\end{figure}
			where $m$ is a positive number different from $1$ (otherwise it is simple a linear function):
			
			If $a<0,x\in\mathbb{R}$ the function is not defined. Indeed for $(-1)^(0.5)=\left\lbrace \mathrm{i},	-\mathrm{i}	\right\rbrace$ therefore it is an application from $f:\mathrm{R}\mapsto\mathbb{C}^2$ and as far as we know there is no nice way to represent it visually and anyways this is not a function in the traditional way.
			
			\item "\NewTerm{Logarithmic functions}\index{logarithmic function}":
				
			with $a\in\mathbb{R}^{+}$ and that by construction of the logarithm (see further below) are of the type $f:\mathbb{R}^{+}\mapsto \mathbb{R}$.
			We have typically::
			\begin{figure}[H]
				\centering
				\includegraphics{img/analysis/logarithm_functions.jpg}
				\caption{Different plots of logarithm $\ln(x)=\ln_e(x)$ in green and $\log_10(x)$ in red with Maple 4.00b}
			\end{figure}
			
			\item "\NewTerm{Periodic/Trigonometric functions}\index{period function}\index{trigonometric function}":
			
			We already defined previously what is a periodic function. For the trigonometric function the reader can see below a plot of the main one but for more details it is strongly recommended to read the section Trigonometry:
			\begin{figure}[H]
				\centering
				\includegraphics[scale=0.9]{img/analysis/trigonometric_functions.jpg}
				\caption{Different plots of trigonometric functions with Maple 4.00b}
			\end{figure}
			
			\item "\NewTerm{Polynomial functions}\index{polynomial function}":
			
			
			where as we already know $a_0,a_1,...,a_n$ are constant numbers named "\NewTerm{coefficients}\index{coefficients}" and $n$ is a positive integer that we name "\NewTerm{degree of the polynomial}\index{degree of a polynomial}". Obviously this function is defined for all values of $x$, that is to say, it is define on an infinite interval.
			
			If follows that functions the power functions of the type $x^m$ and linear functions of the type $f(x)=x$ are a subclass of polynomial for $m\in \mathbb{N}$.
			
			We have already study more deeply polynomials in the section Calculus with their main properties but let us give us again the plot of some of them as recall: 
			\begin{figure}[H]
				\centering
				\includegraphics{img/algebra/polynomials.jpg}
				\caption{Some polynomials plotted with R.3.2.1 (see by book on R)}
			\end{figure}
			We will see and study in this book some famous polynomials as: Legendre polynomials (
			
			\item "\NewTerm{Rational fractions}\index{rational fractions}" are polynomials divisions (\SeeChapter{see section Calculus}):
			
			\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
			Obviously two rational fractions are equal, if one is obtained from the other by multiplying the numerator and denominator by the same polynomial.
			\end{tcolorbox}
			The rational function:
			
			is not defined at $x^2=5 \Leftrightarrow x=\pm \sqrt{5}$. It is asymptotic (see further below) to $\frac{x}{2}$ as x approaches infinity:
			\begin{figure}[H]
				\centering
				\includegraphics{img/analysis/rational_function.jpg}
				\caption{Example ration function $f(x) = \frac{x^3-2x}{2(x^2-5)}$}
			\end{figure}
			The rational function:
			
			 is defined for all real numbers, but not for all complex numbers, since if x were a square root of -1 (i.e. the imaginary unit or its negative), then formal evaluation would lead to division by zero!
			 
			 A constant function such as is a rational function since constants are polynomials. Every polynomial function $f(x) = P(x)$ is a rational function with $Q(x) = 1$. The power functions $f(x)=x^m$ are also rational functions when $m\in\mathbb{N}$.
			 
			 \item "\NewTerm{Algebraic functions}\index{algebraic function}" are defined by the fact that the function $f(x)$ is the result of addition, subtraction, multiplication, division, of variables put to an integer or non-integer power. Therefore most of the functions defined previously can be included in this definition: linear functions, affine function, power function, polynomial function, rational functions.
			 
			 \item A "\NewTerm{piecewise function}\index{piecewise function}" is a function defined by different formulas on different parts of its domain. The absolute value is a famous example of a piecewise-defined function because the formula changes with the sign of $x$:
			 
			 
			 \item A "\NewTerm{step function}\index{step function}" $f:[a,b]\in \mathbb{R}$ is defined if and only if there exists a subdivision $(a_i)_{0\leq i \leq n}$ of $[a, b]$ such that $a_0=a$ and $a_n=b$ and $(\lambda_0,...,\lambda_n)\in \mathbb{R}^n$ such as:
			 
			\begin{figure}[H]
				\centering
				\includegraphics{img/analysis/step_function.jpg}
				\caption{Example of step function}
			\end{figure}
			Such functions can be found in signal processing and also in statistics for survival analysis.
		\end{itemize}
		
		However, there are a very large number of other elementary functions that will meet in the individual sections of this book. Examples include the "Bessel functions" (\SeeChapter{see section Sequences and Series}), the "Lipschitz functions" (\SeeChapter{see section Topology}), the "Dirac functions" (\SeeChapter{see section Differential and Integral Calculus}), the "distribution functions" (\SeeChapter{see section Statistics}), the "Euler gamma function" (\SeeChapter{see section Differential and Integral Calculus}), etc. The reader will notice that the Dirac function also belongs to the family of distribution functions.
	\end{enumerate}
	
	\subsubsection{Limits and Continuity of Functions}
	We will now consider ordered variables of a special type, which we define by the relation "the variable tends to a limit." In what will follow, the concept of limit of a variable will play a fundamental role, being intimately related to the basic notions of mathematical analysis, derivatives, integrals, etc.
	
	\textbf{Definition (\#\mydef):} The number $a$ is named the "\NewTerm{limit}\index{limit}" of variable magnitude $x$, if for any arbitrarily small positive number $\varepsilon$ we have:
	
	If the number $a$ is the limit of the variable $x$, we say that "\NewTerm{
$x$ tends to the limit $a$}".

	We can also define the concept of limit from geometrical considerations (this can help to better understand ... but not always ...):
	
	The constant number $a$ is the limit of the variable $x$, if for any given neighborhood, no matter how small, of center $a$ and of radius $\varepsilon$, we can find a value $x$ such that all the points corresponding to the following values of the variable belong to this neighborhood (notions that we defined earlier). We represent geometrically this as:
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/limit_geometric_representation.jpg}
		\caption{Geometric concept of limit in $\mathbb{R}^1$}
	\end{figure}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	\textbf{R1.} It should be trivial that the limit of a constant value is equal to this constant, since the inequality $|x-c^{te}|=|c^{te}-c^{te}|=0<\varepsilon $ is always satisfied for an arbitrary $\varepsilon>0$.\\
	
	\textbf{R2.}  Not all variable have limits. For example $y=\sin(x)$ as this trigonometric function fluctuates between $[-1,+1]$ from $[-\infty,+\infty]$.
	\end{tcolorbox}
	
	\textbf{Definition (\#\mydef):} A variable $x$ tends to infinity if for any positive chosen $M$, we indicate one value of $x$ from which all successive values of the variable $x$ (vaules in the neighborhood of the previous choosen value) satisfy the inequality $|x|>M$. Formally:
	
	\begin{itemize}
		\item A variable $x$ "\NewTerm{tends to $+\infty$}" if for any positive chosen $M>0$, we indicate one value of $x$ from which all the successive values of the variables $x$ satisfies the inequality $M<x$.
	
		It is typically the type of consideration that we have for divergent sequences (divergent to infinity) where for a given term of value $M$ of the sequence all the other terms are greater ant $M$.
		
		
		\item A variable $x$ "\NewTerm{tends to $-\infty$}" if for any negative chosen $M<0$, we indicate one value of $x$ from which all the successive values of the variables $x$ satisfies the inequality $x<M$.
		
	\end{itemize}
	\textbf{Definition (\#\mydef):} Given $y=f(x)$ a function defined in a neighborhood of $a$ or on some point of this neighborhood. The function $y=f(x)$ tends to the limit $b$ (that is to say $y\rightarrow b$) when $x$ tends to $a$ (that is to say $x a$) if for any positive number $\varepsilon$ as small as possible, we can indicate  positive number $\delta$ such that all $x$ different from $a$ satisfying the inequality $|x-a|<\delta$ also satisfy $|f(x)-b|<\varepsilon$. Formall ya function has a limit $b$ on $a$ when in a domain $E$ if:
	
	The inequality $|x-a|<\delta$ gives the possibility to have the distance from which we come with our $x$ without taking care of the direction (left or right) as we take for measurement of distance the absolute values. Indeed on a system of axis representing ordinates values, we can, for a given value, coming from the left or from the right (if necessary you can imagine a bus coming to a bus stop that can from the left or from the right only since the absolute distance from it to the bus stop is less than or equal to $\delta$.
	
	If $b$ is the limit of the function $f (x)$ when $x\rightarrow a$ we then write in this book in any case:
	
	Obviously the above definition is available when $a=\pm \infty$ or/and $b=\pm \infty$!	
	
	To define the direction from which we come from by applying the limit, we use a special notation (recall that this will give us the information of which side of the road comes our bus from...). Thus, if $f (x)$ tends towards the limit $b_1$ when $x$ approaches a number $a$ by taking only values smaller than $a$, then we write:
	
	(notice the small $-$ subscript) and we name $b_1$ the "\NewTerm{left limit}\index{left limit}" of the function $f (x)$ at point $a$ (because remember that the horizontal axis goes from left to right from $-\infty$ to $+\infty$, so small values compared to a given value, are on the left).
	
	If $x$ takes values greater than $a$, then we will write:
	
	(notice the small $+$ subscript) and we name $b_2$ the "\NewTerm{right limit}\index{right limit}" of the function $f (x)$ at point $a$.
	
	In the figure below we have for example:
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/limits.jpg}
		\caption{Left and Right limit examples}
	\end{figure}
	It is not always easy to calculate limits of some functions. Let us see two typical examples:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Let us prove that:
		
	is true. For this purpose we have to prove that for any small $\varepsilon$ the inequality:
	
	will be satisfied as soon as $|x|>M$ where $M$ is defined by the choice of $\varepsilon$. The previous inequality is obviously equal to:
	
	which is satisfy if we have $x$:
	
	We admit that the example and the method can be discussed.... But in fact it is only an application of the Hospital rule (ratio of the derivatives) already proved in the section of Differential and Integral Calculus. The reader must also know that we will see also other techniques to determine limits further below with better examples.\\
	
	E2. Now using Taylor series and change of variables consider we want to calculate:
	
	The method is quite to intuitive. Indeed, first we do a change of variable:
	
	Now consider the Taylor series about $x=0$ for the function $f(x)=\sqrt{1+ax}$. We have:
	
	Which gives:
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	as a Taylor expansion about $x=0$. Applying this to our limit we see that:
	
	\end{tcolorbox}
	
	The signification of the symbols $x\rightarrow -\infty$ and $x\rightarrow +\infty$ makes obvious the signification of the expressions:
	
	and:
	
	that we denote formally by:	
	
	We have defined the case where the function $f (x)$ tends to a certain limit $b$ when $x\rightarrow a_{+,-}$ or $x\rightarrow \pm \infty$. Now let us consider the case where the function tend to infinity when the variable $x$ change in a certain way.
	
	We then have typically and obviously:
	
	Or when we need to indicate the direction:
	
	If the function $f(x) \rightarrow +\infty$ when $a \rightarrow +\infty$ the we write:
	
	And as we have four possibilities for the sign we write when we do only theore:
	
	that is sot say the four following possibilities:
	
	And once again don't forget as we already mentioned before that some function such as for example $f(x)=\sin(x)$ don't have any finite limit when $x\rightarrow \pm \infty$. Then we say that the function is just "bounded" (\SeeChapter{see section Set Theory}).
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/limite_delucq.jpg}
	\end{figure}
	Now that we've roughly an overview of the concept of limit, we will give an extremely important definition that has a very important place of many areas of high-level mathematics, theoretical physics and computing science (numerical methods).
	
	\textbf{Definition (\#\mydef):} Given a function $f(x)$ and one of its subdomain (or whole one) $E$ (most of time $E \subseteq \mathbb{R}$ and $x_0\in E$, we say that we have a "\NewTerm{continuous function}\index{continuous function}" on $x_0$ if and only if:
	
	That is to say more formally (you have to be able to to read the fact that we are going close in an infinitely small way of a limit an this allows the continuity):
	
	In other words: a function is continuous if for every point $x_0$ in the domain $E$, we can make the images of that point ($f(x_0)$) and another point ($f(x)$) arbitrarily close (of a distance $\varepsilon$) if we move the other point ($x$) close enough (distance $\delta$) to our given point.
	
	The latter relation will be generalized a little bit in the section of Topology and completed with the concept of... "uniform continuity"!
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1. }$f$ is "\NewTerm{continuous on the left}\index{continuous on the left}" or respectively "\NewTerm{continuous on the right}\index{continuous on the right}", if we add to the definition above the condition $x>x_0$, respectively $x<x_0$.\\
	
	\textbf{R2.} A continuous function with a continuous inverse function is named a "\NewTerm{homeomorphism}\index{homeomorphism}".\\
	
	\textbf{R3.} Instead of saying when necessary that a function is not continuous on $x_0$ or on a given domain, some practitioners prefer to say that the function has an "\NewTerm{oscillation}\index{oscillation}".
	\end{tcolorbox}	
	We have the following trivial corollaries:
	\begin{enumerate}
		\item[C1.] $f(x)$ is continuous on $x_0$ if and only if $f(x)$ is continuous on the left right and on the right left.
		
		\item[C2.] $f(x)$ is continuous on $E$ if and only if $f(x)$ is continuous on any point of $E$.
	\end{enumerate}
	
	\paragraph{Limit laws}\mbox{}\\\\
	We now take a look at the "\NewTerm{limit laws}\index{limit laws}" , the individual properties limits in the univariate case. The proofs will be omitted as it is quite intuitive but any reader can request us the proof of one of them if needed!
	
	Let $f(x)$ and $g(x)$ be defined for all $x\neq a$ over some open interval containing $a$. Assume that $L$ and $M$ are real numbers such that:
	
	Let $c^{te}$ be a constant. Then, each of the following statements holds:		
	\begin{itemize}
		\item The sum law for limits gives:
		
		
		\item The difference law for limits gives:
		
		
		\item Constant multiple law for limits:
		
		
		\item Product law for limits:
		
		
		\item Quotient law for limits:
		
		for $M\neq 0$.
		
		\item Power law for limits:
		
		for every positive integer $n$.
		
		\item Root law for limits:
		
		for all $L$ if $n$ is odd and for $L\geq 0$ if $n$ is even.
	\end{itemize}
	
	\subsubsection{Asymptotes}
	The term "\NewTerm{asymptote}\index{asymptote}" is used in mathematics to precise possible properties of an infinite branch of curve which growth tends to an infinitesimal value.
	
	In analytic geometry, an asymptote of a curve is simply said to be a line such that the distance between the curve and the line approaches zero as they tend to infinity. In some contexts, such as algebraic geometry, an asymptote is defined as a line which is tangent to a curve at infinity.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The word asymptote is derived from the Greek and means "not falling together".
	\end{tcolorbox}	
	
	\textbf{Definitions (\#\mydef):}
	
	\begin{enumerate}
		\item[D1.] When the limit of a function $f(x)$ tends to a constant 
$c^{te}$ when $x \rightarrow \pm \infty$, then the graphical representation of this function leads us to draw a horizontal line that we name "\NewTerm{horizontal asymptote}\index{horizontal asymptote}" which equation is satisfies:
		
		
		\item[D2.] When the limit of a function $f(x)$ tends to  
$\pm \infty$ when $x \rightarrow a_{+,-}$, then the graphical representation of this function leads us to draw a vertical line that we name "\NewTerm{vertical asymptote}\index{vertical asymptote}" which equation is satisfies:
		
		Vertical asymptots is the typical symptom of a division by zero in a fraction and has a very important place in physics. The syndrome is also named a "\NewTerm{singularity}\index{singularity}".
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		The graph of the function:
		
		 has the straight line of $x=1$ and $y=0$ as horizontal asymptote:
		 \begin{figure}[H]
			\centering
			\includegraphics{img/analysis/asymptote_vertical_horizontal_example.jpg}
			\caption{Graphical representation of a  horizontal and vertical asymptote}
		\end{figure}
		\end{tcolorbox}
		
		\item[D3.] The straight line of equation equation is an "\NewTerm{oblique asymptote}\index{oblique asymptote}" of a curve of the function $f (x)$ if:
		
		the values of $a$ and $b$ can be easily found using the following relations:
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		Caution! A curve may have two distinct oblique asymptotes in $+\infty$ and $-\infty$.
		\end{tcolorbox}	
		To find a possible oblique asymptote, one must already be certain that the function $f(x)$ admits an infinite limit in $+\infty$ or $-\infty$ then only we look for the limits at $-\infty$ and $+\infty$ of  $f (x) / x$ and $f(x)-ax$.
		
		Three typical cases can be considered for oblique asymptotes:
		\begin{enumerate}
			\item The representative curve of $f(x)$ has for asymptotical direction the affine equation $y=ax$:
			
			\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
			\textbf{{\Large \ding{45}}Example:}\\\\
			The graph of the function:
			
			 has the straight line of $y=x$ as oblique asymptote:
			 \begin{figure}[H]
				\centering
				\includegraphics{img/analysis/asymptote_oblique_affine_example.jpg}
				\caption{Graphical representation of an oblique affine asymptote}
			\end{figure}
			\end{tcolorbox}
			
			\item The representative curve of $f(x)$ has an infinite branch (this branch has not close form asymptote) and the only one thing we can say is that $x$-axis is the direction of of this asymptote. Such an asymptote exists when:
			
			\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
			\textbf{{\Large \ding{45}}Example:}\\\\
			The functions $f(x)=\sqrt{x}$ (in red) or $\ln(x)$ (in green) have a limit $f(x)/x$ equal to $0$ and both have a "parabolic branch"  of direction following the $x$-axis:
			 \begin{figure}[H]
				\centering
				\includegraphics{img/analysis/asymptote_parabolic_branche_example_x.jpg}
				\caption[]{Graphical representation of an parabolic branch example following $x$-axis}
			\end{figure}
			\end{tcolorbox}
			
			\item The representative curve of $f(x)$ has an infinite branch (this branch has not close form asymptote) and the only one thing we can say is that $y$-axis is the direction of of this asymptote (we then also speak of "parabolic branch"):
			
			\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
			\textbf{{\Large \ding{45}}Example:}\\\\
			The function $f(x)=x^2$ has an infinite $f(x)/x$ limit and therefore has a parabolic branch of direction following the $y$-axis.
			 \begin{figure}[H]
				\centering
				\includegraphics{img/analysis/asymptote_parabolic_branche_example_y.jpg}
				\caption[]{Graphical representation of a parabolic branch example following $y$-axis}
			\end{figure}
			\end{tcolorbox}
			
			\item A function $f(x)$ is say to have a "\NewTerm{curvilinear asymptote}\index{curvilinear asymptote}" if it satisfies:
			
			for $n>1 $where for recall $P_n(x)$ is a polynomial of degree $n$.
			\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
			\textbf{{\Large \ding{45}}Example:}\\\\
			The function :
			
			has a curvilinear asymptote that is:
			
			Indeed:
			
			 \begin{figure}[H]
				\centering
				\includegraphics{img/analysis/asymptote_curvilinear_example.jpg}
				\caption[]{Graphical representation of curvilinear asymptote}
			\end{figure}
			\end{tcolorbox}
		\end{enumerate}
	\end{enumerate}
	
	\pagebreak
	\subsection{Logarithms}
	We hesitated to put the definition of logarithms in the section Calculus. After a moment of reflection, we decided it was better to put it in this section because to understand it well, we must be aware of the concept of limits, of definition domain and of the power function. We hope that our choice will suit you best.
	
	Given the power (bijective) function of any base where $a \in \mathbb{R}_{+}^{*}/1$ (we exclude $1$ ontherwise it is not bijective) and denoted for recall by:
	
	for which it corresponds to each real number $x$, exactly one positive number $a^x$ (the image set of the function is in $\mathbb{R}$) such as the powers calculation rules are applicable (\SeeChapter{see section Calculus}).
	
	We know that for such a function that if $a>1$, then $f (x)$ is an increasing and positive (monotone) in $\mathbb{R}$, and if $0<a<1$, then $f(x)$ is positive and decreasing (monotone) in $\mathbb{R}$.
	
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} If $a>1$, when $x$ decreases to negative values, the graph of $f (x)$ approaches the $x$-axis. Thus, the $x$ axis is a horizontal asymptote. When $x$ increases in positive values, the graph rises quickly. This type of change is characteristic of the "\NewTerm{law of exponential of growth}\index{law of exponential of growth}" and $f(x)$ is sometimes named "\NewTerm{growing function}\index{growing function}"... If $0<a<1$, when $x$ increases, the graph tends asymptotically to the $x$-axis. This type of variation is known as an "\NewTerm{exponential decay}\index{exponential decay}".\\
	
	\textbf{R2.} By studying $a^x$, we exclude the case where $a\leq 0$ and $a=1$. Notice that if $a<0$, then $a^x$ is not a real number for many values of $x$ (we recall that the whole image set is forced to $\mathbb{R}$ in our previous definition). If $a=0$, the $a^0=0$ is not defined. Finally, if $a=1$, then $a^x=1$ for all $x$ and the graph of $f(x)$ is a horizontal line.
	\end{tcolorbox}
	As the power function $f (x)$ is bijective then there exists an inverse function $f^{-1}(x)$ and is named "\NewTerm{logarithm function}\index{logarithm function}" of base $a$ and is denoted by:
	
	and therefore:
	
	if and only if $y=a^x$.
	
	More generally it is defined by:
	
	
	Considering $\log_a(x)$ as an exposant, we have the following properties:	
	
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} The word "logarithm" means "number of logos", "logos" meaning "reason" or "ratio".\\
	
	\textbf{R2.} The logarithm and power functions are defined by their bases (the number $a$). When using a power of $10$ as a base ($10, 100, 1000, ...$) then we speak of "\NewTerm{common system}\index{common system}" because they have for $\log$ successive integers.\\
	
	\textbf{R3.} The integer part of the logarithm is named the "\NewTerm{characteristic}"\index{characteristic of a logarithm}.
	\end{tcolorbox}
	There are two types of logarithms that we find almost exclusively in mathematics and physics: the logarithm of base $10$, logarithm of base $e$ (the latter often named "\NewTerm{natural logarithm}\index{natural logarithm}") and logarithm of base $2$ for information theory.
	
	First the on in base $10$ (the most used on graphical representations):
	
	abusively noted:
	
	and the base (Eulerian) $e$:
	
	historically noted:
	
	the "$n$" meaning "Napierian".
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Historically, it is John Napier (1550-1617) whose name was Latinized "Napier" that we own the study of logarithms and the name of "natural logarithms" which aimed to facilitate greatly the time for manual calculations.
	\end{tcolorbox}
	In English for the logarithm function in base-$10$ logarithmic we need to calculate:
	
	ask the following question: at what power $n\in \mathbb{R}$ should we raise $10$ to get $x$?
	
	Formally, this consist to solve the equation:
	
	or written in another way:
	
	with $x$ being known and therefore in base $10$:
	
	The logarithm in base $10$ is used a lot in graphical representations in the scientific perspective when we look at amplitudes variations. For example with Maple 4.00b  we have for two sine function  having respectively for their respective mean the same amplitude variation of $50\%$ visible below that do not highlights necessarily this fact trivially:
	
	\texttt{>plot({10+0.5*10*sin(x),100+100*0.5*sin(x)},x=1..10);}
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/two_sinus_for_comparison_without_logarithm_scale.jpg}
		\caption[]{Plot with Maple 4.00b with two sine functions having same amplitude change compared to their average}
	\end{figure}
	While in logarithmic scale, this gives:
	
	\texttt{>with(plots):\\
	>logplot({10+0.5*10*sin(x),100+100*0.5*sin(x)},x=1..10);}
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/two_sinus_for_comparison_with_logarithm_scale.jpg}
		\caption[]{Same plot with Maple 4.00b but with the $y$-axis in logarithm (base $10$) scale}
	\end{figure}
	For the logarithmic function in Eulerian base $e$ it is necessary to calculate:
	
	to ask ourself the following question: at what power $n\in \mathbb{R}$ we must raise the number $e$ to get $x$?
	
	Formally this consists to solve the equation:
	
	with $x$ being known and therefore:
	
	Technically, we say that the exponential function (see below for details):
	
	is the inverse bijection of the $\ln (x)$ function.
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/bijection_ln_x_exp_x.jpg}
		\caption{Graphical representation of the correspondence between the natural logarithm and the exponential}
	\end{figure}
	But what is that "Eulerian" number also named "\NewTerm{Euler number}\index{Euler number}"? Why do we find so often in physics and mathematics? Let us first determine the origin of its value:
	
	with $\alpha \in \mathbb{N}$ and when $\alpha +\infty$.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The second term of equality is typically the type of expression that we find in compound interest in finance (\SeeChapter{see section Economy}) or in any other type of identical increase factor. And what interests us in this case is when this type of increase tends to infinity.
	\end{tcolorbox}
	The interest we have to pose the problem as in this way is that if we do tend $\alpha \rightarrow +\infty$ the function written above tends to $e$ and this function has the special property of being calculable more or less easily for historical reasons using Newton's binomial.
	
	So according to the development of the Newton binomial (\SeeChapter{see section Calculus}) we can write:
	
	This development is similar to the Taylor expansion (\SeeChapter{see section Sequences and Series}) of some given functions for particular cases of development values (hence the reason why we find this eulerian number in many places that we will later).
	
	By performing some algebraic transformations that should now be obvious to the reader, we find:
	
	We see in this last equality that the function $\left(1+\frac{1}{\alpha}\right)^\alpha$ is increasing when $\alpha$ increases. Indeed, when we move from $\alpha$ to the value $\alpha+1$ each term of this sum increases:
	
	Let us prove now that the variable $\left(1+\frac{1}{\alpha}\right)^\alpha$ is bounded. By seeing that:
	
	So we get by analogy with the extended expression of Newton binomial determined just previously the following order relation:
	
	On the other hand:
	
	We then can write the inequality:
	
	The underlined terms constitute a geometric sequence of reason $q=1/2$ (see section Sequences and Series) and whose first term is $1$. If follows using the result obtained in the section of Sequences ans Series, that we can write:
	
	Therefore, we have:
	
	We have therefore proved that the function $\left(1+\frac{1}{\alpha}\right)^\alpha$ is bounded.
	
	The limit:
	
	tends to this limited value that is the number $e$ whose value is:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	As we have proved it in the section Numbers, this number is irrational.
	\end{tcolorbox}
	We can then define the "\NewTerm{natural exponential function}\index{natural exponential function}" (reciprocal of the natural logarithm function) by:
	
	also sometimes denoted by:
	
	The number $e$ and the function that determines it are very useful. We find them in all areas of mathematics and physics and thus in almost all the chapters of this book.
	
	As we have proved it in the section of Differential and Integral Calculus the functions $e^x$ has for remarkable property that its derivative is equal to itself:
	
	and this is used a lot for the resolution of differential equations in physics and finance.
	
	Logarithms have several properties. Here are the most important one in our point of view (we are referring to a given base $X$) and that are very useful in physics, electronics, chemistry and so on...
	
	Let us begin. First:
	
	If we put $X^m=a$ and $X^n=b$ we get:
	
	If we have the special case when $a=b$ then:
	
	Now let us try to express:
	
	in another way. For this we put first:
	
	which leads us to the development:
	
	Now let us try to express:
	
	with $n\in \mathbb{N}^{*}$ in another way. For this we put first:
	
	which leads us to the development:
	
	There is  also another relation used a lot of time in physics in respect to the change of logarithm basis. The first relation is trivial and follows from the algebraic properties of logarithms:
	
	the second one:
	
	is a bit less trivial and requires perhaps a proof (we used it for our study of continued fractions in the section Number Theory).
	\begin{dem}
	We first use the equivalent equations (of the first relation above):
	
	and we proceed as follows:
	
	What finally brings us to:
	
	\begin{flushright}
		$\square$  Q.E.D.
	\end{flushright}
	\end{dem}

	\pagebreak
	\subsection{Transformations}
	\subsubsection{Fourier Transform}
	\subsubsection{Laplace Transform}
	The Laplace transform is used extensively  in order to solve  differential equations that arise in many modeling situations of real life. 

	A power series about an origin is any series that can be written in the following form:

	where $a_n$ are  numbers and $n$ is a nonnegative integer. One can think of $a_n = a(n)$ as a function of $n$ for each non-negative integer $n = 0, 1, 2, \ldots$. In order to give birth to Laplace transformation technique, we  make some associations. The discrete variable $n$ is converted into a real variable $t$. The coefficient term $a_n$ is written as $f(t)$. The term $x^n$ can equivalently be written as $e^{(\ln x^t)}$. Finally, summation notation can be replaced by its continuous analogue, that is, integration. By doing so, we have following:
	 
For convergence, it is obviously important to have following condition for the above integral (yes think for this about the original sum!):
	 
	Therefore:
	 
	Since $0<x<1 $ so it implies that:
	
	Thus $\ln(x)$ has to be negative for the integral to converge, In this regard, we suppose $\ln(x)=-s$ where $s>0$. Thus, the final integral takes the form:
	
	In this way, we can say that Laplace Transform is simply  stretching a discrete (infinite series)  into a continuous (integration) analogue. 
	\subsubsection{Hilbert Transform}
	
	
	\subsection{Functional dot product (inner product)}
	The "\NewTerm{functional dot product}\index{function dot product}\index{orthogonality of functions}\index{functions orthogonality}" (very strong analogy with the dot product in seen in the section Vector Calculus) may seem unnecessary when examined for the first time outside of an application context or only as generalization purpose, but in fact it has many practical applications. We will make such direct use in the section of Wave Quantum Physics and Quantum Chemistry, or even more important in the context of trigonometric polynomials through the Fourier series and transforms (\SeeChapter{see section Sequences and Series}) that we find everywhere in contemporary physics and computer science.
	
	However, if the reader has not traveled the section of Vector Calculus and the part treating the vector dot product, we would highly recommend reading it otherwise what follows may be a little incomprehensible.
	
	We put ourselves in the space $\mathcal{C}([a,b],\mathbb{R})$ of continuous functions in the interval $[a, b]$ into $\mathbb{R}$ with the inner product defined by (we find here again the specific notation of the dot product in its functional version as we had mentioned during our definition of the vector dot product in the section of Vector Calculus):
	
	A family of orthogonal polynomials, as we can make the analogy with the dot product in the section Vector Calculus, is therefore a polynomial family $(p_0,...,p_n,...)$ such as:
	
	if $j \ne k$. We recall that an orthogonal family is a free family. We also saw in the section of Vector Calculus that in the space $\mathcal{C}([a,b],\mathbb{C})$ the only possible coherent choice was:
	
	
	We name the two previous relations "\NewTerm{$L^2$-dot product}\index{dot product!$L^2$-dot product}".
	
	Therefore we can build the "\NewTerm{$L^2$-norm}\index{norm!$L^2$-norm}":
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We will see further below that the definition above is not the most general one as especially physicists and engineers say that functions are orthogonal under a more constraint situation.
	\end{tcolorbox}
	The development that will follow will remind us the Gram-Schmidt procedure (\SeeChapter{see section Vector Calculus}) to build an orthogonal family.
	
	\begin{theorem}
	Given $(p_0,...,p_n,...)$ a family of linearly independent polynomial defined on $[a,b]$ and $V$ the vector space defined by this family. The family $(y_0,...,y_n,...)$ defined by recurrence in the following way:
	
	and $y_0=p_0$ is orthogonal and generates $V$.
	\end{theorem}
	\begin{dem}
	Let us show by induction on $n$ that $(y_0,y...,y_n,...)$ is an orthogonal family which generates the same space as $(p_0,...,p_n,...)$. The assertion holds for $n=0$. Let us suppose that the assertion holds for $n\geq 0$, for $0\leq k\leq n$ we have:
	
	$(y_0,...,y_n,...)$ is therefore orthogonal. Finally, the equality:	
	
	\begin{flushright}
		$\square$  Q.E.D.
	\end{flushright}
	\end{dem}
	\addcontentsline{toc}{paragraph}{Orthogonality of trigonometric functions}
	Let us see a first example very important is signal processing and statistic that is relatively to frequency analysis:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider the very important example in modern physics that is the set of continuous $2\pi$-periodic function denoted $P_{2\pi}$ that forms a vector space (\SeeChapter{see section Vector Calculus}).\\
	
	We define the scalar product of two functions of this set by:
	
	The aim of this definition is to build an abstract functional basis $P_{2\pi}$ on which we can break down any $2\pi$-periodic function!!!\\
	
	The simplest idea is then to use the trigonometric functions sine and cosine:
	
	The relations below show that the basis chosen above are orthogonal and therefore form a free family, plus it's a generating family of the vector space $P_{2\pi}$ because as we have seen in our study of Fourier series (\SeeChapter{see section Sequences and Series}), we have the following values:
	
	where $\delta_{km}$ is the kronecker symbol (\SeeChapter{see section Tensor Calculus}).\\
	
	Therefore it is also an orthogonal basis but not orthonormal. If we want to normalized the vectors of the basis we just need obviously to take:
	
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	If the reader remembers that for a random variable $X$ defined on $\mathbb{R}$, the mean was calculated as (\SeeChapter{see section Statistics}):
	
	The we can assimilate:
	
	where:
	
	to the expected mean of the function $g(x)$! Analogy sometimes very useful in practice!
	\end{tcolorbox}
	\addcontentsline{toc}{paragraph}{Orthogonality of complex exponential functions}
	Let us see now another example that is an extension of the previous one and that has also a great importance in signal processing but also in quantum physics and quantum chemistry:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider a basis of complex functions of the form $r e^{\mathrm{i}n\varphi}$ with $n\in\mathbb{Z}$. We therefore can write:
	
	We get:
	
	It is obvious that if we take for basis functions of the type:
	
	then we have an orthonormal basis (and not just an orthogonal one).
	\end{tcolorbox}
	\addcontentsline{toc}{paragraph}{Orthogonality of Bessel functions}
	Another example that will be useful for us in the section of Wave Mechanics for our study of the ideal circular membrane of a drum.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We have proved in the section Sequences and Series that the Bessel function\index{Bessel function} $J_p(x)$ satisfies the following differential equation (Bessel's equation):
	
	which can be written as:
	
	The variable $p$ need not be an integer as we will see it in the section of Mechanical Engineering with the study case of the self-buckling column.\\
	
	It turns out to be useful to define a new variable $t$ by $x = a t$, where $a$ is a constant which we will take to be a zero of $J_p$, i.e. $J_p(a) = 0$. Let us define:
	
	which implies:
	
	and substituting into (\ref{eq}) gives:
	
	since $x \mathrm{d}/\mathrm{d}x$ is equivalent to $t d/dt$.
	We can also write down the equation obtained by picking another zero, $b$. Defining:
	
	which implies:
	
	we have then:
	
	To derive the orthogonality relation, we multiply (\ref{eq1}) by $v$, and (\ref{eq2}) by $u$. Subtracting and dividing by $t$ gives:
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	The first two terms in (\ref{combine}) can be combined as:
	
	since the extra terms present in (\ref{totalderiv}), but not in (\ref{combine}), when the derivatives are expanded out are equal and opposite and so cancel. Hence we have:
	
	We next integrate this over the range of $t$ from $0$ to $1$ ($0$ since the Bessel function is not defined for $t<0$ and to $1$ since it's the place where there is a zero by construction for recall!), which gives:
	
	The integrated term vanishes at the lower limit because $t=0$, and it also vanishes at the upper limit because $u(1) = v(1) = 0$, see (\ref{u10}) and (\ref{v10}). Hence, if $a \ne b$, (\ref{int01}) gives:
	
	which, using (\ref{ut}) and (\ref{v10}), can be written
	
	This is the desired orthogonality equation. Remember we require that $a$ and $ b$ are distinct zeroes of $J_p$, so both Bessel functions in (\ref{orthog}) vanish at the upper limit.
	\end{tcolorbox}
	
	\pagebreak
	\addcontentsline{toc}{paragraph}{Orthogonality of Hermite polynomial}
	A last example that will be useful for us in the section of Wave Quantum Physics during our study of the harmonic oscillator:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We will introduce in the section of Wave Quantum Physics the following "physicist Hermite polynomial\index{Hermite polynomial}":
	
	Therefore (see the plot in the section of Wave Quantum Physics):
	
	where we notice almost immediately that (useful for for further below):
	
	And we need to prove that they are orthogonal (or even better: orthonormal!).\\
	
	For this purpose we introduce the weight function $w(x)=e^{-x^2}$ therefore:
	
	So we get (we use the Gauss integral as proved in the section Statistics):
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	Finally, using Kronecker symbol (\SeeChapter{see section Tensor Calculus}):
	
	\end{tcolorbox}
	From what we have seen above we deduce that:
	
	is in fact generalized by:
	
	where $w(x)$ is the "\NewTerm{weight function}\index{weight function}". 
	
	So the engineer, physicist, mathematician must always be careful when he see in a textbook a sentence of the type: \textit{these functions are orthogonal}. Indeed the author/redactor should instead read: \textit{these functions are orthogonal with a given weight}.
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{100} & \pbox{20cm}{\score{4}{5} \\ {\tiny 47 votes,  71.49\%}} 
	\end{tabular} 
	\end{flushright}
	
	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Complex Analysis}

\lettrine[lines=4]{\color{BrickRed}B}efore starting this section on the study of differential and integral calculus in the generalized case of complex numbers, I should point out that I used many illustrations inspired by the PDF of E.~Hairer (with his permission). This text also contains many sentences and developments taken, homogenized and simplified from the same PDF (at the risk to make some purist readers climbing to the curtains...) according to the notations and educational objectives of the rest of this book.

The subject of the complex analysis is the study of functions $\mathbb{C} \mapsto \mathbb{C}$ and their differentiability (which is different from that in $\mathbb{R}^2$). The "holomorphic functions" (that is to say differentiable in a subset of $\mathbb{C}$) have as we will see it later surprising and elegant properties that can be reused in the situation of the special case of functions in $\mathbb{R}^2$ (remember that $\mathbb{C}$ is a generalization of $\mathbb{R}^2$ ) that have important applications in advanced physics (we will use the results of this section for our study of quantum physics and also for some applications of fluid mechanics and also for advanced models in financial options pricing).

Before we start let us first explain the interest of Complex Analysis in a simplified way!

We studied in the chapter Algebra a part of the Differential and Integral Calculus with some useful and important theorems in physics and engineering. However, staying in $\mathbb{R}$ or $\mathbb{R}^2$ the list of theorems runs out somehow and we end up finding much relevant tools in practice that allows to simplify the integration calculation that we can sometimes found in industrial applications. So, when we remember that $\mathbb{R} \subset \mathbb{C}$ (thus the set of complex numbers generalizes the set of real numbers) and that we can also build a correspondence $\mathbb{R}^2 \mapsto \mathbb{C}$ as we shall see, then new theorems appear with very interesting results that can be exploit for the integrals in $\mathbb{R}$ or $\mathbb{R}^2$!! It is because of this reason that the engineer needs to know Complex Analysis!

After studying this particular field of mathematics, it is common to say that the shortest path between two truths of the real domain often requires to pass trough the complex domain...

\subsection{Linear Applications}

A good introduction to complex analysis and its representation is to look at first (for educational purposes mainly) the special case of complex linear applications. Let us see this!

Let $U \subset \mathbb{C}$ be a set and $V \subset \mathbb{C}$ another set. A function that associates to each $z \in U$ an $w \in V$ such that:
	
is a "\NewTerm{complex function}\index{complex function}":
	
What is important is to remember (\SeeChapter{see section Numbers}) that we can identify:
	
and:
	
We have thus two functions of two real variables $x, y$:
	
which are the coordinates of the point $w$.

\textbf{Definition (\#\mydef):} An application is named "\NewTerm{$\mathbb{C}$-linear}\index{$\mathbb{C}$-linear function}" if for example a function of the type:
	
where $c$ is a fixed complex number and $z$ an arbitrary complex number, satisfying:
	

That is to say that $f(z)$ must me additive and homogeneous or just briefly when this two properties are satisfied we say that $f(z)$ is a "\NewTerm{linear map}\index{linear map}".

We have seen and proved in the section on Numbers during our study of complex numbers, that the multiplication of two complex numbers could be equivalent to an orthogonal rotation followed by a scaling and that this same multiplication could be represented in matrix form! Or the transcription into a matrix form involves as we saw in the section on Linear Algebra automatically  the property of linearity!

So the reader can easily check that a matrix of rotation/scaling is an example of an $\mathbb{C}$-linear application (on request we can detail) that we will now write:
	
Which can be typically represented as follows (we can clearly see a rotation and a scaling which conserve the angles and proportions):

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/clinear_application.eps}
\caption{$\mathbb{C}$-linear application example}
\end{figure}

It is the fact that the proportions and the angles are kept that makes a complex function $\mathbb{C}$-linear. Otherwise, we would say that the function is $\mathbb{R}$-linear.

So a matrix equation is $\mathbb{C}$-linear if and only if it is of the form:
	
Let us see some examples of quite remarkable $\mathbb{C}$ non-linear functions.

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Examples:}\\\\
E1. Consider the function:
	
In real coordinates this gives:
		
So let's look what this function do with the points of the complex plane which are coincident with the vertical lines of this same plane (which take us to write $x=a$). Then we have:
	
	and eliminating y, we find the equation of a parabola or rather a family of  parabolas (for several values of $b$) which are open to the left of the pictured complex plane.\\

	Here is a picture representation of the complex plane on which we have drawn a cat head:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.75]{img/analysis/c_linear_image_cat.eps}
		\end{center}	
		\caption{Complex representation of the image of the example function}
	\end{figure}
	\end{tcolorbox}

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
and if we look at the corresponding pre-image complex plane  then we have two heads of cats that appear:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.75]{img/analysis/c_linear_pre_image_cat.eps}
		\end{center}	
		\caption{Pre-image representation of the example function}
	\end{figure}
The appearance of these two heads of cats is that this function has 2 possible pre-images for each image point (so it is a surjective function - see section Set Theory).\\

Here is a nice Maple 17.00 script by Carl Ebehart to check this (shame that this can not be done in an easier way in Maple):\\

\texttt{> complextools[gridimage] := proc(p)\\
local llhc, width, height, xres, yres, clrs, V, H, i,j,k,l,pz,x,y,z,f,g,xtcs,ytcs,opts,margs;\\
llhc := [-1, -1];\\
width := 2;height := 2;\\
xres := .25;yres := .25;\\
xtcs := 1; ytcs := 1;\\
clrs := [red, black];\\
opts := NULL;\\
opts := op(select(type,[args],`=`));\\
margs:= remove(type, [args] ,`=`) ;\\
if nops(margs) >1 and margs[2] <> `` then llhc := margs[2] fi:\\
if nops(margs) >2 and margs[3] <> `` then width := margs[3] fi:\\
if nops(margs) >3 and margs[4] <> `` then height := margs[4] fi:\\
if nops(margs) >4 and margs[5] <> `` then xres := margs[5] fi:\\
if nops(margs) >5 and margs[6] <> `` then yres := margs[6] fi:\\
if nops(margs) >6 and margs[7] <> `` then xtcs := margs[7] fi:\\
if nops(margs) >7 and margs[8] <> `` then ytcs := margs[8] fi:}
	\end{tcolorbox}

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\texttt{if nops(margs) >8 and margs[9] <> `` then clrs := margs[9] fi:\\
z:= x + I*y;\\
pz := evalc(p(z));\\
f := unapply(evalc( Re(pz)),x,y); g := unapply(evalc(Im(pz)),x,y);\\
V:= plot( [
seq([seq(op([[f(llhc[1] + i*xres ,llhc[2]+(j-1)*yres/ytcs),g(llhc[1] + i*xres ,llhc[2]+(j-1)*yres/ytcs)], [f(llhc[1] + i*xres , llhc[2] +j*yres/ytcs),g(llhc[1] + i*xres , llhc[2] +j*yres/ytcs)]]),
j=1..ytcs*height/yres)], i = 0 .. width/xres)
],color=clrs[1]);\\
H := plot( [
seq([seq(op([[f(llhc[1]+(j-1)*xres/xtcs,llhc[2] + i*yres),
g(llhc[1]+(j-1)*xres/xtcs,llhc[2] + i*yres)], 
[f(llhc[1] +j*xres/xtcs, llhc[2] + i*yres),
g(llhc[1] +j*xres/xtcs, llhc[2] + i*yres)]]),
j=1..xtcs*width/xres)], i = 0 .. height/yres)
],color=clrs[2]);\\
plots[display]([V,H],scaling=constrained,opts);
end:\\
with(complextools);}\\

\texttt{>plots[display]([seq(plots[display]([gridimage(z->z), gridimage(z->z\string^2)]),i=10)],insequence=true);}
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.75]{img/analysis/c_linear_maple_transform.eps}
		\end{center}	
		\caption{Practical Maple 17.00 example of simple $C$-linear application}
	\end{figure}
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
E2. Another interesting feature is the "\NewTerm{Cayley transformation}\index{Cayley transformation}" used in some areas of physics and defined as:
	
having as domain definition: $\mathbb{C}/\left\lbrace 1\right\rbrace$.\\

We note that this is an involutive function since:
	
and as we have proved in the section of Proofs Theory that any involution function is both injective and surjective, then the Cayley transform is a bijective function.\\

This function transforms the imaginary axis $\mi y$ in unit circle (and vice versa as it is involutive). Let us see that:
	
where:
	
satisfies:
	
That is to say:
	
This is the equation of a circle as proven in the section of Analytical Geometric.
	\end{tcolorbox}

\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	E3. As another example of function, consider the "\NewTerm{Joukovski transformation}\index{Joukovski transformation}" defined by:
	
If the definition domain is built in polar coordinates look at how a circle or ellipse transforms with this function:

	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.75]{img/analysis/joukovski_pre_image.eps}
		\end{center}	
		\caption{Transformation into polar coordinates of an ellipse with the example function}
	\end{figure}
Then the image plane will be:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.5]{img/analysis/joukovski_image.eps}
		\end{center}	
		\caption{Result of the Joukovski transformation in polar coordinates}
	\end{figure}
It thus transforms the circles respectively centered at $0$ and the rays passing through $0$ into a family cofocal ellipses and hyperbole . To prove this fact, we use the complex polar coordinates (Euler formula) seen in the section on Numbers (\SeeChapter{see chapter Arithmetics}):
	\end{tcolorbox}

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		
	and:
		
Then we have:
		
therefore:
		
and we immediately see that (\SeeChapter{see section Trigonometry}):
		
has the form of the equation of an ellipse (\SeeChapter{see section Analytical Geometry}) and we also have:
		
which is the equation of a hyperbole (\SeeChapter{see section Analytical Geometry}).\\

This function is useful in case we cleverly place a circle through the point $z=1$ (as in the case of the first figure) the plan represented in polar coordinates with a dotted line might looks like an airplane wing. This allowed a time ago in aerodynamics (but the technique is obsolete today)  to transpose the study of a vector field of an airplane wing profile to the study of a circle profile and to do after the Joukovski transformation.
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
Indeed, let us see a part of this still with Maple 4.00:\\

\texttt{> assume(x,real,y,real);}\\
\texttt{> z:=x+I*y;}\\
\texttt{> F:=1/2*(z+1/z);}\\
\texttt{> u:=Re(F);}\\
\texttt{> u:=evalc(u);}\\
\texttt{> v:=Im(F);}
\texttt{> v:=evalc(v);}\\
\texttt{> with(plots):with(plottools):}\\
\texttt{> p1:=disk([0,0],1,color=black):}\\
\texttt{> p2:=implicitplot({seq(v=b8,b=-10..10)},x=-4..4,y=-2..2,color=black):}\\
\texttt{> display([p2,p1],scaling=constrained);}\\

We thus get:

	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.5]{img/analysis/joukovski_application.eps}
		\end{center}	
		\caption{Important application example of the Joukovski function}
	\end{figure}

	\end{tcolorbox}

Let us see a last example that shows an electric dipole with its electric field and potential lines (\SeeChapter{see section Electrostatic}) can bee seen as the emergence of the $\mathbb{C}$-linear function $1/z$:

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Examples:}\\\\
E4. Always with Maple 4.00b we write:\\

\texttt{>assume(x,real,y,real);}\\
\texttt{> z:=x+I*y;}\\
\texttt{> F:=1/z;}\\
\texttt{> u:=Re(F);u:=evalc(u);}\\
\texttt{> v:=Im(F);v:=evalc(v);}\\
\texttt{> with(plots):}\\
\texttt{> p1:=implicitplot({seq(u=a,a=-5..5)},x=-1..1,y=-1..1,numpoints=1000):}\\
\texttt{> p2:=implicitplot({seq(v=b,b=-5..5)},x=-1..1,y=-1..1,numpoints=1000,}\\
\texttt{color=green):}\\
\texttt{> display([p1,p2],scaling=constrained);}\\
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.5]{img/analysis/dipole.eps}
		\end{center}	
		\caption{Another important application of a complex application}
	\end{figure}
	\end{tcolorbox}
	
	\subsection{Holomorphic Functions}
	The definition of the derivative with respect to a complex variable is naturally formally identical to the derivative with respect to a real variable.
	
	We then have, if the function $f(z)$ is differentiable in $z_0$:
	
	
	and we say (abusively in this book) that function is "\NewTerm{holomorphic}\index{holomorphic function}" (while in $\mathbb{R}$ we say "differentiable") or "\NewTerm{analytical}\index{analytical function}" in its domain or in a subset of it if it is differentiable at any point.
	
	In other words a holomorphic functions is a complex-valued function of one or more complex variables that is complex differentiable in a neighborhood of every point in its domain. The existence of a complex derivative in a neighborhood is a very strong condition, for it implies that any holomorphic function is actually infinitely differentiable and equal to its own Taylor series.
	
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} A complex function is derived like a real function, we just have to put $z$ as being $x$... at the condition of what we will see in what follows is respected!\\
	
	\textbf{R2.} In fact if the function is holomorphic in a subset of the complex plane, we will see a little further below in our study of the convergence of power series that this is than always an open subset.
	\end{tcolorbox}
	Equivalently, we say that the function $f$ is $\mathbb{C}$-differentiable if the following limit exists in  $\mathbb{C}$:
	
	Let us now present and prove a central theorem for complex analysis named "\NewTerm{Cauchy-Riemann theorem}\index{Cauchy-Riemann theorem}"!
	
	If the function:
	
	is $\mathbb{C}$-differentiable on $z_0=x_0+\mathrm{i}y_0$, then we have:
	
	which is somewhat the equivalent to the Schwarz theorem (limited to $\mathbb{R}$) proved in the section of Differential and Integral Calculus. The above two relations are named "Cauchy conditions". So these are the two conditions that must verify a complex function to be differentiable on $z_0$. Thus, it is possible to use these relations to examine the points where the function is not analytic.
	
	\begin{theorem}
	If these conditions are satisfied (what will prove right below), then we deduce that $u$ and $v$ must both harmonic functions of $x$ and $y$.
	\end{theorem}
	\begin{dem}
	As:
	
	by choosing:
	
	with $x \in \mathbb{R}$ we get:
	
	and as $x$ approaches a small value $\mathrm{d}x$, we have (\SeeChapter{see section Differential and Integral Calculus}):
	
	by choosing:
	
	with $y \in \mathrm{R}$, we get:
	
	and when $y$ tends to a small value we have (\SeeChapter{see section Differential and Integral Calculus}):
	
	So now we have:
	
	But remember we proved in the section of Integral and Differential Calculus the following theorem:
	
	Therefore:
	
	Therefore using directly Shwartz theorem:
	
	Which can be written:
	
	A trivial solution is obviously to have:
	
	Therefore the right to write:
	
	By identifying real and imaginary parts, we finish the proof!
	\begin{flushright}
		$\square$  Q.E.D.
	\end{flushright}
	\end{dem}
	So for $f$ to be differentiable in the complex domain $\mathbb{C}$ (holomorphic) at a point, it is sufficient that it be differentiable as a function of two real variables ($\mathbb{R}^2$-differentiable on $(x_0,y_0)$) and that its partial first derivatives at this point satisfy the Cauchy-Riemann equations.
	
	But, for it to be $\mathbb{C}$-differentiable, Cauchy-Riemann's equations must valid at all points of the complex plane (we sometimes speaks about "\NewTerm{complete functions}\index{complete functions}") and not only in a subdomain thereof! Otherwise, it contains therefore "\NewTerm{singularities}\index{singularities}" and we then speak of "\NewTerm{meromorphic function}\index{meromorphic function}" (which is therefore a holomorphic function excepted on singularities points).
	
	The Gamma function studied in the section of Differential and Integral Calculus is such a wellknown function:
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/analysis/gamma_meromorphic.jpg}
		\caption{Gamma function is meromorphic in the whole complex plane (source: Wikipedia)}
	\end{figure}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	Geometrically, we will prove later that a holomorphic function has a possible interpretation in the sense that it is a conformal transformation (angles conservation).
	\end{tcolorbox}
	
	Notice therefore that if $f (z)$ is $\mathbb{C}$-differentiable it can be developed as Taylor series (\SeeChapter{see section Sequences and Series}):
	
	Note an important thing too. If we rewrite:
	
	as following:
	
	Then we say that $f$ is "\NewTerm{irrotational}\index{irrotational}" (\SeeChapter{see section Vector Calculus}") since the first relation can be seen as:
	
	which is an important analogy! Finally, the second equation:
	
	Also let us say by analogy (but it stops at a simple analogy!) that the function $f$ is non-divergent (\SeeChapter{see section Vector Calculus}) what is good mnemonic way to remember this equation.
	
	Let's also show something else in evidence. If we take the two Cauchy-Riemann equations:
	
	and that we derivate them once again we get:
	
	and that we sum these two relations, we get then:
	
	It is the same with v. Then we have:
	
	And we know very well this form of equations (Maxwell-Poisson equation in the section of Electrodynamics and Newton-Poisson equation in the section of Astronomy...). This is a wave equation also named "\NewTerm{Laplace equation}\index{Laplace equation}" (nothing to do with that of the same name seen in our study of the hydrostatic!) and given by the scalar Laplacian (\SeeChapter{see section Vector Calculus}):
	
	Then it is traditional to say that $u$ is harmonic and of course we can get the same result with $v$! Well obviously ... we knew it, since we have already studied in the section Numbers that the real and imaginary parts of a complex number could be put in trigonometric form.

	Thanks to this discovery, Riemann opened the application of holomorphic functions in many problems of physics, since these equations are satisfied by the gravitational potential (Newton-Poisson equation in the section of Astronomy) by electric and magnetic fields (Maxwell-Poisson equation in the section of Electrodynamics) by heat balance (no examples yet in this book) and by movements without rotational of certain fluids (no examples either in this book yet).

	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The potential of a dipole can be described by the following holomorphic function:
	
	The figure below:
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/holomorphic_dipole_plot.jpg}
		\caption{Plane representation of a well known holomorphic function...}
	\end{figure}
	shows level-curves (iso-curves) of the given harmonic functions $u (x, y)$ and $v (x, y)$ as real and complex parts of the function $f (z)$ of this example.
	\end{tcolorbox}
	
	
	\pagebreak
	\subsubsection{Orthogonality of real and imaginary iso-curves}
	We will now proove a friendly property that have the functions that satisfy the Cauchy conditions (i.e. that are analytic functions!). Indeed, remember that we have already seen above the function:
	
	which gave the following diagram:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.75]{img/analysis/c_linear_image_cat.eps}
		\end{center}	
		\caption{Reminder of plane representation of a complex function seen earlier}
	\end{figure}
	
	\begin{theorem}
	Well he functions satisfying the conditions Cauchy have the simple following geometrical property following: the lines whose real part of the function is constant $\mathcal{R}(f(z))=c^{te}$ and lines whose imaginary part is constant $\mathcal{I}(f(z))=c^{te}$  are orthogonal to each other (think to the trigonometric form of complex numbers it helps to better visualize!).
	
	In other words, the analytical complex functions are transformation functions of of an area of the plane into a new plane where the angles are preserved. Then we say that the function is a "\NewTerm{complete transformation}\index{complete transformation}".
	\end{theorem}
	
	\begin{dem}
	For the proof remember that we have proved in section of Vector Calculus that  gradient of a function $f$ of $\mathbb{R}^2$ is given by:
	
	and as part of our study of isolines in the section of Differential Geometry that the tangent vector to isolines of the function $f$ will always be parallel to the vector of the plane:
	
	and that the latter two vectors are perpendicular, such that:
	
	Now assimilate the tangent (parallel) vector $\vec{t}_u$ to the real isolines:
	
	with:
	
	and the normal vector to the imaginary isolines:
	
	with the gradient $v$ of components:
	
	Using the Cauchy conditions proved above, we have for this last relation:
	
	By comparing:
	
	we therefore see that $\vec{t}_u$ and $\vec{\nabla}(v)$ are parallel (collinear). And since $\vec{t}_u$ is colinear the real isolines and that $\vec{\nabla}(v)$  is perpendicular to the imaginary isolines we finished our proof.
	\begin{flushright}
		$\square$  Q.E.D.
	\end{flushright}
	\end{dem}
	The reader may take as an example the function:
	
	mathematically and schematically detailed earlier above! But to change a little bit, consider an example that will accompany us throughout the rest of this section and that is the following holomorphic function:
	
	That gives us with Maple 4.00b:
	
	\texttt{
	>assume(x,real,y,real);\\
	> z:=1/(1+(x+I*y)\string^2);\\
	> F:=1/z;\\
	> u:=Re(F);\\
	> u:=evalc(u);\\
	> v:=Im(F);\\
	> v:=evalc(v);\\
	> with(plots):\\
	> p1:=implicitplot({seq(u=a,a=-5..5)},x=-5..5,y=-5..5,numpoints=1000):\\
	> p2:=implicitplot({seq(v=b,b=-5..5)},x=-5..5,y=-5..5,numpoints=1000,color=green):\\
	> display([p1,p2]);
	}
	
	which gives:
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/holomorphic_isoclines.jpg}
		\end{center}	
		\caption{Representation of an important holomorphic function with its isolines}
	\end{figure}
	
	\subsection{Complex Logarithm}
	We need for all functions built into $\mathbb{R}$ found their equivalent in $\mathbb{C}$ while knowing that if we reduce the case of $\mathbb{R}$ to $\mathbb{C}$ we must get back on our feet!
	
	To do this, let us start with the most classical and academic function which is the logarithm and also the only one function for which we will need the complex version in other sections of this book. As always we will focus only on the properties that we will need later for practical applications and nothing more!
	
	In the same way that we built the logarithm as being by definition by the inverse function of the natural exponential $e^x$ in the section of Functional Analysis, we first start from:
	
	where $z$ is a complex number and we will define the complex logarithm that must be reduced to the natural logarithm if $z$ has no imaginary part!
	
	So by definition the complex logarithm will be:
	
	and in this entire book, the complex logarithm will be differentiated by the real logarithm by a capital L for the first letter!
	
	Let us write $z$ and $w$ in the Euler form as viewed in the section Numbers:
	
	Then we have:
	
	By correspondence, we find immediately
	
	with $k \in \mathbb{Z}$. Therefore we get:
	
	Therefore:
	
	or more explicitly:
	
	So if $w$ has no imaginary part, we fall back on our feet since $\text{arg} (w)$ becomes zero.
	
	A big difference is highlighted between the logarithm of the complex and real numbers: the complex numbers logarithms can take several values because of the argument!!
	
	For a function to have an inverse, it must map distinct values to distinct values, i.e., be injective. But the complex exponential function is not injective, because $e^{z+2\pi \mathrm{i} }= e^z$ for any $z$, since adding $\mathrm{i}\theta$ to $w$ has the effect of rotating $e^z$ counterclockwise $\theta$ radians. So all the points of the form $z+\mathrm{i}k\theta$  are all mapped to the same number by the exponential function. So the exponential function does not have an inverse function in the standard sense.
	
	There are two solutions to this problem.
	
	\begin{enumerate}
		\item One is to restrict the domain of the exponential function to a region that does not contain any two numbers differing by an integer multiple of $2\pi \mathrm{i}$: this leads naturally to the definition of "\NewTerm{branches}\index{branches}" of $\text{Log}(w)$, which are certain functions that single out one logarithm of each number in their domains.
		
		\item Another way to resolve the indeterminacy is to view the logarithm as a function whose domain is not a region in the complex plane, but a Riemann surface (\SeeChapter{see section Non-Euclidean Geometries}) that covers the punctured complex plane in an infinite-to-1 way.
	\end{enumerate}
	Branches have the advantage that they can be evaluated at complex numbers. On the other hand, the function on the Riemann surface is elegant in that it packages together all branches of $\text{Log}(w)$ and does not require an arbitrary choice as part of its definition.
	
	We can see this with Maple 4.00b easily:
	
	\texttt{>plot3d([r*cos(f),r*sin(f),f],r=0..1,f=-2*Pi..2*Pi,axes=boxed,style=patch,\\
	shading=ZHUE);}
	
	which gives:
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/complex_logarithm.jpg}
		\end{center}	
		\caption{Complex Logarithm plot with Maple 4.00b}
	\end{figure}
	
	For this reason, one cannot always apply $\text{Log}$ to both sides of an identity $e^{z_1}=e^{z_2}$ to deduce $z_1=z_2$ . Also, the identity $\text{Log} (z_1z_2)= \text{Log}(z_1) + \text{Log}(z_2)$ can fail: the two sides can differ by an integer multiple of $2\pi \mathrm{i}$.
	
	For each nonzero complex number $w = x + i\mathrm{y}$, the principal value $\text{Log}(w)$ is the logarithm whose imaginary part lies in the interval $[-\pi,+\pi]$. The expression $\text{Log}(0)$ is left undefined since there is no complex number $z$ satisfying $e^z = 0$.
	
	Then the principal value of the complex logarithm can be defined by (\SeeChapter{see section Trigonometry}):
	
	We see also obviously that the function $\text{Log}(w)$ is discontinuous at each negative real number (we can see it on the figure above), but continuous everywhere else in $\mathbb{C}^*$.
	
	\subsection{Complex Integral Calculus}
	We have seen just above how to check if a complex function $f (z)$ was differentiable (it must at least respect the Cauchy-Riemann equations) at any point.
	
	Now let us see the opposite case... the integration that is absolutely fascinating in complex plane!
	
	We have obviously taking again the notations of the section of Differential and Integral Calculus:
	
	either in explicit form:
	
	Well once this expression established, let us give a little explanation about how to read it:
	\begin{enumerate}
		\item We know that $u$ and $v$ are dependent both in the general case of $x$ and $y$.
		
		\item We know that $ u $ and $ v $ represent (see examples at the beginning of this section) closed or open curves and also straight lines when $ x $ (or respectively $y$) is fixed and that the other associated variable varies!
	\end{enumerate}
	So each term have an integral in the above expression is in fact a line integral on a family of open or closed curves (including a specific case that is straight lines...)!
	
	This integral can be evaluated using the Green's theorem in the plane (\SeeChapter{see section Vector Calculus}) if we consider the particular case of a closed curvilinear path such as:
	
	Let us first study the real part:
	
	Indeed we proved (it is strongly advised to read again this Green's theorem) in the section of Vector Calculus that:
	
	What will be written in our situation:
	
	However, if the function is holomorphic and thus satisfies the Cauchy-Riemann equations we get immediately:
	
	Thus our integral is reduced in the particular case of a closed path:
	
	and... reusing Green's theorem for this imaginary part:
	
	However, if the function is holomorphic (for reminder that is to say differentiable at every point of the complex plane or an open subset of it) and thus satisfies the Cauchy-Riemann equations we get immediately:
	
	and we thus obtain the "\NewTerm{Cauchy theorem}\index{Cauchy theorem}", or "\NewTerm{Cauchy-Goursat theorem}\index{Cauchy-Goursat theorem}" for its generalized version for non continuous functions, which says that if a function is holomorphic (thus satisfying the Cauchy-Riemann equations) and integrated on a closed contour then:
	
	As a corollary (without proof), any function that satisfies the above relation is holomorphic (in the whole complex plane or an open subset of it).
	
	This result gives the possibility in certain fields like quantum physics fields (we think of the Yukawa potential that is not yet treated in this book in detailed) to calculate complicated real definite integrals using the above property. The idea is when choosing the closed contour of the path integral to play to make the real definite integral only as a part only of the path (by generalizing to the complex case) and by equality with zero we deduce its value thanks to the other parts of the integrals of the chosen path (parts that are obviously simple to calculate).
	
	In other words, the idea is to calculate by difference! The difficulty residing in practice in finding the function $f (z)$ and the closed contour that permits to make appear the function $f (x) $ of the researched definite integral...
	
	Using this result, let us make a very important academic example which will be useful later (but who has no connection with the case of calculating a real definite integral).
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us calculate:
	
	For this purpose, we will use the simplification that consist to remember (\SeeChapter{see section Numbers}) that:
	
	Therefore:
	
	We can then write the path integral as:
	
	Or as on a closed path differentiable at any point (without nodes) the angle to make a full turn will necessarily be between $0$ and $+\pi$. It comes then:
	
	\end{tcolorbox}
	Before we continue by noticing a very interesting and important fact the we will detail later formally: An integral (we do not speak of primitive but of integral!) of the type $1/x$ in $\mathbb{R}$ would not be calculable. But now if we generalize the concept of $\mathbb{C}$, we see that we get go around the singularity via a path integral that enclose the singularity. And ... and ... in our previous calculation $z$ might have only the real value and not the imaginary one (so $z$ reduce to $x$). So the integral of $1/x$ becomes calculable and has a result in the set of complex numbers which is remarkable!
	
	Some mathematicians interpret this by figuring that $1/x$ is a flat projection of a three-dimensional space in which the imaginary axis is perpendicular to the plane $\mathbb{R}^2$ (see figures below). Hence the fact that $1/x$ can integrated in the set $\mathbb{C}$.
	
	Finally, let us indicate that $1/z$ is holomorphic on the whole complex plane except on $0$ (the derivative being the same as $1/x$). Then the function $1/z$ is thus not $\mathbb{C}$-differentiable!
	
	This being done, let us do an important and similar case with the following path integral:
	
	where $z_0$ is a constant complex number. Let us write:
	
	We can then write if we make one turn counter clockwise:
	
	which is valid only if our integration path avoids $z_0$ what otherwise there is a singularity. This latter integral is a little simplistic generalization of the previous one.
	
	Now let us show the important theorem that interests us since the beginning of this section using many proven results so far!
	
	We know that if a function $f (z)$ satisfies the Cauchy-Riemann equations, the if we carefully avoid the value $z_0$ (as in the above calculations), the expression:
	
	is differentiable at all points except in on $z_0$ (where the expression is no longer holomorph) is name a "\NewTerm{singularity}\index{singularity}".

	Indeed, take a holomorphic function $f (z)$ satisfying Cauchy-Riemann equation and subtract a constant ($f(z_0)$) does not change the fact that the expression (in this case the numerator in previous relationship) remain holomorphic. Finally, multiply it by a fraction (the denominator of the above equation) which is also holomorph gives a holomorphic function. But singularities can then appear, we then speak of "\NewTerm{meromorphic functions}\index{meromorphic functions}" (this is the ratio of two holomorphic functions).
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	A meromorphic function is a function holomorphic in the whole complex plane, except possibly on a set of isolated points each of which is a pole (singularity) for the function (see further below for the concept of pole/singularity). The gamma function (see the plot in the Differential and Integral calculus section) is a famous example of meromorphic function!
	\end{tcolorbox}	
	So if we take the path integral on a closed path avoiding $z_0$, the Cauchy theorem gives us immediately (remember the proof above):
	
	However, this can also be written after rearrangement of terms:
	
	Therefore:
	
	But we have proved above that:
	
	Then we get the result named "\NewTerm{Cauchy's integral theorem}\index{Cauchy's integral theorem}", or more rarely "\NewTerm{Cauchy formula}\index{Cauchy formula}" (of which there is a generalized result we will prove later below):
	
	In fact, in practice all the subtlety is to be able to take back a given holomorphic function $g(z)$ (which therefore satisfies the Cauchy-Riemann equations) by manipulating it in a form of the type:
	
	when its possible... then the calculation of its path integral (closed path) becomes extremely simple since it will be equal to:
	
	by the Cauchy's integral theorem!
	
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
\textbf{R1.} So we know how to calculate the value of a path integral of an epxression that is not holomorp but for which the numerator is holomorph!\\

\textbf{R2.} Caution! The sign of the value of a path integral will depend on the direction in which its integration path will be done. If the direction is straightforward (that is to say "counterclockwise") its sign will be positive; if on the contrary the direction is clockwise his sign will be negative. You probably think that this information is irrelevant since this value is usually zero. Yes... it is, but we will see later the importance of this information when referring to the calculation of what we name the "residuals".
	\end{tcolorbox}
	
	There is a similar relation for the derivative $f'(z_0)$ to that given by the Cauchy's integral theorem. Let us see this:
	
	Therefore:
	
	thereby continuing, we have:
	
	In short, we therefore note that:
	
	which is the "\NewTerm{Generalized Cauchy's integral theorem}\index{Generalized Cauchy's integral theorem}"
	
	This result is very powerful because it shows that holomorphic functions are infinitely differentiable (because of the denominator), that is to say analytical, and it is much more difficult to find an equivalent theorem with such simple conditions for real functions.
	
	If we now return to our Taylor expansion of a complex function:
	
	um ... and what do we see here? Well this !:
	
	It follows the following relation named "\NewTerm{Laurent series in positive powers}\index{Laurent series in positive powers}" (there is a more generalized version will be prove later below):
	
	that gives us the formal expression of a complex function in the form of infinite series of integer powers near a point $z_0$ of the complex plane with therefore:
	
	Remembering that $d^{n}f(z_0)/\mathrm{d}z^n$ can be written equivalently $f^n(z_0)$, we see that all the two previous relations gives us the Taylor series expansion that we had obtained in real analysis (\SeeChapter{see section Sequences and Series}) and that was:
	
	Thus, the Taylor series in $\mathbb{R}$ are a special case of Laurent series that are in $\mathbb{C}$!!!
	
	This result is quite remarkable because it also shows that we can use the path integral in the complex plane for calculating the coefficients $c_n$ of the Laurent series instead of calculating the derivatives of order $n$ of the function $f$ if these latter are too complicated to determine. Or vice versa... calculate a simple derivation instead of calculating a headache type path integral (typically the case in physics) using the fact that:
	
	The only unfortunate point being that the latter relation is calculable only if we can put the function in path line integral in the form:
	
	where $n$ is a positive or null integer. This is honestly far from to be easy in most cases! The idea would be to find a general path for line integral, valid for any function $f (z)$ such that the denominator (which additionally contains a singularity on $z_0$) disappears. That would be ideal ... but we need a track ... and it will come from the study of the convergence of series of complex powers. Let's see what it is with a qualitative approach!
	
	\pagebreak
	\subsubsection{Convergence of a complex series}
	We saw in the section of Sequences and Series that many real functions could be expressed in Maclaurin series (special case of the Taylor series on $x_0=0$) in the form:
	
	We also showed, by example the only, that this series expansion of infinite powers was valid for some real functions only in a certain domain of definition named "radius of convergence".
	
	Even if this radius of convergence can be determined more or less easily in each case, there are some baffling examples that could not in the early 19th century be understood without complex analysis.
	
	Let's see a simple example to understand what kind of problem it is. Consider for this the two functions:
	
	and before continuing our example, recall that we have proved in the section of Sequences and Series the relation:
	
	relative to a geometric series, that is to say a series whose terms are of the type:
	
	Therefore it comes immediately if $n \rightarrow +\infty$ and $q \in ]-1,+1[$:
	
	if $u_0=1$, we get:
	
	So if we change the notation, we have:
	
	Then it comes immediately:
	
	Therefore the two previous functions $g(x)$ and $h(x)$ are defined for a infinite series expansion in powers only in radius of convergence $x \in ]-1,+1[$.
	
	We would get the same result by making a Maclaurin series expansion!
	
	We see that there is trivially for $g(x)$ two singularities that are $x=\left\lbrace -1,+1\right\rbrace$ by cons, basically we do not see trivial singularities for $h (x)$ if we reason only in $\mathbb{F}$ so it can be hard for the latter function to understand the origin of the radius of convergence!
	
	Indeed, if we trace these two functions in $\mathbb{R}$ with Maple 4.00b we get respectively:
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/g_h_example_functions.jpg}
		\end{center}	
		\caption{Complex Logarithm plot with Maple 4.00b}
	\end{figure}
	hence the problem of why there is still implicitly a radius of convergence $x \in ]-1,+1[$ for $h (x)$???
	
	An even more blatantly way to highlight the problem, is to show the approach of these two functions by a Maclaurin series expansion with ten terms.
	
	For $g(x)$ we get for example:
	
	\texttt{>with(plots):\\
	>xplot:= plot(1/(1-x\string^2),x=-5..5,thickness=2,color=red):\\
	>tays:= plots[display](xplot):\\
	>for i from 1 by 2 to 10 do\\
		tpl:= convert(taylor(1/(1-x\string^2), x=0,i),polynom):\\
		tays:= tays,plots[display]([xplot,plot(tpl,x=-5..5,y=-2..2,\\
		color=black,title=convert(tpl,string))])\\
		od:\\
	>plots[display]([tays],view=[-5..5,-2..2]);}
	
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/g_function_expansion_inspection.jpg}
		\end{center}	
		\caption{Plane representation of the function $g$ to visualize the problem}
	\end{figure}
	where we see well that the Maclaurin series (or expression in power series) does not converge outside $x \in ]-1,+1[$  which can be intuitive because of both singularities.
	
	For $h(x)$ we have by cons:
	
	\texttt{
	> with(plots):\\
	> xplot:= plot(1/(1+x\string^2),x=-5..5,thickness=2,color=red):\\
	> tays:= plots[display](xplot):\\
	> for i from 1 by 2 to 10 do\\
		tpl:= convert(taylor(1/(1+x\string^2), x=0,i),polynom):\\
		tays:= tays,plots[display]([xplot,plot(tpl,x=-5..5,y=-2..2,\\
		color=black,title=convert(tpl,string))])\\
	od:\\
	> plots[display]([tays],view=[-5..5,-2..2]);\\
	}
	
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/h_function_expansion_inspection.jpg}
		\end{center}	
		\caption{Surprisingly, here the Maclaurin series (in black) does not converge}
	\end{figure}
	where we see well that the Maclaurin series (or the expression in power series) does not converge either outside $x \in ]-1,+1[$ which was unsettling and against-intuitive at the beginning of the history of real analysis.
	
	Today even a high school student knows that he can also think in $\mathbb{C}$ and that $\mathbb{R} \subset \mathbb{C}$. So the reals analysis is just a special case and restricted of the field of complex analysis.
	
	The singularity for $h (x)$ in $\mathbb{C}$ comes that latter it is written:
	
	and there are therefore two singularities $z=\left\lbrace{-\mathrm{i},+\mathrm{i} }\right\rbrace$ that we see well if we represent:
	
	with Maple 4.00b (fortunately we now have the equivalent of a microscope in mathematics with Maple...):
	
	\texttt{>plot3d(abs(1/(1+(re+I*im)\string^2)),re=-3..3,im=-3..3,view=[-2..2,-2..2,-2..2]\\
	,orientation=[-130,70],contours=50,style=PATCHCONTOUR,axes=frame,\\
	grid=[100,100],numpoints=10000);}
	
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/g_inspection_in_C.jpg}
		\end{center}	
		\caption{Complex representation of the function $h$ to highlight the reason for the divergence}
	\end{figure}
	where we can see the two singularities on the imaginary axis and the function $h (x)$ on the real axis (between the two peaks). So when we develop a function in power series, we conclude that the radius of convergence is defined by the whole complex plane and not by the traditional axis of the real analysis.
	
	This makes it more natural to understand why we were talking in section of Sequences and Series of "radius" as seen from above, we have in the complex plane:
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/h_various_radius_convergence.jpg}
		\end{center}	
		\caption{Representation of the various convergence of radius of $h(z)$}
	\end{figure}
	hence the fact that we are talking sometimes about (open) convergence disk and sometimes of (open) convergence radius. Moreover, we notice on the chart that the domain of convergence is convex (any couple of points of the domain can be connected by a straight line that is in the area of convergence).
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Let us Recall that a subset, interval or "open" disc means that we do not take its border as we have seen in the section Topology.
	\end{tcolorbox}
	Then we understand better why the Taylor series does not converge trivially for $h(x)$: it must converge on the whole disc of the complex plane and not just converge on the real axis!
	
	From all this we deduce that our Laurent series in positive powers proved above:
	
	not necessarily converge, unsurprisingly... on the whole complex plane (just like the Taylor series on the real line as this is the equivalent!) but sometimes only in a opened subdomain (convex?) of this plane around $z_0$ (which in the particular example taken above was obviously: $0$).
	
	With our function $h(x)$ expressed using a development of Maclaurin with 5 terms, we see immediately with Maple 4.00b that on the borders of the square inscribed in the disc of convergence, the series does not converge and we're guessing the start of the two singularities:
	
	\texttt{>plot3d(abs(1-(re+I*im)\string^2+(re+I*im)\string^4-(re+I*im)\string^6+(re+I*im)\string^8),\\
	re=-0.7..0.7,im=-0.7..0.7,view=[-1.5..1.5,-1.5..1.5,0..1.5]\\
	,orientation=[-130,70],contours=50,style=PATCHCONTOUR,axes=frame,\\
	grid=[100,100],numpoints=10000);}
	
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/h_zoom_on_complex_representation.jpg}
		\end{center}	
		\caption{Focus on the complex representation to understand the reason for the divergence}
	\end{figure}
	a little outside the disc of convergence, we obviously have a little bit nonsense:
	
	\texttt{>plot3d(abs(1-(re+I*im)\string^2+(re+I*im)\string^4-(re+I*im)\string^6+(re+I*im)\string^8),re=-3..3,\\
im=-3..3,view=[-1.5..1.5,-1.5..1.5,0..1.5],orientation=[-130,70]\\
	,contours=50,style=PATCHCONTOUR,axes=frame,grid=[100,100],numpoints=10000);}
	
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/h_divergence.jpg}
		\end{center}	
		\caption{This diverges ... (stalactites ???)}
	\end{figure}
	There is still something interesting to try ... since we are now on a plane, not a straight line right (axis), it is possible for us to makethe Taylor expansion around a singularity $z_0$ by deforming the disk in a convex crown/ring simply connected as shown below (the crown/ring being the simplest simply convex geometry arising from the deformation of a disk):
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/h_representation_transformation_disc_in_crow.jpg}
		\end{center}	
		\caption{Representation of the deformation of a disc in a crown/ring}
	\end{figure}
	The advantage of this is to deform the area of convergence on the whole complex plane by avoiding (bypassing) all the singularities. Thus, unlike the Taylor series that are only valid on an interval of the $x$-axis, we would have a new type of series describing a function absolutely everywhere, that is to say before AND after (so around...) singularities!
	
	So obviously we will require that in the deformed crown above the function is always holomorph and analytical (as in the initial convex disc). Before determining what we are going down (generalized Laurent series!), we must first do a study of the decomposition of path integral:
	
	\pagebreak
	\subsection{Path Decomposition}
	The path integrals as given previously can also be written in another form almost classical and used many times in the literature.
	
	Let us see this. First, remember that we have just proved in the special case of a holomorphic function that:
	
	But a closed path can be seen as a path having a round trip:
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/closed_path.jpg}
		\end{center}	
		\caption{Representation of a closed path with round trip}
	\end{figure}
	Therefore we can write:
	
	And now comes what interest us... for this purpose let us focus one one of the path integral of the type:
	
	We already well know (1st form of notation) that any complex number $z$ of the type:
	
	can be (2nd form of notation) written as (Euler form):
	
	and to integrate on a path, nothing prevent us to choose a path where $r$ (the module) would be fixed and $\theta$ variable (we could not have the possibility to do this with the 1st form because by modifying the imaginary or real part, we can't get be guarantee to get a nice smooth curve but this is possible with the Euler form of a complex number)!
	
	Therefore we have:
	
	We write then naturally:
	
	and as:
	
	Therefore:
	
	what we often find in the following form in the literature:
	
	
	
	\subsubsection{Inverse Path}
	If $C$ is a curve going from a point $P$ to a point $Q$, then we denote by $C^-$  the same curve but traveled from $Q$ to $P$.
	
	Let us parametrized $C^-$:
	
	If $C(t)$ it is the curve defined on $[a, b]$, then we define the curve $C^-(t)$ on $[a, b]$ by:
	
	Indeed we have with this parameterization:
	
	and when $t$ increases from $a$ to $b$, $a + b - t$ decreases from $b$ to $a$. $C^-$ is therefore only $C$ but traveled in the opposite direction.
	
	We then have using the last proof:
	
	Let us put:
	
	Therefore:
	
	Then we have:
	
	Therefore if $C^-$ and $C$ are the paths of the same function but travel in the opposite direction, we have by taking our conventional notation (Caution! In the second term it is implicit that the parameterization is different from the first one!):
	
	Therefore:
	
	this is why we often say that the sign of the value of a line integral will depend on the direction in which its integration path is travel. If the direction is straightforward (that is to say "counterclockwise") its sign will be positive; if on the contrary the direction is clockwise his sign will be negative (\SeeChapter{see section Differential and Integral Calculus}).
	
	\pagebreak
	\subsection{Laurent Series}
	This last relation obtained, we can return to the deformation of our disc of convergence in a crown. We recall that initially the idea is to have the analytical expression of a function as an infinite series of powers in a limited area around a singularity point and all this... in the puprose to be able to calculate for physicists complex path integrals through a method using the properties of complex series!
	
	Let's start with the point (2) that is to say have a infinie power series for a path integra, which will take us more easily to point (1) that is to say get the an analytical expression of a function around a signularity point, by zooming on our crown:
	
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/crown.jpg}
		\end{center}	
		\caption{Zoom on our crown from our starting example}
	\end{figure}
	We therefore have if the function $f$ is analytic and holomorphic in the crown of outer radius $R$ and inside radius $r$, the following path curvilinear in the crown as we proved above (we change notation: $z=z'$ and $z_0=z$):
	
	
	therefore we denote now by $z$ the point where we want to know the function and $z'$ variable of which $f$ depends. This notation change will be justified later for a purely practical reason.
	
	The crown can be broken down into four paths:
	
	If both segments $C_c$ and $-C_c$ are infinitely close, they then correspond to the same path traveled once in a positive direction and once in the negative direction. As we have proved just above that:
	
	It therefore follows that:
	
	Which brings us to write:
	
	where we have put a "+" between the last two terms, because as we shall see immediately, the convergence criterion associated with the traditional notation in this field of study, makes automatically emerge the sign "-".
	
	For the two integral $f_1,f_2$, we know that the fraction can be written as a geometric series as already seen above. Effectively, starting from (now you will understand why we changed the notation):
	
	by assimilating:
	
	where as we have seen, the convergence requires that:
	
	so that $x$ is lower in absolute value to $1$.
	
	We then shee the infinite geometric series appearing:
	
	Therefore:
	
	To come back to:
	
	we have in any point $z$ inside the circle of radius $R$ whose border is described by the variable $z'$ and of center $z_0$ the convergence that is assured because:
	
	Then we can write:
	
	Integrating term by term integration, we highlight the development (already known):
	
	with the definition of coefficients $c_n$, where $n$ is a positive or null integer:
	
	This development may do think to the development of Taylor in the sense that only positive (or zero) powers of $(z'-z_0)$ appear, but this is not Taylor development in the case of the crown! Indeed, $c_n$ can not be written this time as:
	
	since, by assumption, $f(z)$ is assumed analytic in the crown only and may therefore very well not be inside the small circle of radius $r$, in particular on $z_0$, in which case $f^{n}(z_0)$ may simply not exist (let us repeat that $z$ is strictly constrained to be in the crown, therefore $r<\vert z \vert <R$). We will see later what happens when $f (z)$ is holomorphic in this disk and that, in particular, $z_0$ is not a singular point.
	
	We still need to treat $f_2$. We then do the same type of development as for $f_1$, with the difference that now:
	
	when $z'$ browse the small circle of radius $r$. To make a geometric series appear, we must write this time:
	
	Therefore:
	
	So we have:
	
	Integrating term by term, we highlight the (new) development:
	
	with:
	
	By changing $n$ in $-n$ in the summation for $f_2$, we have for the sum $f_1(z)+f_2(z)$:
	
	with at this time two distinct $c_n$:
	
	We will now see that these two relations can be combined into one!
	
	For this purpose if we observe well the last two relations, we find that they do not depend at all of $z$ (!) and this is normal since the $c_n$ are the coefficients of the series expansion of $f(z)$ and these are the same at any point of the domain of definition of the function where it is analytic!
	
	So the two contours (circles) can be merged into only one circle since it is located in the crown and has for center $z_0$:
	
	Furthermore, the attentive reader will have noticed that this contour does not even need to be a circle finally! It may be any geometry as long as it is closed and is located in an analytical area!
	
	Thus, we get the two relations:
	\begin{equation}
  \addtolength{\fboxsep}{5pt}
   \boxed{
   \begin{gathered}
   		\begin{aligned}
		f_(z)&=\sum_{n=-\infty}^{+\infty} c_n(z-z_0)^n\\
		c_n&=\dfrac{1}{2\pi\mathrm{i}}\int\limits_{\gamma_R} \dfrac{f(z')}{(z'-z_0)^{n+1}}\mathrm{d}z' \qquad (n=0,\pm 1,\pm 2, ...)
   		\end{aligned}
   \end{gathered}
   }
	\end{equation}
	The two previous relation define the "\NewTerm{general Laurent series}\index{general Laurent series}". It is remarkable and differs from a Taylor series in the sense that it contains all the positive and negative integer powers and the coefficients $c_n$ can a priori not be expressed with the derivatives of $f$.
	
	%--------------------
	The power series of $n\geq 0$ is named "\NewTerm{regular part}\index{regular part of a power series}", the negative powers is commonly named "\NewTerm{main part}\index{main part of a power series}".
	
	The series of negative powers converges uniformly everywhere outside $\gamma_r$, that of positive powers within $\gamma_R$. In total the development of Laurent converges uniformly in the common area, which is the crown and therefore also on the unique path $\gamma$.
	
	Let us now show a point that we have mentioned above. If the circle contains no singularity, then all the coefficients:
	
	are zero. First note that $-n-1$ is a positive or zero integer, which we will denote by $p$ such as:
	
	We then have the following integrand along a closed path:
	
	But, if we remove the singularity that requires $f(z')$ is holomorphic (and in anyway this is required by all initial developments of the Laurent series).
	
	As $(z'-z_0)^p$ is polynomial with positive and not null integer powers and that as we know any polynomial satisfying these conditions is differentiable at least once without showing singularity. Thus this term is also holomorphic.
	
	Assuming that the product of two holomorphic functions is holomorphic and that contour $\gamma$ is closed, then we have using the following result proved above (for a holomorphic function):
	
	the following immediate consequence:
	
	if there is no singularity in the small circle of the crown. We then fall back again on a development with only positive powers, the $c_{n\geq 0}$ this time being equal to:
	
	according to the generalized Cauchy integral theorem proved earlier above. Conversely, we see well that this is the main part (when it exists!) which contains the information on the fact that $f$ is not a priori holomorphic in the small disk. The existence of negative powers shows that $f$ is clearly not bounded on $z_0$.
	
	The classification of singularities of a function will be precisely based on the consideration of the characteristics of the main part of the Laurent  development centered on a singular point of this function.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us see to what looks like the Laurent serie of our famous example function:
	
	on a simply convex domain that would be the crown rounding the singularity  $\mathrm{i}$ for example (we could have taken the second singularity $-\mathrm{i}$ but we had to choose one to not repeat twice the explanations below...). This is equivalent therefore to search the power series development of $z-\mathrm{i}$. \\
	
	We will proceed as following:
	
	For what will follow we will use:
	
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	The second fraction can be expressed as a geometric series if as we have already seen:
	
	Therefore it comes:
	
	Let us multiply both sides of this equality by $-i / $2 and then divide them by $z - i$ (the second term in the denominator of the original fraction) for obtain the left term:
	
	and for the right term:
	
	Finally we have the following geometric series:
	
	We see then on this Laurent series around $\mathrm{i}$ of the holomorphic function $f(z)$ that the following coefficient appears:
	
	and then we have with Maple 4.00b:\\
	
	\texttt{>plot3d(abs(-I/2*1/((re+I*im)-I)-(I/2)\string^2-(I/2)\string^3*(re-I*im)-\\
	(I/2)\string^4*(re-I*im)\string^2-(I/2)\string^5*(re-I*im)\string^3),\\
	re=-1.5..1.5,im=-1.5..1.5,view=[-2..2,-2..2,-1..2],\\
	orientation=[-130,70],contours=50,style=PATCHCONTOUR,axes=frame,\\
	grid=[100,100],numpoints=10000);}
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	This gives the following figure:
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/laurent_series_representation.jpg}
		\caption{Laurent series representation of $f(z)$ with Maple 4.00b}
	\end{figure}
	where we see that the Laurent series allows us to express $f (z)$ in a neighborhood close to the singularity $\mathrm{i}$ by taking five terms.\\
	
	Ditto if we make the sum of the two Laurent series for the two singularities with seven terms:\\
	
	\texttt{>plot3d(abs(-I/2*1/((re+I*im)-I)-(I/2)\string^2-(I/2)\string^3*(re-I*im)-\\
	(I/2)\string^4*(re-I*im)\string^2-(I/2)\string^5*(re-I*im)\string^3 -(I/2)\string^6*(re-I*im)\string^4\\
	-(I/2)\string^7*(re-I*im)\string^5+I/2*1/((re+I*im)+I)+(I/2)\string^2+
(I/2)\string^3*(re+I*im)+(I/2)\string^4*(re+I*im)\string^2+(I/2)\string^5*(re+I*im)\string^3\\
	+(I/2)\string^6*(re+I*im)\string^4
+(I/2)\string^7*(re+I*im)\string^5),re=-1.5..1.5, im=-1.5..1.5, view=[-2..2,-2..2,-1..2],orientation=[130,70], contours=50, style=PATCHCONTOUR, axes=frame,grid=[100,100],\\
numpoints=10000);}

	This give the image visible on the next page:
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	This gives the following figure:
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/sum_of_two_laurent_series.jpg}
		\caption{Sum of the two Laurent Series of $f(z)$ for both singularities with Maple 4.00b}
	\end{figure}
	\end{tcolorbox}
	
	\subsection{Singularities}
	We have seen just before that it was possible to calculate the path integral of a function, on condition of analyticity, on the outline of a singularity. Our goal will now be to enhance this approach.
	
	We have already mentioned and highlighted in our previous proofs that the integrant in the "Cauchy integral theorem" was of the form:
	
	where $f(z)$ is well defined in $z_0$.
	
	The point $z-z_0$ is of course a singularity of $f(z)$ and it is not defined there.
	
	As we saw during our proof of Laurent series, $f(z)$ can be expressed as a Laurent series in the form a positive power Laurent series in a convergence disk (or what remains the same: as a series of Laurent in a crown not centered on a singularity...) in the form:
	
	Before continuing, it is customary in mathematics to define a small conventional vocabulary regarding this time the possible singularities of $f(z)$!
	
	Let us first recall that we know, and that we have proved, that all information on the singularities of $f (z)$ are contained in the main part of the Laurent series (negative powers) defined on the crown surrounding $z_0$:
	
	The following classification focus on "\NewTerm{isolated singularities}\index{isolated singularities}", that is to say, a singular point where $f(z)$ is analytic everywhere in the neighborhood excepted on $z_0$. This classification, as we will see permits us to distinguish three types of singular points, will be useful when developing the theory of residues further.
	
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] When the limit of the function $\vert f(z) \vert$ exists on $z_0$, we say that the singularity is a "\NewTerm{removable singular point}\index{removable singular point}" or "\NewTerm{apparent singularity}\index{apparent singularity}".
		
		For example:
		
		does not seem to be defined on $z=z_0=0$ but we have a numerator having a Laurent series without negative powers (therefore a simple Taylor series). It then comes into by doing the Maclaurin series (that is to say the Taylor series on $z=z_0=0$...):
		
		We then see that $f (z)$ finally has no term with negative power and therefore we have eliminated the singularity (or that it contains simply no signularities... which can easily be check with Maple 4.00b).
		
		\item[D2.] When on $z_0$ the limit $\vert f(z) \vert $ does not exist we speak about "\NewTerm{essential singularity}".
		
		For example, $z_0=0$ is an essential singularity for the function:
		
		Indeed, if $z$ approaches zero coming from the positive real axis $\mathbb{R}_+$, the function diverges, more precisely, it tends to $+\infty$. If $z$ comes from $\mathbb{R}_-$, the function tends to zero as illustrated by the following Maple 4.00b plot:
		
		
		\texttt{>plot3d(abs(exp(1/(re+I*im))),re=-5..5,im=-5..5,\\
		view=[-3..3,-3..3,-0.5..3],orientation=[-130,70],contours=50,\\
		style=PATCHCONTOUR,axes=frame,grid=[100,100],numpoints=10000)\\
		>plot3d(abs(exp(1/(re+I*im))),re=-5..5,im=-5..5,\\
		view=[-3..3,-3..3,-0.5..3],orientation=[-130,70],contours=50,\\
		style=PATCHCONTOUR,axes=frame,grid=[100,100],numpoints=10000)}
		\begin{figure}[H]
			\centering
			\includegraphics{img/analysis/plot_essential_singularity.jpg}
			\caption{Essential singularity example with $e^{1/z}$ in Maple 4.00b}
		\end{figure}
		Indeed:
		
		So an equivalent way of defining an essential singularity, is to say that there are an infinite number of terms with negative powers in the main part of the Laurent series.
		
		\item[D3.] When on $z_0$ the limit of $\vert f(z) \vert$ is $+\infty$, we speak about a "\NewTerm{pole}".
		
		This is the last category (as far as we know...) in which we can store a function that is not classifiable neither in the first nor in the second definition above.
		
		So another equivalent way of defining a "pole", is to say that there is a finite number of terms with negative powers in the main part of the Laurent series. If the number of terms is $k$, then we speak of "\NewTerm{pole of order $k$}".
		
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} We sometimes say that an essential singularity is a " \NewTerm{pole of order $+\infty$}".\\
	
	\textbf{R2.} A pole of order 1 is named a "\NewTerm{simple pole}". One of order 2 is named a "\NewTerm{double pole}" and so on...
	\end{tcolorbox}
	\end{enumerate}
	If we come back on our example:
	
	We have proved previously that que Laurent series of the function was:
	
	This function has therefore a trivial pole of order $1$ on $z_0=\mathrm{i}$ and also on $z_0=-1$ because in this latter case this infinite series diverge to $+\infty$ and we can easily check this with the following Maple 17.00 command:
	
	\texttt{>sum(-(I*(1/2))\string^n*(-2*I)\string^(n-2), n = 0 .. infinity)}
	
	\subsection{Residue Theorem}
	Consider a function $f(z)$ whose pole is of order less or equal to $k$.
	
	Let us make it  analytic:
	
	that is to say that we have take a function $f(z)$ that we have made analytic after elimination of the poles supposed in finite number - order - less than or equal to $k$ on $z_0$. 
	
	This function $\phi(z)$ has therefore a Laurent series development  in a disc center on $z_0$.
	
	As we have prove it previously, we can therefore by using the following relation:
	
	write:
	
	Using $f(z)$ under the integral it comes:
	
	You must deeply analyze this relation and understand that it link together the integral of a function having singularities with the value on one point of an analytical function having no more singularities!!!
	
	This latter relation can be rewritten by rearranging terms:
	
	And by expressing $\phi^{(k)}(z_0)$ by using (this is authorized because this latter function is analytical) the fact that by definition:
	
	We get obviously:
	
	Therefore by making $\phi(z)$ explicit again:
	
	This latter relation is valid onlye for ONE isolated singularity (in case you forget!) and where $k$ is equal at least to $1$!

	Mathematicians therefore define:
	
	as being the residue of the function $f(z)$ at the point being an isolated singularity of order $k$. Or respectively:
	
	where the path integral is centered on $z_0$.
	
	Now notice that the term on the right of the equality in the previous relation correspond to the coefficient $c_{-1}$ of the Laurent Serie. Indeed:
	
	Therefore:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Therefore it comes that on an isolated singularity that can be eliminated, the residue is null because as we saw it before, the path integral rounding a domain without singularity is equal to zero!
	\end{tcolorbox}
	To resume, the relation:
	
	is very interesting for the physicist... because this is a very elegant way for him to calculate the path integral of a non analytic function $f(z)$ having a unique isolated singularity and this just by knowing the order of its poles!
	
	For example if a function $f(z)$ has only a pole of order $1$, we have therefore:
	
	and we replace therefore $z_0$ by the desired value in the parenthesis $(z-z_0)$ and after we calculate the limit between brackets!
	
	Now to go more fare, let us remind that outline of the path integral:
	
	and the curvilinear path of the integral:
	
	are in fact combined (identical) and the coefficients $c_n$ do not depend on $z$! The only constraint on the path is that is is closed and in an analytical domain centered on one point.
	
	So if we have several isolated singularities, surrounded by connected curvilinear paths as shown below on the complex plane of a function having a pole of order 3 (i.e. three non-removable singularities $z_0,z_1,z_2$):
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/multiple_surrounded_singularities.jpg}
		\caption{Multiple isolated singularities surrounded by curvilinear paths}
	\end{figure}
	then we have still only one closed curvilinear path but whose different isolated singularities are connected by cross which as we know: the paths that are opposed  cancel themselves! And let us remind that the coefficients are the same throughout on all the path since it is on an analytical domain.
	
	We then have the generalized version of the residue theorem for a function $f$ with $n$ isolated singularities:
	\begin{equation}
  		\addtolength{\fboxsep}{5pt}
	   \boxed{
	   \begin{gathered}
	   		\begin{aligned}
			\oint f(z)\mathrm{d}z&=2\pi\mathrm{i}\sum_{i=1}^n \text{Res}\left[f(z);z_i\right]\\
			\text{Res}\left[f(z);z_i\right]&=\lim_{z \rightarrow z_0}\dfrac{1}{(k_i-1)!}\dfrac{\mathrm{d}^{k_i-1}}{\mathrm{d}z^{k_i-1}}\left[(z-z_i)^{k_i}f(z)\right]
	   		\end{aligned}
	   \end{gathered}
	   }
	\end{equation}
	with a rigorous approach that is specific to engineers ... who sometimes write this latter relation as following:
	
	where $r$ is therefore a residue. This is an important result in the field of solving differential equations associated with some inverse Laplace transforms (\SeeChapter{see section Functional Analysis}). This intermediate result will give us the possibility to get an another one a little further of major importance for the section of Corpuscular Quantum Physics.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us take again our famous function:
	
	We know it has a pole of order $1$ on $z_0=\mathrm{i}$ and a pole of order $1$ on $z_0=-\mathrm{i}$. So if we take this time Laurent series with a path that surrounds the two singularities (and not only one) then we have a function with a pole of order $2$.
	
	It comes then for this particular case:
	
	with $n$ being equal to $2$.
	
	Then we have:
	
	and:
	
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	We can easily check this with Maple 4.00b:\\
	
	\texttt{>readlib(singular):\\
	>singular(1/(1+z\string^2),z);\\
	>readlib(residue):\\
	>residue(1/(1+z\string^2),z=-I);\\
	>residue(1/(1+z\string^2),z=I);\\}

	and therefore:
	
	In fact in this case, the residue theorem gives zero because the function has no poles to infinity which is true since in our example:
	
	Physicists meanwhile say that... the force does make any work on this path ...!
	\end{tcolorbox}
	
	\subsubsection{Pole at infinity}
	We have say before that any function that did not have poles at infinity had therefore the sum of the residues of all the poles that are equal to zeros. This result is very important in physics and merits to be study!
	
	It is almost trivial to recognize the number of poles... but to recognize the poles that are at infinity there are many times traps we can easily fall in.
	
	Let us consider the expression $f(z)\mathrm{d}z$. If $z$ is at the neighborhood of the infinity then $1/z$ is neare $0$. Let us write:
	
	Then we have:
	
	Then the residue at infinity is such that:
	
	with:
	
	Therefore with:
	
	The latter relation we will be indispensable to us in the section of Corpuscular Quantum Physics Corpuscular to build the relativistic Sommerfeld model of hydrogenous atom because we will need to to calculate a path integral with a pole.
	
	Let's see an example with the function that accompanies us since the beginning of this section. That is to say:
	
	Therefore it comes:
	
	And we recognize immediately the initial function, in absolute value, and that has therefore no pole on $0$. Therefore $f(z)$ has no pole at infinity.
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{100} & \pbox{20cm}{\score{3}{5} \\ {\tiny 14 votes,  61.43\%}} 
	\end{tabular} 
	\end{flushright}
	
	%to force start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Topology}
	\lettrine[lines=4]{\color{BrickRed}T}opology is an extremely broad field of mathematics for which it is difficult to define precisely the object so the areas where it applies are varied (real line topology, graphs topology, differential topology, complex topology, symplectic topology, etc.). 

We mainly make a distinction with:
	\begin{itemize}
		\item "\NewTerm{General topology}" that establishes the foundational aspects of topology and investigates properties of topological spaces and investigates concepts inherent to topological spaces. It includes point-set topology, which is the foundational topology used in all other branches (including topics like compactness and connectedness).
		
		\item "\NewTerm{Algebraic topology"} tries to measure degrees of connectivity using algebraic constructs such as homology and homotopy groups.
		\item "\NewTerm{Differential topology"} is the field dealing with differentiable functions on differentiable manifolds. It is closely related to differential geometry (see section of the same name in the chapter about Geometry) and together they make up the geometric theory of differentiable manifolds.
		\item "\NewTerm{Geometric topology}" primarily studies manifolds and their embeddings (placements) in other manifolds. A particularly active area is low dimensional topology, which studies manifolds of four or fewer dimensions. This includes knot theory, the study of mathematical knots.	
	\end{itemize}
	What we can say at first is that in its foundations Topology is very closely related to the set theory, the study of convergence of sequences and series, functional analysis, analysis complex, the differential and integral calculus, vector calculus and the geometry to mention only the most important cases that the reader can already found in this book.

	The origin of Topology comes from the problems that laid to the progress of functional analysis in the rigorous study of continuous functions, their differentiability, their limits at a point (finite or note), the existence of extremums, etc. in higher-dimensional spaces (in fact, implicitly, the goal for topology is to create tools that easily allow to study the properties of functions in all dimensions). All these concepts, needed a rigorous mathematical definition of the intuitive idea of proximity, especially when doing operations on such functions.

	We will try in this section to identify the basis of the structures that allow us to speak about limits and continuity and this only for curiosity as consultants in R\&D and financial engineering we never saw a business application where the subjects below are absolutely necessary to develop a new business or solve a problem. 
	
	The majority of examples that we will take in this section will be in $\mathbb{R}$ (the $\mathbb{R}$ straight line to be more exact...) because it is the most used one by the engineers (and most of times the only one!) and the one we will have to use for the sections on Graph Theory, Statistics, Differential and Integral Calculus and also on Fractals. When we restrict our study of Topology on $\mathbb{R}$ we then speak of "\NewTerm{Real Analysis}".

	\subsection{General Topology}

General topology is the branch of topology dealing with the basic set-theoretic definitions and constructions used in topology. It is the foundation of most other branches of topology, including differential topology, geometric topology, and algebraic topology.

The fundamental concepts in point-set topology are "\NewTerm{continuity}", "\NewTerm{compactness}", and "\NewTerm{connectedness}".  
	\begin{itemize}
		\item Continuous functions take \underline{nearby} points to nearby points.
		\item Compact sets are those that can be covered by finitely many sets of \underline{arbitrarily small} size. 
		\item Connected sets are sets that cannot be divided into two pieces that are \underline{far apart}. 
	\end{itemize}		
	The words \underline{nearby}, \underline{arbitrarily small}, and \underline{far apart} can all be made precise by using open sets. If we change the definition of open set, we change what continuous functions, compact sets, and connected sets are. Each choice of definition for open set is named a "\NewTerm{topology}". A set with a topology is named a "\NewTerm{topological space}".

	"\NewTerm{Metric spaces}" are an important class of topological spaces where distances can be assigned a number named a "\NewTerm{metric}". Having a metric simplifies many proofs, and many of the most common topological spaces are metric spaces.
	
	An "\NewTerm{inner product}" (\SeeChapter{see section Vector Calculus}) induces a "\NewTerm{norm}"  (\SeeChapter{see section Vector Calculus}) and the norm induces a metric space. 
	
	Therefore we understand better what we will study in this section that can be summarized by the following figure:
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/topological.jpg}
	\end{figure}

	\subsubsection{Topological Spaces}
	Topological spaces form the conceptual foundation on which the concepts of limit, continuity or equivalence are defined.
	
	The framework is general enough to be applied in many different situations: finite sets, discrete sets, geometry spaces, $n$ dimensional numerical spaces and most complex functional areas. These concepts appear in almost all branches of mathematics, they are therefore central to the modern view of mathematics.
	
	\textbf{Definition (\#\mydef):} Consider a nonempty set $X$ (the length of a plastic ruler for example). A "\NewTerm{topology $\mathcal{T}$}" or "\NewTerm{topological space $(x,\mathcal{T})$}" on $X$ is a family $\mathcal{T}$ of parts of $X$ (of length of our rule...) named "\NewTerm{open $V$}"  (as the open intervals seen in the section  of Functional Analysis) such that the following axioms are true:
	\begin{enumerate}
		\item[A1.] The empty set $\varnothing$ and $X$ are considered as open $V$ and must belong to the family of the topology $\mathcal{T}$ (these only both open sets define what we name the "\NewTerm{trivial topology}" that is the most minimal one satisfying all the axioms):
		
		In other words, if we imagine our plastic ruler, the measure zero (strictly speaking: the empty set) must belong to the topology defined on the ruler and the ruler itself (seen as a subset).
		
		\item[A2.] Any finite intersection of open of $\mathcal{T}$ will be an open of $\mathcal{T}$:
		
		
		\item[A3.] Any union of open of $\mathcal{T}$ will be an open of $\mathcal{T}$:
		
		\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
		\textbf{R1.} Mathematicians frequently note by $O$ the family of open sets and by $F$ the family of closed sets. Convention we will not follow in this book.\\

		\textbf{R2.} The close sets of a topology are complementary of open sets. Therefore, the family of  close sets contains among other $X$ and the empty set $\varnothing$...\\
	
		\textbf{R3.} There is no difference between part and subset of a set.
		\end{tcolorbox}
		
		\item[A4.] The couple $(X,\mathcal{T})$ is a "\NewTerm{Hausdorff space}" or "\NewTerm{separate space}" if moreover the property named "\NewTerm{Hausdorff axiom}" is verified:
				
	\end{enumerate}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
		\textbf{R1.} A well known example of topological space is $\mathbb{R}$ provided with the set $F$ generated by the open intervals (by the union law), that is to say the intervals of the type $] a, b [$.\\
		
		\textbf{R2.} We will see a very concrete and beautiful + nice application of Hausdorff spaces in our study of fractals in the chapter Theoretical Computing.
	\end{tcolorbox}
	
	\textbf{Definition (\#\mydef):} 
	\begin{enumerate}
		\item[D1.] If we denote by $(X, V)$ a a topological space, $V$ designating the open sets of $X$, a "\NewTerm{base}", in the topological sense, of $(X, V)$ is a part $B$ of $V$ such that any open set of $V$ is a an union of open sets of $B$ (this is the same idea as vector spaces but in fact applied to sets ... nothing bad and difficult! If you want an example see the section of Measure Theory).
		
		\item[D2.] In topology a subset $A$ of a topological space $X$ is named "\NewTerm{dense}" (in $X$) if for every point $x$ in $X$ either belongs to $A$ or is a limit point of $A$. Informally, for every point in $X$, the point is either in $A$ or arbitrarily "close" to a member of $A$. For instance, every real number is either a rational number or has one arbitrarily close to it. Therefore $\mathbb{Q}$ is dense in $\mathbb{R}$.
	\end{enumerate}
	
	\subsection{Metric Space and Distance}
	\textbf{Definition (\#\mydef):} A "\NewTerm{metric space}" denoted by $(X, d)$ or sometimes $X_d$ (or evend sometimes just $X$ if the type of distance $d$ cannot not be confused) is by definition a set $X$ with provided with an application:
	
	named "\NewTerm{distance}" or "\NewTerm{metric}", which satisfies the following axioms:
	\begin{itemize}
		\item[A1.] Positivity:
		
		
		\item[A2.] Separation:
		
		\item[A3.] Triangular inequality:
		
		
		\item[A4.] Symetry: 
		
	\end{itemize}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
		\textbf{R1.} Some readers will probably see immediately that some of these properties have already been seen in other sections of this book during our study of of distances between functional points and during our study of norms (triangle inequality proved in the section of Vector Calculus - the symmetry, positivity, the separation have already been study in the section of Functional Analysis).\\
		
		\textbf{R2.} Some authors omit the axiom A1 which is strictly correct as it trivially follows from A3.\\
	\end{tcolorbox}
	The "distance function" of $\forall x,y \in X$ is thus usually denoted in the more possible sense in mathematics (at least as far as we know):
	
	we will see three examples much more further below with a schema.
	
	\textbf{Definition (\#\mydef):} If we do not impose the axiom A2, we say that $d$ is a "\NewTerm{semi-distance}" on $X$ and if we allow a semi-distance $d$ to take the value $+\infty$, we prefer to say that $d$is a "gap".
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
		\textbf{R1.} If a distance $d$ satisfies the property:
		
		property more restrictive that the triangle inequality in some spaces, we say that $d$ is "\NewTerm{ultrametric}".\\
		
		An example of ultrametric distance is the family tree (...):
		
		\begin{figure}[H]
			\centering
			\includegraphics{img/analysis/family_tree.jpg}
			\caption{Example of ultrametric distance with an orgchart}
		\end{figure}
		We have the following distances:
		
		We note that the distances above do not add up, but we have by cons:
		
		Therefore:
		
		
		\textbf{R2.} Let $(X, d)$ be a metric space and consider $F=\varnothing$ a part of the set $E$. The metric space $(F,\delta)$ where $\delta$ denotes the restriction $d_{F \times F}$ of $d$ is named "\NewTerm{metric subspace}" of $(X, d)$ (we should check that the distance $d$ is equivalent to the distance $\delta$). In this case, we also say that $F$ is provided with the distance induced by this of $X$. We therefore simply note $d$ the induced distance.
	\end{tcolorbox}

	\pagebreak
	Let us give now some examples:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. If we take for $X$ the plane, or the three-dimensional space of Euclidean geometry and a unit of length, the "distance" in the usual sense is a distance within the meaning of the 4 axioms mentioned above. In these spaces, the three points $A, B, C$ satisfy as we have proved it in section Vector Calculus:
	
	with other inequality obtained by circular permutation of $A, B, C$. These inequalities are well known, for example between the side lengths of a triangle.\\
	
	E2. If we take $X=\mathbb{R}^n$, $n \in \mathbb{N} \geq 1$ and that we equip $\mathbb{R}^n$ of an Euclidean vector space structure (and not non-Euclidean!) and we take two points:
	
	in $\mathbb{R}^n$, the distance is then given by (we have already proved this in the sections of Functional Analysis and Vector Calculus):
	
	\end{tcolorbox}
	This latter distance satisfies the five axioms of distance and we name it the "\NewTerm{Euclidean distance}". We can take (it is an interesting property for the general culture), any relation of the form:
	
	is also a distance in $\mathbb{R}^n$ (without proof). In the particular case with $n=1$, we have of course:
	
	This is the usual distance on $\mathbb{R}$.
	
	Mathematicians are even stronger by generalizing ever more (the proof has little interest for now in this book) the prior-previous relation (taking into account the definition of the distance) in the form:
	
	which is named "\NewTerm{HÃ¶lder distance}".
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	Following the intervention of a reader we would like to point out that strictly speaking the above inclusion should be noted $[1,+\infty[ \subset \mathbb{\overline{R}}$ where $\mathbb{\overline{R}}$ is the achieved line (also valid for precision for the Minkowski inequality below).
	\end{tcolorbox}	
	As for the triangle inequality, then given by (\SeeChapter{see section Vector Calculus}):
	
	The generalization, by the verification of the existence of the HÃ¶lder distance, gives us true "\NewTerm{Minkowski inequality}":
		

	Let us continue with our examples:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E3. If we take $X=\mathbb{C}$ we will consider the distance:
	
	Therefore if $z=a+\mathrm{i}b=(a,b)$ and $z'=a'+\mathrm{i}b'=(a',b')$ we have the module that the same manner as norm in $\mathbb{R}^2$, forms a distance:
	\\
	
	E4. Let us consider $E=\varnothing$ an arbitrary set. Let us write:
	
	It is quite check that this distance satisfies the five axioms and that is furthermore an ultrametric distance. This distance is named "\NewTerm{discreet distance}" and the reader will notice that, by analogy, we choosed to express this distance by the Dirac symbol $\delta$ (this is not innocent !!) rather than the traditional $d$.
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{Equivalent Distances}
	Sometimes two different distances $d$ and $\delta$ on the same set $E$ are quite similar so that the related metric spaces $(E,d),(E,\delta)$ have the same properties for certain mathematical objects defined by $d$ on one hand, and by $\delta$ on the hand. There are several concepts of equivalences for example first (before the others that require mathematical tools that we have not yet defined):

	\textbf{Definition (\#\mydef):} Let $d$ and $\delta$ be two distances on the same set $E$, $d$ and $\delta$ are named "\NewTerm{equivalent distances}" if there are two real constants $c>0,C>0$ such that:
	
	Therefore:
	
	with $c\leq C$. We note this equivalence by:
	
	The advantage of this definition is the following: if we have convergence for one of the metric, then we have convergence for the other too. More clearly:
	
	verbatim:
	
	
	\subsubsection{Lipschitz Functions}
	With respect to the above definitions, we can now assign some additional properties to functions such as we had define in the section of Set Theory or Functional Analysis and analyzed (in part...) in the section of Differential and Integral Calculus. The idea is also mainly to build a set of tools enabling the study of differential properties of non differentiable functions.
	
	Let $(E, d)$ and $(F,\delta)$ be metric spaces, and $f:E \rightarrow E$ a function. We define the following properties:
	\begin{enumerate}
		\item[P1.] We say that $f$ is an "\NewTerm{isometry}" if (it is rather intuitive ...!):
		
		
		\item[P2.] If we take the usual distance, the $L$-Lipschitz or "\NewTerm{Lipschits function of order $L$}" is then defined by on a given interval by:
		
		that we can also write:
		
		or what remains the same: all line drawn between two arbitrary points of the graph must have a bounded and finite slope coefficient (derivative) between $L$ and $-L$.
		
		Any such $L$ is referred to as a "\NewTerm{Lipschitz constant}" for the function $f$. As $L$ can be define on intervals (not necessarily the whole domain of definition) smallest constant is sometimes named the "\NewTerm{best Lipschitz constant}".
		
		Otherwise, one can equivalently define a function to be Lipschitz continuous if and only if there exists a constant $L$ such that, for all $x\neq y$:
		
		For real-valued functions of several real variables, this holds if and only if the absolute value of the slopes of all secant lines are bounded by $k$. 
		
		Therefore all Lipschitz must be continuous and any function $f$ that has a bounded $L$ value is more restrictive than just simply being continuous! 
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Examples:}\\\\
		E1. The function $f(x)=\sin (x)$ is $1$-Lipschitz as the derivative of the cosine is between $-1$ and $1$.\\
		
		E2. The function $f(x)=x^2$ is locally Lipschitz as for any interval closed and finite interval we can found a bound $L$ but is not globally Lipschitz as when $x\rightarrow \pm\infty$ then the derivative has also no bounds.\\
		
		E3. The function $f(x)=|x|$ has no derivatives on $x=0$ in the ordinary sense. But its derivative in the Lipschitz sense on $x=0$ is given by the a closed interval denoted $\partial_L f(0)=[-1,1]$ given by the bound of $L$ as the ordinary derivatives is less than or equal to $L$ in absolute value!\\
		
		This last example show us that the notion of a local minimum of a function $f(x)$ in the ordinary sense is generalized with Lipschitz condition. As we can now simple defined the condition of local minimum of a non-smooth function $f(x)$ as:
		
		rather than in the ordinary sense (more restrictive):
		
	\end{tcolorbox}
		
		
		Schematically for a Lipschitz continuous function, there is a double cone (shown in white) whose vertex can be translated along the graph, so that the graph always remains entirely outside the cone.
		\begin{figure}[H]
			\centering
			\includegraphics{img/analysis/lipschitz.jpg}
			\caption{Example of Lipschitz function (source: Wikipedia)}
		\end{figure}
		
		Intuitively, a Lipschitz continuous function is therefore limited in how fast it can change: there exists a definite real number such that, for every pair of points on the graph of this function, the absolute value of the slope of the line connecting them is not greater than this real number; this bound is named a "Lipschitz constant" of the function (or "modulus of uniform continuity"). For instance, every function that has bounded first derivatives is Lipschitz.
		
		\item[P3.] If $L=1$, we say that the function $f(x)$ is a "\NewTerm{short map}". If $-1<L<1$, we say that $f(x)$ is "\NewTerm{strictly contracting}".
		
		\item[P4.] We say that two metric spaces are "\NewTerm{isometric spaces}" if there is a surjective isometry of one over the other (which is quite natural in geometry ...).
	\end{enumerate}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} An isometry is always injective as:
	
	but in general it is not surjective.\\
	
	\textbf{R2.} If $(E,d)$ and $(F,\delta)$ are isometric, of the point of view of the theory of metric spaces they are not discernible, as all their properties are the same, but their elements can be  of very different nature (sequences in one and functions in the other).\\
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{Continuity and Uniform Continuity}
	As we already see it in the section of Functional Analysis, a a continuous function is, roughly speaking, a function for which small changes in the input result in small changes in the output and that permits the analysis of limits. Otherwise, a function is said to be a discontinuous function. Formally it was defined by:
	
	In other words remember that this mean that a function is continuous if for every point $x_0$ in the domain $E$, we can make the images of that point ($f(x_0)$) and another point ($f(x)$) arbitrarily close (of a distance $\varepsilon$) if we move the other point ($x$) close enough (distance $\delta$) to our given point.
	
	Hence it is not continuous if:
	
	
	The previous definition is not so good as it make usage of a special case of distance (the absolute value). It is therefore more common to generalize by writing:
	
	
	Now let us state a more restrictive definition!
	
	\textbf{Definition (\#\mydef):} A function $f(x)$ is "\NewTerm{uniformly continuous}" if it satisfies:
	
	with $\lambda=\varepsilon/L$ and $L\neq 0$. In other words, if we can bring two points as close as we want in a space, so can we in the other way (which ensures somehow the derivation.
	
	Hence it is not uniformly continuous if:
	
	
	If we compare the two relations:
	
	the only difference is the order of the quantifiers. Indeed, for something to be continuous, you can check "one $x$ at a time", so for each $x$, you pick a $\varepsilon$ and then find some $\varepsilon>0$ that depends on both $x$ and $\varepsilon$ so that $|f(x)-f(x_0)|<\varepsilon$ if $|x-x_0|<\delta$. If we want uniform continuity, we need to pick first a $\varepsilon$, then find a $\delta$ which is good for ALL the $x$ values we might have.
	
	As the previous definition are not quite easy for everybody let us see the engineer version of these two definitions:
	
	\pagebreak
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] A function $f(x):E \rightarrow \mathbb{R}$ is "\NewTerm{continuous}" at a point $x_0\in A$ if, for all $\varepsilon>0$, there exists a $\delta>0$ such that whenever $|x-x_0|<\delta$ (and $x\in E$) it follows that $|f(x)-f(x_0)|<\varepsilon$.
		
		\item[D2.] A function $f(x):E \rightarrow \mathbb{R}$ is "\NewTerm{uniformly continuous}" on $A$ if, for all $\varepsilon>0$, there exists a $\delta>0$ such that whenever $|x-x'|<\delta$ (and $(x,x')\in E$) it follows that $|f(x)-f(x')|<\varepsilon$.
	\end{enumerate}
	Therefore we see better the difference: continuity is defined at a point $x_0$, whereas uniform continuity is defined on a set $E$. Roughly speaking, uniform continuity requires the existence of a single $\delta>0$ that works for the whole set $E$, and not near the single point $x_0$.
	
	From this definition wee that any uniformly continuous function is continuous but the reciprocity is not true (any uniformly continuous function is not necessarily continuous):
	
	If the chosen distance is known (for example the absolute value for scalar functions such that $d=|\cdot|$ and $\delta=|\cdot|$) the previous definition notation change obviously a little bit:
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The function $f(x) = x^2$ is continuous but not uniformly continuous
on the interval $E = [0,+\infty[$.\\

	We prove first that our function $f(x)$ is continuous on $E$. Remember first that:
	
	In our case we can therefore write and check that:
	
	
	Let us choose $x_0=a-1$ with $a>1$ and $\delta=\min(1,\varepsilon/2a)$ (note that $\delta$ depends on $x_0$ since $a$ does). Choose $x \in S$. Assume $|x-x_0|<\delta$. Then $|x-x_0|<1$ so $x<x_0+1$ so $x,x_0<a$ so:
	
	We prove now that $f(x)$ is not uniformly continuous on $E$, i.e.:
	
	Let $\varepsilon=1$. Choose $\delta>0$. Let $x_0=1/\delta$ and $x=x_0+\delta/2$. Then $|x-x_0|=\delta/2<\delta$ but:
	
	as required.
	\end{tcolorbox}
	
	\subsection{Opened and Closed Set}
	\textbf{Definition (\#\mydef):} Consider a set $E$ with a distance $d$. A subset $U$ of $E$ is named "\NewTerm{open subset}" if, for each element of $U$, there is a non-null distance $r$ for which all the elements of $E$ whose distance from this element is less than or equal to $r$, belong to $U$, which gives in mathematical language:
	
	In topology, an open subset is then only an abstract concept generalizing the idea of an open interval in the real line.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	For recall, the symbol "|" means in this context: satisfies the property...
	\end{tcolorbox}	
	In practice, however, open sets are usually chosen to be similar to the open intervals of the real line. The notion of an open set provides a fundamental way to speak of nearness of points in a topological space, without explicitly having a concept of distance defined. 
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The points $(x, y)$ satisfying $x^2 + y^2 = r^2$ are colored blue. The points $(x, y)$ satisfying $x^2 + y^2 < r^2$ are colored red. The red points form an open subset of the plane $\mathbb{R}^2$. The blue points form a boundary set. The union of the red and blue points is a closed set.
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/opened_set.jpg}
	\end{figure}
	\end{tcolorbox}
	This definition may perhaps seem complicated but in fact, its real meaning is simpler than it seems. In fact, according to this definition, an open set in a topological space is nothing more than a set of contiguous points and without borders.
	
	The lack of border comes from the condition $r\neq 0$. Indeed, by reductio ad absurdum, if an open set $U$ had an edge, then for each point on it (the edge) it would still be possible to find a point not belonging to $U$ as close as we want from it. It follows that the distance $r$ becomes necessary therefore zero.

	\textbf{Definitions (\#\mydef):} 
	\begin{enumerate}
		\item[D1.] A "\NewTerm{closed subset}" is an "\NewTerm{open with edge}"

		\item[D2.] A "\NewTerm{neighborhood}" of a point $E$ is a subset of $E$ containing an open subset containing this point.
	\end{enumerate}
	The definition of an open set can be simplified by introducing an additional concept, that of "open ball":
	
	\subsubsection{Balls}
	Given $x$ an element of $E$:
	
	\textbf{Definition (\#\mydef):} An "\NewTerm{open ball of center $x$ and radius $r>0$}" or "\NewTerm{metric ball of radius $r$ centered at $x$ without border}" is the subset of all the points of $E$ whose the distance $x$ is less than $r$, that we write in general:
	
	An open set can also be defined as a set for which it is possible to define an open ball at each point.
	
	Typically in the real plane where $d$ is the euclidean distance:
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/open_set.jpg}
		\caption{An open ball of radius $r$, centered at the point $x$}
	\end{figure}
	An open set can also be defined as a set for which it is possible to define an open ball at each point.
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} The open such defined, form what we name an "\NewTerm{induced topology}" by the distance $d$ or also a "\NewTerm{metric topology}".\\
	
	\textbf{R2.} We name an "\NewTerm{open cover}" $U$ of $E$, a set of open of $E$ whose union is equal to $E$. In other words: A collection of open sets that collectively cover another set.\\
	
	Formally, if:
	
	is an indexed family of open sets $U_\alpha$, then $C$ is a cover of $X$ if:
	
	Visually in a naive way this gives:
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/open_cover.jpg}
	\end{figure}
	\end{tcolorbox}	
	\textbf{Definition (\#\mydef):} A "\NewTerm{closed ball}" is similar to an open ball but differs in the sense that we include the elements located at a distance $r$ from the center:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	For $0<r<r'$ the inclusions $_oB_x^r \subset B_x^r \subset B(x,r')$ are direct consequences of the definition of the open and closed ball.
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The usual distance in $\mathbb{R}$ is given by $d(x,y)=|x-y|$. The balls are there simple intervals. For $x \in \mathbb{R}$ and $r\in \mathbb{R}_{+}^{*}$, we have:
	
	\end{tcolorbox}
	\textbf{Definition (\#\mydef):} A "\NewTerm{sphere}" is given by:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Since by definition $r>0$, open and closed balls are not empty because they contain at least their center. By cons, a sphere may be empty!
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	With $\mathbb{R}^n,\mathbb{C}^n$ we have seen in the previous examples we could set different distances. To distinguish them, we denote then by:
	
	So in $\mathbb{R}^2$ the closed balls with center O and of radius unit equivalent to the previous three formulations, have the following shapes (remember that $0<r\leq 1$ in this example):
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/shape_some_distances.jpg}
		\caption{Examples of closed balls of unit radius with different distances}
	\end{figure}
	\end{tcolorbox}
	For example in statistics (see the section of the same name) we also use (among a lot of others) the Chi-2 distance given by:
	
	Or always in (multivariate) statistics (have a look to the Statistics section but also to the Industrial Engineering one) the  Mahalanobis distance:
	
	Or in the Error Correcting Codes section we use the Hamming distance given by:
	
	and so on...
	
	\subsubsection{Partititions}
	Now that we have defined the concepts of balls, we can finally (almost) rigorously define the concepts of open and closed intervals (which in a space of more than one dimension are named "partitions") that we have so often used in the section of Functional Analysis and Differential and Integral Calculus.
	
	\textbf{Definition (\#\mydef):} Let $(X,d)$ of a metric space. We say that a subset $A$ of $X$ is "bounded" if there is a closed ball $_fB_{r_0}^r(X)$ such that $A \subseteq _fB_{r_0}^r(X)$:
	
	Given the previous note on balls inclusions, it is clear that we can replace the word "closed" by"open". Moreover the triangle inequality implies that the bounded character of $A$ does not depend on the choice of $x_0$ (with a $x_0^{'}$ we simply need to replace $r$ by $r'=r+d\left(x_0,x_0^{'}\right)$).
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The oddâ€“even topology is the topology where $X = \mathbb{N}$ and:
	
	the unbounded partitions of radius $r<1$. \\
	
	 Therefore we see that unless $P$ is trivial, at least one set in $P$ contains more than one point, and the elements of this set are topologically indistinguishable: the topology does not separate points!
	\end{tcolorbox}
	
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] Let $X$ be a set and $(Y,d)$ a metric space. If $X$ is a set, we say that a function $f: X\mapsto Y$ est "bounded" if its image $f (X)$ is bounded (the case of the sine or cosine function, for example).
		
		\item[D2.] Given $(E, d)$ a metric space, and given $A$ a non-empty subset of $E$. For any $u\in E$ we note $d(u, A)$ and name "\NewTerm{distance $u$ to $A$}", the positive real number:
		
		We extend the concept by writing:
		
		If $A$ and $B$ are two parts (subsets) of $E$ we have respectively (perhaps this is more understandable in this way for some readers...)
		
		The reader must take care here to interpret $d(A,B)$ as the infinimum of the distance between the sets $A$ and $B$, because the distance between the parties does not always define a distance in the usual way on the part for example of $\mathcal{P}(\mathbb{R})$.
		
		Indeed, if we take again our famous example:
			
	 	we have $d(A,B)=0$ when $n \rightarrow 0$ while $A\neq B$.
	 	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
		\textbf{R1.}If the reader has well understood the definition of "parts" (and especially the previous example) he has probably noticed that it does not necessarily always exist a $a\in A$ such that $d(u,A)=d(u,a)$. Accordingly, we write:
		
		Moreover, if such an $\alpha$ exists, it is obviously not necessarily unique.\\
		
		\textbf{R2.} It should be remembered that this distance also meets the $5$ axioms of distances (we can give the proof on request)!
		\end{tcolorbox}	
	
		\item[D3.] Given $(E, d)$ a metric space, and let $A$ be a part (subset) of $E$. We name "\NewTerm{adhesion}" of $A$ and denote by $\text{adh} (A)$ the subset of $E$ defined by:
		
		For example, the adhesion of the part (subset) of rational numbers $\mathbb{Q}$ (part $A$) of $\mathbb{R}$  (the metric space $E$) is a subset of $\mathbb{R}$ itself since any real number is the limit of a rational.
		
		Especially, since $\forall u\in A:\quad =+\infty$, we have $\text{adh}(\varnothing)=\varnothing$, and since $\forall u\in E:\quad d(u,E)=0$, we have $\text{adh}(E)=E$.
		
		\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
		\textbf{R1.} Any element of the set $\text{adh}(A)$ est named "adheherent point" of $A$.\\
		
		\textbf{R2.} We say that a part $A$ of $E$ is and "\NewTerm{closed part}" if it is equal to its adherence.\\
		
		\textbf{R3.} We say that a part $A$ of $E$ is an "\NewTerm{open part}" if its complementary relatively to $E$:
		
		is closed.
		\end{tcolorbox}	
	\end{enumerate}
	It follows from the definitions that (without proof):
	
	and:
	
	with some properties (supposed as very obvious but we can give the proof on request):
	\begin{enumerate}
		\item[P1.] If $A\subset E$ and $B\subset E$ satifies $A\subset B$, then we have:
		
		
		\item[P2.] For all $A\subset E$, any $u\in E$ we have:
		
		The latter property has for corollary (obvious and therefore without proof excepted on request):
		If for any $u\in E$, we have $d(u,A)=d(u,B)$ and $A,B\neq \varnothing$, we then have:
		
	\end{enumerate}
	
	\subsubsection{Formal Ball}
	The concept of distance from a point to a set gives the possibility to extend the notions of ball and sphere see previously. We will see now the basis concepts of a "\NewTerm{formal ball}" also named "\NewTerm{generalized ball}".
	
	\begin{enumerate}
		\item[D1.] Given $A\neq \varnothing$ and given a $r>0$. We name "\NewTerm{generalized open ball}" of center $A$ of radius $r$, the following set:
		
		and respectively "\NewTerm{generalized closed ball}":
		
		and respectively  a "\NewTerm{generalized sphere}":
		
		
		\item[D2.] Given $(E, d)$ a metric space and let $A, B$ be two non-empty parts (subsets) of $E$. We denote by $g (A, B)$ and name "\NewTerm{gap}" of $A$ to $B$, the real number greater than or equal to zero such that:
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		The triangle inequality $g(A,B)\leq g(A,C)+g(C,B)$ is not valid in the context of gaps. To prove it, a single example that contradicts this inequality is sufficient.
		\end{tcolorbox}
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		In $\mathbb{R}$ let us take $A=\{0,1\},B=\{2,3\},C=\{1,3\}$ then we have:
		
		\end{tcolorbox}	
	\end{enumerate}
	
	\subsubsection{Diameter}
	\textbf{Definition}: Given $(E, d)$ a metric space and $A$ a non-empty part (subset) of $E$. We denote $\text{diam}(A)$ and name "\NewTerm{diameter}" of $A$, the positive nonzero real number:
	
	Every non-empty part (subset) $A$ of a metric space satisfying $\text{diam}<+\infty$ will also be say "bounded".
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We consider the empty set $\varnothing$ as bound set of diameter $A$.
	\end{tcolorbox}	
	If the whole metric space $(E,d)$ is bounded, we say that the distance $d$ is bounded. For example, the discrete distance is limited, the usual distance on $\mathbb{R}$ is not.
	
	We also have the following properties (the first two are usually trivial, the third one comes from the definition of the diameter itself):
	\begin{enumerate}
		\item[P1.] $\text{diam}(A)=0 \Leftrightarrow A=\{a\}$ or $A=\varnothing$
		
		\item[P2.] $A\subset B \Rightarrow \text{diam}(A)\leq \text{diam}(B)$
		
		\item[P3.] $\text{diam}(_fB_x^r)\leq 2r,\text{diam}(_oB_x^r)\leq 2r,\text{diam}(S_x^r)\leq 2r$
		
		Caution!!! Concerning the latter property, the reader must take the habit of thinking with the Euclidean distance. The first common pitfall is to think that the second diameter (that of the open ball) should be strictly less but that would be forgetting that the board has no thickness strictly speaking!
	
		There is also often a understanding problem wiht $\text{diam}(S_x^r)\leq 2r$. To be convinced just take the discrete distance (that for two points that are not confused is equal to $1$, otherwise $0$). Thus, in a metric space where we take $S_x^r$ with $r=1$, we have indeed $\text{diam}(S_x^1)\leq 1$ (that is an interesting case because almost completely counter-intuitive).
		
		\item[P4.] $\text{diam}(A\cup B)\leq \text{diam}(A)+g(A,B)+\text{diam}(B)$
		
		To be convinced, in $\mathbb{R}$ take $A=B$, then we have (trivial strict inferiority):
		
		
		\item[P5.] $A$ is bounded if and only if: $\exists r>0,\exists x\in E:\quad A\subset _oB_x^r$
	\end{enumerate}

	\textbf{Definition (\#\mydef):} We name "\NewTerm{Hausdorff excess}" or "\NewTerm{Hausdorff distance}" from $X$ to $Y$:
	
	that we found often in the literature with the more condensed notation:
	
	or much more explicitly:
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/hausdorff_distance.jpg}
		\caption{Components of the calculation of $d_H$ between $X$ and $Y$ (source: Wikipedia)}
	\end{figure}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us take $X\subset \mathbb{R}^2$ as the unit radius circle centered at the origin and $Y$ to the square circumscribing it. Elementary geometry concepts obviously leads to finding that the Hausdorff distance between the circle and the square is therefore:
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/hausdorff_distance_highschool_example.jpg}
		\caption{High-school example of a Hausdorff distance in the plane}
	\end{figure}
	technically:
	
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We have generally $e(X,Y)\neq e(Y,X)$ and these quantities may not be finite.
	\end{tcolorbox}
	
	\subsection{Varieties}
	We now introduce the "varieties". These are topological spaces that are "locally as $\mathbb{R}^2$" (our space for example ...).
	
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] A "\NewTerm{topological variety of dimension $n$}" is a Hausdorff space $M$ such that for every $p\in M$ there exists an open neighborhood $U\subset M$ with $p\in U$, an open neighborhood $U' \subset \mathbb{R}^n$ and a homeomorphism such that:

		\item[D2.] A "\NewTerm{homeomorphism}" between two spaces is a continuous bijection whose inverse is also continuous.

		\item[D3.] The pairs $(U,\varphi)$ are named "\NewTerm{maps}", $U$ being the "\NewTerm{domain of the map}" and $\varphi$ the "\NewTerm{coordinate application}". Instead of "map" sometimes we say also "coordinate system".

		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		We will denote by $\dim(M)$ the dimension of a topological variety. Therefore:
		
		\end{tcolorbox}
	
		\item[D4.] Given $M$ be a topological variety of dimension $n$. A family $A$ of maps of $M$ family is named a  "\NewTerm{atlas}" if for each $x\in M$, there is exists a map $(U, \varphi)\in A$ such as $x\in U$.
	\end{enumerate}
	If $(U_1,\varphi_1),(U_2,\varphi_2)$ are two maps of $M$ such as $U_1\cap U_2\neq \varnothing$, then the application of map changes:
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/homeomorphism_of_maps.jpg}
	\end{figure}
	is obviously also a homeomorphism.

	\subsubsection{Surfaces Homeomorphism}
	\textbf{Definition (\#\mydef):} In the mathematical field of topology, a "\NewTerm{homeomorphism} or "\NewTerm{topological isomorphism}" is a continuous function between topological spaces that has a continuous inverse function. Roughly speaking, a topological space is a geometric object, and the homeomorphism is a continuous stretching and bending of the object into a new shape. Thus, a square and a circle are homeomorphic to each other, but a sphere and a torus are not. 
	
	More formally, remember that an application $\varphi: X \mapsto Y$ between two topological spaces is named a homeomorphism if it has the following properties:
	\begin{enumerate}
		\item $\varphi$ is a bijection (one-to-one and onto)
		
		\item $\varphi$ is continuous
		
		\item The reciprocal function $\varphi^{-1}$ is continuous 
	\end{enumerate}
	
	Can we say that a square (being a special map in $\mathbb{R}^2$) is homeomorph to circle (being another special map in $\mathbb{R}^2$), or a torus to a cup of tee... If this is possible we must be able to found a closed form bijective expression between the two surfaces.
	
	As the pure theoretical concepts are very not friendly in our point of view let us begin with a two dimension special case. Let us show (prove) first that we can transform all interior points of  square of side $1$ into all interior points circle of radius $1$. This is represented by the  know figure:
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/isomorphism_circle_square.jpg}
	\end{figure}
	Such mappings have particular interest in industrial design or just simply for communication purposes (Photoshop effects or Statistics charts deformation as we do many times in the R Software):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/analysis/chessboard_isomorphic_circle_square.jpg}
	\end{figure}
	or for defishing fish eyes captors, picture or security mirrors:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1.25]{img/analysis/defishing.jpg}
	\end{figure}
	Recall that we defined unit disc as the set:
	
	If we think of the unit dis as a continuum of concentric circles with radii growing from zero to one, we ca parameterize the unit disc as the set:
	
	In doing so, we introduced a parameter $t$ this is the distance of point $(u,v)$ to the origin.
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/continnum_disc.jpg}
	\end{figure}
	In analogy to the circular continuum of the unit disc, one can write the square region $[-1,1] \times [-1,1]$ as the set:
	
	In other words, the square can be considered as a continuum of concentric shrunken FG-squircles (\SeeChapter{see section Analytical Geometry}).
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/continnum_square.jpg}
	\end{figure}

	Topologists denote the proof that the interior points of two geometries are homeomorph in this special case using the following notation
	
	We will now show that $\mathring{\mathcal{D}}\mapsto \mathring{\mathcal{D}}$ as it is the most common case in practice in our point of view. That is to say:
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/mapping_circle_to_square.jpg}
	\end{figure}
	We can establish a correspondence between the unit disc and the square region by mapping every circular contour in the interior of the disc to a squircular contour in the interior of the square. In other words, we map contour curves in the circular continuum of the disc to those in the squircular continuum of the square. This can be done by equating the parameter $t$ of both sets to get the equation:
	
	We name this equation the "\NewTerm{squircularity condition}" for mapping a circular disc to a square region.
	
	It is easy to derive the FG-Squircular mapping by combining the squircularity condition:
	
	That we can also write:
	
	Using radial to cartesian coordinates (\SeeChapter{see section Vector Calculus}):
	
	Therefore by equivalence we get:
	
	After substitution of parameter $t$, we get:
	

	In other words, this is a radial mapping that converts circular contours on the disc to squircular contours on the square.
	
	We shall now derive the inverse equations for the FG-Squircular mapping. But as it is boring to write in in \LaTeX{} and it is not used to much in practice we will omit the latter for the moment.
	
	\subsubsection{Differential Varieties}
	\textbf{Definitions}:
	\begin{enumerate}
		\item[D1.] A "\NewTerm{differentiable variety}" is a topological space $M$ where the applications $\varphi$ are of class $\mathcal{C}^{+\infty}$.

		\item[D2.] A "\NewTerm{diffeomorphism}" is an application where $\varphi: U\mapsto U' $ where $U,U'$ are open domains of $\mathbb{R}^n$ and if $\varphi$ is a homeomorphism and furthermor $U,U'$ are differentiable.
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		"Differentiable" in this context will always mean of class $\mathcal{C}^{+\infty}$.
		\end{tcolorbox}

		\item[D3.] Given a topological variety $M:=M^n$ (to simplify the notations), two maps $(U_1,\varphi_1),(U_2,\varphi_2)$ of $M$ are named \NewTerm{compatible maps} (more precisely: compatible of class $\mathcal{C}^{+\infty}$ if one of these two properties is satisfied:
		\begin{enumerate}
			\item[P1.] $U_1\cap U_2\neq \varnothing$ and the applicaton $\varphi_2\circ \varphi_1^{-1}$ of map changes is a diffeomorphism.

			\item[P2.] $U_1\cap U_2 =\varnothing$
		\end{enumerate}
		An atlas $A$ of $M$ is differentiable if all maps of $A$ are compatible between them.
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Given a differentiable atlas, it is sometimes necessary to complete it: we say that a map of $M$ is compatible with a differentiable atlas if it is compatiable with every map of $A$. An atlas of $A$ is a "\NewTerm{maximal atlas}" if every compatible map of $A$ belongs already to $A$. A maximal atlas is named a "\NewTerm{differentiable structure}".
	\end{tcolorbox}

	\begin{flushright}
	\begin{tabular}{l c}
	\circled{70} & \pbox{20cm}{\score{4}{5} \\ {\tiny 12 votes,  71.67\%}} 
	\end{tabular} 
	\end{flushright}

	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Measure Theory}
	\begin{tcolorbox}[colback=red!5,borderline={1mm}{2mm}{red!5},arc=0mm,boxrule=0pt]
	\bcbombe Caution! The level of abstraction and of motivation required for reading and understanding this section is quite high for engineers (target audience of this book for recall). The reader should be comfortable with the concepts seen in the section Set Theory as well as the one one Topology. We also apologize for the actual lack of figures. 
	\end{tcolorbox}
	\lettrine[lines=4]{\color{BrickRed}T}he measure, in the topological sense, will allow us to generalize the elementary notion of measure of a segment or area (in the Riemann sense, for example) and is inseparable from the new theory of integration that will build Lebesgue from the years to 1901-1902 and we will address here to build mathematical tools much more powerful than the simple Riemann integral (\SeeChapter{see section Differential and Integral Calculus}) with practical and numerical example in MATLAB\textsuperscript{TM}.
	
	The philosophers of science who developed measurement theory were largely concerned with epistemic questions like: we can't observe correlations between physical objects and real numbers, so how can the use of real numbers be justified in terms of things we can observe? Indeed, privileges a single unit of mass, involves real numbers in the facts of
mass. Why is the latter bad?: 
	\begin{enumerate}
		\item Real numbers are abstract and therefore causally inert
		
		\item Real numbers don't fundamentally exist
		
		\item Real numbers are constructed entities, and constructed entities can't be involved  in fundamental facts
	\end{enumerate}

	The Measure Theory will also allow us to rigorously define the concept of measurement (no matter what is the measure) and so return to the important results of the study of probabilities (\NewTerm{see section Probabilities}). Indeed, we will see (we will define the vocabulary that follows just now further below) why $(U, A, P)$ is a "\NewTerm{probability space}" where $A$ is in fact a "\NewTerm{tribe}" on $U$ and $P$ a measure on the measurable space $(U , A)$.
	
	\subsection{Measurable Spaces}
	When in mathematics we calculate derivatives, primitives or simply count stuff, we carry implicitly a measure of an object or set of objects. Rigorously, mathematicians want to define how the measured thing can be structured, how to make a measurement of it and the properties resulting!
	
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] Let $E$ be a set, a "\NewTerm{tribe}" on $E$ is a family $\mathcal{A}$ (this notation comes from the fact that many people speak of "\textbf{A}lgebra sets" instead of "tribe") of subsets of $E$ satisfying the following axioms:
		\begin{enumerate}
			\item[A1.] $E\in \mathcal{A}$ (see examples below - $E$ being one of the possible elements of $\mathcal{A}$).
			
			\item[A2.]  If $A$ is a member of a tribe then:
			
			This means that $\mathcal{A} $ is "\NewTerm{stable by transition to complementary}". This axiom implies that the empty set is always an element of a tribe!
			
			\item[A3.] For any sequence $(A_n)$ of elements of $\mathcal{A}$ we have:
			
			 We then say that $\mathcal{A}$ is then "\NewTerm{stable by countable union}".
		\end{enumerate}
		For example, the graduating from a simple ruler of measurement ... satisfies these three axioms!
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} We write $E\in \mathcal{A}$ because we consider with this notation $E$ not anymore as a subset of $\mathcal{A}$  but as an element of $\mathcal{A}$!\\
	
	\textbf{R2.} The uncountable cases are typical of topology, statistics or integral calculus!
	\end{tcolorbox}	
	
	\item[D2.] The pair $(E,\mathcal{A})$ is named "\NewTerm{measurable space}" and we say that the elements of $\mathcal{A}$ are "\NewTerm{measurable sets}".
	
	\item[D3.] If in the third axiom we require that $\mathcal{A}$ is stable under finite (uncountable) union then we impose the more general notion of "\NewTerm{$\sigma$-algebra}". Thus, a tribe is necessarily contained in an $\sigma$-algebra (but the opposite is not true just because the axiom is stronger) such that we can write:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	In the field of probabilities, $E$ is assmiliated to the Universe of events and $\mathcal{A}$ to a family of events and we speak then of "\NewTerm{probabilistic space}" or simply of... "\NewTerm{measurable space}".
	\end{tcolorbox}	
	\end{enumerate}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Given $E=\{1,2\}$ a set of cardinal 2... The only two tribe $\mathcal{A}$ that satisfy the three axioms are:
	
	There are no other tribes for the set $E$ as these two (the normal one, and the maximum one), because we must not forget that the union of each elements of the tribe must also be in the tribe (axiom A3), and also the complement of a member (axiom A2).\\
	
	We also see from this example that if $E$ is set then $\{E,\varnothing\}$ is indeed a tribe!\\
	
	E2. The set of parts of $E$, denoted $\mathcal{P}(E)$ is also a tribe (dixit previous example).
	\end{tcolorbox}
	A tribe $\mathcal{A}$ is also "\NewTerm{stable by the union of the finite complementaries}". Indeed, if $(A_n)$ is a sequence of elements of $\mathcal{A}$ we have (trivial when taking for example the previous first example):
	
	A tribe is also "\NewTerm{stable by finite intersection}", that is to say (trivial also by taking the previous first example):
	
	which brings to the property that a tribe is stable by finite unions and intersections. Especially, if we take two elements of a tribe $A,B\in \mathcal{A}$, then $A\setminus B\in \mathcal{A}$ with for recall (\SeeChapter{see section Set Theory}):
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Most readers should probably easily see with the previous first example that if $(\mathcal{A}_i)_I$ is a family of tribes on $E$ the $\bigcap_I \mathcal{A}_i$ is also a tribe (the verification is almost immediate).
	\end{tcolorbox}	
	Well it is nice to play with potatoes and sub-potatoes... and their complementary but let us continue...
	
	\textbf{Definition (\#\mydef):} Given $E$ a set and $\mathcal{B}$ a family of subsets of $\mathcal{P}(E)$ such that $\mathcal{B}\subset \mathcal{P}(E)$. We denote by definition:
	
	the "\NewTerm{generated tribe}" by $\mathcal{B}$. Therefore $\sigma(\mathcal{B})$ is by defintion the smallest tribe containing $\mathcal{B}$ (and by extension the smallest tribe of $E$).

	Below are three small examples that gives the opportunity to check if what precedes has been well understood and that also gives the possibility to highlight important results for what will follow:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	Given a set $E$ and $A\subset E,A \neq E$ and also $\mathcal{B}=\{A\}$ then (when $A$ is seen as a subset of $E$ as given by the statement of a family of subsets!):
	
	
	E2. If $\mathcal{A}$ is a tribe on $E$ then:
	

	E3. Given $E=\{1,2,3,4\}$ and $A=\{\{1,2\},{3}\}$ we then have (take care because now $A$ is a family of parts (subsets) and not only a unique subset!) the following generated tribe:
	
	Rather than determining this tribe by seeking the smallest tribe $\mathcal{P}(E)$ containing $A$ (which would be laborious) we play with the axioms defining a tribe to easily find it.
	
	So therefore we find well in $\sigma(A)$ at least the obligatory empty set $\{\varnothing\}$ and also:
	
	follwing the axiom A1 and:
	
	itself by the definition of $\sigma(A)$ and the complementaries of:
	
	following the axiom A2 and also the unions:
	
	following the axiom A3.
	\end{tcolorbox}
	\textbf{Definition}: Let $E$ be a topological space (\SeeChapter{see section Topology}). We denote by $\mathcal{B}(E)$ the tribe generated by the open sets of $E$. $\mathcal{B}(E)$ is named the "\NewTerm{borelian tribe}" of $E$. The elements of $\mathcal{B}(E)$ are named the "\NewTerm{borelians}" of $E$. 
	
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} The notion of borelian tribe is especially interesting because it is necessary for the definition of "Lebesgue tribe" and afterwards to the "Lebesgue measure" that will lead us to define the  famous "Lebesgue integral"!\\
	
	\textbf{R2.} The tribe $\mathcal{B}(E)$ being stable by going to the complementary, it also contains all closed subsets.\\
	
	\textbf{R3.} If $E$ is a topological space with a finite basis, $\mathcal{B}(A)$ in generated by the opens of the basis.
	\end{tcolorbox}	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	If $\mathbb{R}$ designates the space provided of real numbers with the Euclidean topology (\SeeChapter{see section Topology}), the family of open intervals with rational bounds is a "\NewTerm{countable base}" (given the bounds...) of $\mathbb{R}$ and therefore generates $\mathcal{B}(\mathbb{R})$ . Same thing for $\mathbb{R}^d$ with for countable basis the family of open spaces with rational bounds.
	\end{tcolorbox}
	\begin{theorem}
	Let us now consider a dense set (\SeeChapter{see section Topology}) in $\mathbb{R}$. The following families generate $\mathcal{B}(\mathbb{R})$:
	
	\end{theorem}
	\begin{dem}
	Given (the family of open subsets):
	
	We have obviously:
	
	Furthermore:
	
	Therefore the intervals of the type $[a,b[$ with $a$ and $b$ in $\mathcal{S}$ also belongs to $\sigma(\mathcal{F})$. Therefore, if we generalize, with $x<y$, it exists a sequence $(a_n)$ of elements of $\mathcal{S}$ decreasing to $x$ and a sequence $(b_n)$ of elements of $\mathcal{S}$ increasing to $y$ such that:
	
	which bring in the same way as $E\in \mathcal{A}$ that $\mathcal{B}(\mathbb{R})\subseteq\sigma(\mathcal{F})$. Other cases can be treated analogously.
	\begin{flushright}
		$\square$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{theorem}
	Given $(E, \mathcal{A})$ a measurable space and $A\subseteq E$ (and $A\in \mathcal{A}$) (where $A$ is therefore considerate as a subset and non as an element!). The family $\{A\cap B| B\in \mathcal{A}\}$ is a tribe on $A$ named "\NewTerm{trace tribe}" of $\mathcal{A}$ on $A$, that we will denote by $A\cap \mathcal{A}$. Furthermore, if $A\in \mathcal{A}$, the trace tribe is formed by the measurable elements contained in $A$.
	\end{theorem}
	\begin{dem}
	We will do a proof by the example (... yes it is not a real proof...). For this we check the three points that define a tribe:
	\begin{enumerate}
		\item $A=(E\cap A) \Rightarrow A\in (A\cap \mathcal{A})$
		
		\item Given $B\in\mathcal{A},A \setminus (A\cap B)=A\setminus B=A \cap B^c=A \cap B^c$ and therefore $A\setminus (A\cap B)\in (A\cap \mathcal{A})$
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Given $E=\{1,2,3\}$ then (a tribe among others - do not forget the stability by union!):
		
		Let us choose $A=\{1,2\},B=\{2,3\}$ (it is obvious that $\{A\cap B|B\in \mathcal{A}\}$ is a tribe on $A$). Then:
		
		and we have well $\{1\}\in A$ and also $A\in (A\cap \mathcal{A})$.
		\end{tcolorbox}
		
		\item Given $(A\cap B_n)$ a sequence of elements of $A\cap \mathcal{A}\; (B_n\in \mathcal{A})$ then:
		
		The last statement of the proposition will be supposed as obvious (if not, let us know!).
	\end{enumerate}
	\begin{flushright}
		$\square$  Q.E.D.
	\end{flushright}
	\end{dem}
	Given now $E$ a set, $\mathcal{C}$ a family of subsets of $E$ and $A\subseteq E$ non empty. We denote by $A\cap \mathcal{C}$, the trace $\mathcal{C}$ on $A$ and $\sigma_A(A\cap \mathcal{C})$ the tribe generated on $A$. Therefore:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Given the set $E=\{1,2,3,4\},\mathcal{C}=\{\{1,2\},\{3\}\},A=\{3,4\}$ then:
	
	and let us check that $A\cap \sigma(\mathcal{C})=\sigma_A(A\cap \mathcal{C})$:
	
	So the equality is satisfied!
	\end{tcolorbox}
	A trivial corollary of this equality is that if we consider a topological space $ E$ and $A\subseteq E$ with the induced topology, then:
	
	We will study more in details $\sigma$-algebra in measurement theory but first let us recall that a tribe (sometimes named "algebra of sets") on $E$ must satisfy the following properties:
	\begin{enumerate}
		\item[P1.] Has to contain $E$
		\item[P2.] Must be stable by the complementary
		\item[P3.] Must be stable by countable union or intersection
	\end{enumerate}
	and a $\sigma$-algebra on $E$ is less restrictive than a tribe as it has to satisfy:
	\begin{enumerate}
		\item[P1.] Has to contain $E$
		\item[P2.] Must be stable by the complementary
		\item[P3.] Must be stable by finite (uncountable) union or intersection
	\end{enumerate}
	Let us recall (\SeeChapter{see section Set Theory}) that if we have $E$ that is a set, then for every $A,B\subseteq E$ we define the symmetric difference $A\Delta B$ between $A$ and $B$ by:
	
	Trivial properties are as follows:
	\begin{enumerate}
		\item[P1.] A $\sigma$-algebra is stable by symmetric difference ($A,B\in \mathcal{A}$ we have $A\Delta B\in \mathcal{A}$)
		
		\item[P2.] $A\Delta B=B\Delta A$
		\item[P3.] $A^c\Delta B^c=A\Delta B$
			
		\item[P4.] $A\Delta B=(A\cup B)\setminus (A\cap B)$
	\end{enumerate}
	\begin{theorem}
	If $\mathcal{B}$ is a $\sigma$-algebra over $E$, then $(\mathcal{B},\Delta,\cap)$ is a "\NewTerm{Boolean ring}" (or "Boolean algebra" but be careful with the term "algebra" here which can cause confusion with the corresponding structure in Set theory) with $\varnothing$ and $E$ as neutral "additive" element ($\Delta$) and respectively" multiplicative" ($\cap$).
	\end{theorem}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	For reminders on the items listed in the preceding paragraph, the reader can refer to the section of Set Theory and the subsection of Boolean Algebra (\SeeChapter{see section Formal Logic Systems}).
	\end{tcolorbox}	
	\begin{dem}
	The "addition" $\Delta$ is associative because developing we get (this can verified by an arrow diagram if needed - the "potatoes"):
	
	and the latter expression is stable by permutation (commutation) of $A$ and $C$ (same method of verification). Therefore:
	
	We check that $\varnothing$ is neutral with respect to the symmetric difference (the proof that $E$ is neutral with respect to inclusion is obvious). It is trivial that:
	
	$(\mathcal{B},\Delta,\varnothing)$ is therefore well an Abelian group with respect to the law $\Delta$ (symmetric difference).
	
	Finally $\cap$ is distributive with respect to $\Delta$. Indeed:
	
	What makes $(\mathcal{B},\Delta,\varnothing)$ is indeed a ring (furthermore of a commutative ring!).
	\begin{flushright}
		$\square$  Q.E.D.
	\end{flushright}
	\end{dem}

	\pagebreak
	\subsubsection{Monotone Classes}
	\textbf{Definition}: Let $E$ be a set. A "\NewTerm{monotone class}" on $E$ is a family $\mathcal{C}$ of subsets of $E$ satisfying the following axioms:
	\begin{enumerate}
		\item[A1.] $E\in \mathcal{C}$
		\item[A2.] $A,B\in \mathcal{C}$ and $A\subseteq B \Rightarrow \setminus A\in \mathcal{C}$
		\item[A3.] If $(A_n)$ is an increasing sequence (take care to the word "increasing"!) of elements of $\mathcal{C}$ then $\displaystyle\bigcup_{i=1}^{+\infty} A_i\in \mathcal{C}$ (stable by countable increasing union).
	\end{enumerate}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} An increasing sequence of sets is: $A_1\subseteq A_2\subseteq A_3 ...$\\
	
	\textbf{R2.} The first two axioms imply that $\mathcal{C}$ is by complementary.\\
	
	\textbf{R3.} The three axioms together leads to that the monotonous class is stable by decreasing intersection. A way to check this to take the complement of each element of the increasing sequence to fall back on the decreasing sequence and vice versa.
	\end{tcolorbox}
	Every $\sigma$-algebra is a monotone class, because $\sigma$-algebras are closed under arbitrary countable unions and intersections.
	
	Therefore:
	
	In the same way as for the tribes, if we consider a family $(\mathcal{C}_i)_I$ of monotone class on $E$. Then $\bigcap_I \mathcal{C}_i$ is a monotone class (the proof is verified immediately by the three previous axioms).
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Given $E$ a set, $\mathcal{P}(E)$ is a monotone class on $E$. More generally, a tribe is a monotone class.

	Equivalently to tribes, les us consider a set $E$ and $\mathcal{C}\subseteq \mathcal{P}(E)$. Given $\mathcal{S}$ the family of all monotone class containing $\mathcal{C}$, $\mathcal{S}$ is not empty because $\mathcal{P}(E)\in \mathcal{S}$. We denote by:
	
	the monotone class generated by $\mathcal{C}$. Therefore $\mathcal{M}(\mathcal{C})$ is the smallest monotone class containing $\mathcal{C}$ (and satisfying obviously the previous axioms).
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	If $E$ is a set and $\mathcal{C}\subseteq \mathcal{P}(E)$ then $\mathcal{C}\subseteq \sigma(\mathcal{C})$, as $\sigma(\mathcal{C})$ is a monotone class (and also a tribe) containing $\mathcal{C}$ and therefore contains also $\mathcal{M}(\mathcal{C})$ (see the examples with tribes).
	\end{tcolorbox}	
	 \begin{theorem}
	Given $E$ as set. If $\mathcal{C}$ is a family of parts of $E$ that we impose as stable by finite intersection then $\mathcal{C}=\sigma{\mathcal{C}}$ (we then have to prove that the smallest tribe of $\mathcal{C}$ is equal to the smallest monotone class of $\mathcal{C}$. If we do not impose that $\mathcal{C}$ is stable by finite intersection we would not have necessarily the equality!
	\end{theorem}
	\begin{dem}
	As already said: $\mathcal{M}(\mathcal{C})\subseteq \sigma(\mathcal{C})$ (which is trivial). We will prove first that $\mathcal{M}(\mathcal{C})$ is a tribe on $E$. For this it is sufficient to show that $\mathcal{M}(\mathcal{C})$ is (also) stable by countable union (and not necessarily by an increasing sequence of elements!).
	
	Let us considerate following families for the proof:
	
	By the previous definitions $\mathcal{M}_1\subseteq \mathcal{C}$ but $\mathcal{C}$ being (imposed) stable by finite intersections implies that $\mathcal{C}\subseteq \mathcal{M}_1$ and therefore (it is the same reasoning as for the tribes):
	
	$\mathcal{M}_1$ is a monotone class, indeed $E\in \mathcal{M}_1$, if $A_1,A_2 \in \mathcal{M}_1$ and that $A_1\subseteq A_2$ (second axiom) then:	
	
	and therefore (which supports the fact that the other elements $(A_n)$ satisfy the previous relation):
	
	If $(A_n)$ is an increasing sequence of elements of $\mathcal{M}_1$ then:
	
	as $(A_n\cap B)$ is an increasing sequence.
	
	Therefore $\mathcal{M}_1$ is indeed a monotone class and by $\mathcal{C}\subseteq \mathcal{M}_1 \subseteq \mathcal{M}(\mathcal{C})$, we therefore have:
	
	The latter equality implies $\mathcal{C}\subseteq \mathcal{M}_2$. As for $\mathcal{M}_1$, we shos that  $\mathcal{M}_2$ is a monotone class and therefore $\mathcal{C}= \mathcal{M}_2$, which means by extension that $\mathcal{M}(\mathcal{C})$ is therefore stable by finite intersections.
	
	$\mathcal{M}(\mathcal{C})$ being stable by complementary this take us to that $\mathcal{M}(\mathcal{C})$  is, we just proved it, stable by finite unions (but we want to prove that it is stable by countable union!).
	
	Given now a sequence $(A_n)$ of elements of $\mathcal{M}(\mathcal{C})$. We consider the sequence:
	
	$(B_n)$ is an increasing sequence of elements of $\mathcal{M}(\mathcal{C})$, therefore:
	
	but:
	
	Therefore:
	
	Therefore $\mathcal{M}(\mathcal{C}$ is stable by countable union and finally $\mathcal{M}(\mathcal{C})$ is a tribe. But as $\mathcal{C}\subseteq \mathcal{M}(\mathcal{C})$ this brings us to $\mathcal{M}(\mathcal{C})=\sigma(\mathcal{C})$.
	\begin{flushright}
		$\square$  Q.E.D.
	\end{flushright}
	\end{dem}
	We will later see some important applications of this theorem (but first we want to improve the above text with figure and more simple and practical examples!).
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{50} & \pbox{20cm}{\score{4}{5} \\ {\tiny 17 votes,  76.47\%}} 
	\end{tabular} 
	\end{flushright}
